{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModel,DPRQuestionEncoder,DPRContextEncoder,RagTokenizer,RagSequenceForGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据获取 → 文本处理 → 向量编码 → 向量存储 → 检索模块 → 重排序模块 → 生成模块 → 评估模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据抓取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 数据抓取\n",
    "import re\n",
    "\n",
    "class WikipediaCrawler:\n",
    "    def __init__(self,topics,max_pages=20,max_section=5):\n",
    "        self.base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        self.topics = topics\n",
    "        self.max_pages = max_pages\n",
    "        self.max_section = max_section\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def clean_text(self,text):\n",
    "        text = re.sub(r'\\[\\d+\\]','',text) #移除引用\n",
    "        text = re.sub(r'\\s+',' ',text) # 移除多余的空格\n",
    "        return text.strip() # 移除首尾空格\n",
    "    \n",
    "    def fetch_page_content(self,title):\n",
    "        params = {\n",
    "            'action':'query',\n",
    "            'format':'json',\n",
    "            'titles':title,\n",
    "            'prop':'extracts', # 获取页面内容\n",
    "            'explaintext':True,\n",
    "            'redirects':True\n",
    "        }\n",
    "        response = requests.get(self.base_url,params=params).json()\n",
    "        page = next(iter(response['query']['pages'].values()))\n",
    "        return page['extract'] if 'extract' in page and page['extract'].strip() else None\n",
    "    \n",
    "    def fetch_links(self,topic):\n",
    "        params = {\n",
    "            'action':'query',\n",
    "            'format':'json',\n",
    "            'list':'search',\n",
    "            'srsearch':topic, # 关键词\n",
    "            'srlimit':self.max_pages # 结果数量\n",
    "        }\n",
    "        response = requests.get(self.base_url,params=params).json()\n",
    "        return [item['title'] for item in response['query']['search']]\n",
    "    \n",
    "    def chunk_text(self,text,chunk_size=300,overlap=50):\n",
    "        words =  text.split()\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            end = min(start+chunk_size,len(words))\n",
    "            chunk = ' '.join(words[start:end]) # 组合\n",
    "            chunks.append(chunk)\n",
    "            # 计算下一个块的起始位置，设置重叠区域\n",
    "            start = end - overlap if end - overlap > start else end\n",
    "        return chunks\n",
    "    \n",
    "    def run_crawl(self):\n",
    "        all_data = []\n",
    "        for topic in self.topics:\n",
    "            links = self.fetch_links(topic)\n",
    "            for link in tqdm(links[:self.max_pages],desc=f'Crawling {topic}'):\n",
    "                content = self.fetch_page_content(link)\n",
    "                if content:\n",
    "                    cleaned = self.clean_text(content)\n",
    "                    chunks = self.chunk_text(cleaned)\n",
    "                    for i,chunk in enumerate(chunks[:self.max_section]):\n",
    "                        all_data.append({\n",
    "                            'id':f'{link}_{i}',\n",
    "                            'title':link,\n",
    "                            'topic':topic,\n",
    "                            'text':chunk\n",
    "                        })\n",
    "        return all_data\n",
    "    \n",
    "    def expand_query(self,query):\n",
    "        query = re.sub(r'[^\\w\\s]', '', query.lower())\n",
    "        words = [w for w in query.split() if w not in self.stop_words]\n",
    "        synonyms = {\n",
    "            'capital': ['capital city', 'main city'],\n",
    "            'developed': ['created', 'formulated', 'pioneered'],\n",
    "            'france': ['french republic'],\n",
    "            'relativity': ['theory of relativity', 'einstein theory'],\n",
    "            'ai': ['artificial intelligence', 'machine intelligence']\n",
    "        }\n",
    "        expanded = []\n",
    "        for word in words:\n",
    "            expanded.append(word)\n",
    "            if word in synonyms:\n",
    "                expanded.extend(synonyms[word])\n",
    "        \n",
    "        # 去重并保留重要词汇\n",
    "        return \" \".join(sorted(set(expanded), key=len, reverse=True)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling Artificial Intelligence: 100%|████████████████████████████████████████████████| 20/20 [00:31<00:00,  1.60s/it]\n",
      "Crawling Machine Learning: 100%|███████████████████████████████████████████████████████| 20/20 [00:45<00:00,  2.29s/it]\n",
      "Crawling Neural Networks: 100%|████████████████████████████████████████████████████████| 20/20 [00:36<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "crawler = WikipediaCrawler(topics=[\"Artificial Intelligence\",\n",
    "                                    \"Machine Learning\", \"Neural Networks\"])\n",
    "documents = crawler.run_crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Artificial intelligence_0',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known'},\n",
       " {'id': 'Artificial intelligence_1',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI\\'s ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI\\'s long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. == Goals == The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. === Reasoning and problem-solving === Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. ==='},\n",
       " {'id': 'Artificial intelligence_2',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. === Knowledge representation === Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. === Planning and decision-making === An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In'},\n",
       " {'id': 'Artificial intelligence_3',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. === Planning and decision-making === An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent\\'s preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A'},\n",
       " {'id': 'Artificial intelligence_4',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. === Learning === Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types'},\n",
       " {'id': 'Artificial general intelligence_0',\n",
       "  'title': 'Artificial general intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Artificial general intelligence (AGI)—sometimes called human‑level intelligence AI—is a type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks. Some researchers argue that state‑of‑the‑art large language models (LLMs) already exhibit signs of AGI‑level capability, while others maintain that genuine AGI has not yet been achieved. Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin. Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved. Creating AGI is a primary goal of AI research and of companies such as OpenAI, Google, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries. The timeline for achieving human‑level intelligence AI remains deeply contested. Recent surveys of AI researchers give median forecasts ranging from the late 2020s to mid‑century, while still recording significant numbers who expect arrival much sooner—or never at all. There is debate on the exact definition of AGI and regarding whether modern LLMs such as GPT-4 are early forms of emerging AGI. AGI is a common topic in science fiction and futures studies. Contention exists over whether AGI represents an existential risk. Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk. == Terminology == AGI is also known as strong AI,'},\n",
       " {'id': 'Artificial general intelligence_1',\n",
       "  'title': 'Artificial general intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'existential risk. Many AI experts have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk. == Terminology == AGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans. Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution. A framework for classifying AGI by performance and autonomy was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous). == Characteristics == Various popular definitions of intelligence have been proposed. One'},\n",
       " {'id': 'Artificial general intelligence_2',\n",
       "  'title': 'Artificial general intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous). == Characteristics == Various popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches. === Intelligence traits === Researchers generally hold that a system is required to do all of the following to be regarded as an AGI: reason, use strategy, solve puzzles, and make judgments under uncertainty represent knowledge, including common sense knowledge plan learn communicate in natural language if necessary, integrate these skills in completion of any given goal Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy. Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree. === Physical traits === Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include: the ability to sense (e.g. see, hear, etc.), and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) This includes the ability to detect and respond to hazard. Although the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems, these physical capabilities are not strictly required for an entity to qualify as AGI—particularly under the thesis'},\n",
       " {'id': 'Artificial general intelligence_3',\n",
       "  'title': 'Artificial general intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'to hazard. Although the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems, these physical capabilities are not strictly required for an entity to qualify as AGI—particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\". It can be regarded as sufficient for an intelligent computer to interact with other systems, to invoke or regulate them, to achieve specific goals, including altering a physical environment, as HAL in 2001: A Space Odyssey was both programmed and tasked to. === Tests for human-level AGI === Several tests meant to confirm human-level AGI have been considered, including: The Turing Test (Turing) Proposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine. Turing described the test as follows: The idea of the test is that the machine has to try and pretend to'},\n",
       " {'id': 'Artificial general intelligence_4',\n",
       "  'title': 'Artificial general intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine. Turing described the test as follows: The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence. In 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test\\'s implementation and its relevance to AGI. In 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article\\'s authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\". Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\" A 2024 study suggested that GPT-4 was identified as human 54% of the time in a randomized, controlled version of the Turing Test—surpassing older chatbots like ELIZA while still falling behind actual humans (67%). A 2025 pre‑registered, three‑party Turing‑test study by Cameron R. Jones and Benjamin K. Bergen showed that GPT-4.5 was judged to be the human in 73% of five‑minute text conversations—surpassing the 67% humanness rate of real confederates and meeting the researchers’ criterion for having passed the test. The Robot College Student Test (Goertzel)'},\n",
       " {'id': 'Generative artificial intelligence_0',\n",
       "  'title': 'Generative artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts. Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu. Generative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works. Generative AI is used across many industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. == History == === Early history === The first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus,'},\n",
       " {'id': 'Generative artificial intelligence_1',\n",
       "  'title': 'Generative artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator. Computers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings. The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft. === Generative neural networks (2014–2019) === Since its inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling. In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also'},\n",
       " {'id': 'Generative artificial intelligence_2',\n",
       "  'title': 'Generative artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images. In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model. The new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models. Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained. === Generative AI boom (2020–) === In March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI. The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology. In 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery. This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts. These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public. In late 2022, the'},\n",
       " {'id': 'Generative artificial intelligence_3',\n",
       "  'title': 'Generative artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts. These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public. In late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks. The system\\'s ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI\\'s potential impact on work, education, and creativity. In March 2023, GPT-4\\'s release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of \\'general human intelligence\\'\" as of 2023. Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications. In December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano. The company integrated Gemini Pro into its Bard chatbot and announced plans for \"Bard Advanced\" powered by the larger Gemini Ultra model. In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS. In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with'},\n",
       " {'id': 'Generative artificial intelligence_4',\n",
       "  'title': 'Generative artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS. In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google. In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis. Asia–Pacific countries are significantly more optimistic than Western societies about generative AI and show higher adoption rates. Despite expressing concerns about privacy and the pace of change, in a 2024 survey, 68% of Asia-Pacific respondents believed that AI was having a positive impact on the world, compared to 57% globally. According to a survey by SAS and Coleman Parkes Research, China in particular has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. This leadership is further evidenced by China\\'s intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications. A 2024 survey on the Chinese social app Soul reported that 18% of respondents born after 2000 used generative AI \"almost every day\", and that over 60% of respondents like or love AI-generated content, while less than 3% dislike or hate it. == Applications == Notable types of generative AI models include generative pre-trained transformers (GPTs), generative adversarial networks (GANs), and variational autoencoders (VAEs). Generative AI systems are multimodal if they can process multiple types of'},\n",
       " {'id': 'History of artificial intelligence_0',\n",
       "  'title': 'History of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain. The field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true. Eventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from James Lighthill and pressure from the U.S.A. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors\\' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names. In the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing'},\n",
       " {'id': 'History of artificial intelligence_1',\n",
       "  'title': 'History of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases. Investment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, causing debate about the future of AI and its impact on society. == Precursors == === Mythical, fictional, and speculative precursors === ==== Myth and legend ==== In Greek mythology, Talos was a creature made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily. According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless. Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this,\"},\n",
       " {'id': 'History of artificial intelligence_2',\n",
       "  'title': 'History of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'flow out from his body and rendering him lifeless. Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid\\'s Metamorphoses. In the 10th book of Ovid\\'s narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved. ==== Medieval legends of artificial beings ==== In Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant. The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century. During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God\\'s names on it, into the mouth of the clay figure. Unlike legendary automata like Brazen Heads, a Golem was unable to speak. Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals. In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies. ==== Modern fiction ==== By the 19th century, ideas'},\n",
       " {'id': 'History of artificial intelligence_3',\n",
       "  'title': 'History of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies. ==== Modern fiction ==== By the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley\\'s Frankenstein and Karel Čapek\\'s R.U.R. (Rossum\\'s Universal Robots) explored the concept of artificial life. Speculative essays, such as Samuel Butler\\'s \"Darwin among the Machines\", and Edgar Allan Poe\\'s \"Maelzel\\'s Chess Player\" reflected society\\'s growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today. ==== Automata ==== Realistic humanoid automata were built by craftsman from many civilizations, including Yan Shi, Hero of Alexandria, Al-Jazari, Haroun al-Rashid, Jacques de Vaucanson, Leonardo Torres y Quevedo, Pierre Jaquet-Droz and Wolfgang von Kempelen. The oldest known automata were the sacred statues of ancient Egypt and Greece. The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion—Hermes Trismegistus wrote that \"by discovering the true nature of the gods, man has been able to reproduce it\". English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues. During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-Protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have'},\n",
       " {'id': 'History of artificial intelligence_4',\n",
       "  'title': 'History of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'have fabricated a brazen head, having developed a legend of having been a wizard. These legends were similar to the Norse myth of the Head of Mímir. According to legend, Mímir was known for his intellect and wisdom, and was beheaded in the Æsir-Vanir War. Odin is said to have \"embalmed\" the head with herbs and spoke incantations over it such that Mímir\\'s head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel. === Formal reasoning === Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or \"formal\"—reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism), Euclid (whose Elements was a model of formal reasoning), al-Khwārizmī (who developed algebra and gave his name to the word algorithm) and European scholastic philosophers such as William of Ockham and Duns Scotus. Spanish philosopher Ramon Llull (1232–1315) developed several logical machines devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull\\'s work had a great influence on Gottfried Leibniz, who redeveloped his ideas. In the 17th century, Leibniz, Thomas Hobbes and René Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry. Hobbes famously wrote in Leviathan: \"For reason ... is nothing but reckoning, that is adding and subtracting\". Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would'},\n",
       " {'id': 'A.I. Artificial Intelligence_0',\n",
       "  'title': 'A.I. Artificial Intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'A.I. Artificial Intelligence (or simply A.I.) is a 2001 American science fiction drama film directed by Steven Spielberg. The screenplay by Spielberg and screen story by Ian Watson are loosely based on the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. Set in a futuristic society, the film stars Haley Joel Osment as David, a childlike android uniquely programmed with the ability to love. Jude Law, Frances O\\'Connor, Brendan Gleeson and William Hurt star in supporting roles. Development of A.I. originally began after producer and director Stanley Kubrick acquired the rights to Aldiss\\'s story in the early 1970s. Kubrick hired a series of writers, including Aldiss, Bob Shaw, Ian Watson and Sara Maitland, until the mid-1990s. The film languished in development hell for years, partly because Kubrick felt that computer-generated imagery was not advanced enough to create the David character, which he believed no child actor would convincingly portray. In 1995, Kubrick handed A.I. to Spielberg, but the film did not gain momentum until Kubrick died in 1999. Spielberg remained close to Watson\\'s treatment for the screenplay and dedicated the film to Kubrick. A.I. Artificial Intelligence was released on June 29, 2001, by Warner Bros. Pictures in North America. It received generally positive reviews from critics and grossed $235.9 million against a budget of $90–100 million. It was also nominated for Best Visual Effects and Best Original Score (for John Williams) at the 74th Academy Awards. In a 2016 BBC poll of 177 critics around the world, A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg\\'s best works and one of the greatest films of the 21st century, and of all time. == Plot == In the 22nd century, rising sea levels from global warming have wiped'},\n",
       " {'id': 'A.I. Artificial Intelligence_1',\n",
       "  'title': 'A.I. Artificial Intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"A.I. Artificial Intelligence was voted the eighty-third greatest film since 2000. It has since been called one of Spielberg's best works and one of the greatest films of the 21st century, and of all time. == Plot == In the 22nd century, rising sea levels from global warming have wiped out coastal cities and altered the world's climate. With the human population in decline, advanced nations have created humanoid robots called mechas to fulfill various roles in society. In Madison, New Jersey, David, an 11-year-old prototype mecha child capable of experiencing love, is given to Henry Swinton and his wife Monica, whose son Martin is in suspended animation after contracting a rare disease. Initially uncomfortable with David, Monica eventually warms to him and activates his imprinting protocol. Wanting her to love him in return, he befriends Teddy, Martin's old robotic teddy bear. After Martin is unexpectedly cured of his disease and brought home, he jealously goads David into cutting off a piece of Monica's hair. That night, David enters his adoptive parents' room, but as Monica turns over, the scissors accidentally poke her in the eye. While Henry attends to her wounds, Teddy picks up the lock of hair from the floor and places it in his pocket. During a pool party, one of Martin's friends pokes David with a knife, triggering his self-protection programming. David grabs Martin, causing both of them to fall into the pool. While Martin is rescued, David is accused of endangering others. Henry convinces Monica to return David to his creators for destruction. En route, she instead spares David by abandoning him in the woods full of scrap metal and obsolete mechas. Now accompanied solely by Teddy, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy to become human, which he\"},\n",
       " {'id': 'A.I. Artificial Intelligence_2',\n",
       "  'title': 'A.I. Artificial Intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'to return David to his creators for destruction. En route, she instead spares David by abandoning him in the woods full of scrap metal and obsolete mechas. Now accompanied solely by Teddy, David recalls The Adventures of Pinocchio and decides to find the Blue Fairy to become human, which he believes will regain Monica\\'s love. David and Teddy are captured by the \"Flesh Fair\", a traveling circus-like event at which obsolete mechas are destroyed in front of jeering crowds. About to be destroyed himself, David pleads for his life, and the audience revolts and allows David to escape with Gigolo Joe, a prostitute mecha on the run after being framed for murder. David, Teddy and Joe go to the decadent resort town of Rouge City, where \"Dr. Know\", a holographic answer engine, directs them to the top of Rockefeller Center in the flooded ruins of New York City and provides fairy tale information that David interprets as suggesting that a Blue Fairy can help him. Above the ruins of New York, David meets Professor Hobby, his creator, who tells him that their meeting demonstrates David\\'s ability to love and desire. David finds copies of himself, including female variants called \"Darlene\", ready to be shipped. Disheartened by his lost sense of individuality, David attempts suicide by falling from a skyscraper into the ocean. While underwater, David notices a figure resembling the Blue Fairy before Joe rescues him in an amphibious aircraft. Before David can explain, authorities capture Joe with an electromagnet. David and Teddy take control of the aircraft to see the Blue Fairy, which turns out to be a statue from an attraction on Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. Believing that the Blue Fairy is real, David repeatedly asks the statue'},\n",
       " {'id': 'A.I. Artificial Intelligence_3',\n",
       "  'title': 'A.I. Artificial Intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'David and Teddy take control of the aircraft to see the Blue Fairy, which turns out to be a statue from an attraction on Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. Believing that the Blue Fairy is real, David repeatedly asks the statue to turn him into a real boy until his power source is depleted. Two thousand years later, humanity is extinct and Manhattan is buried under glacial ice. Mechas have evolved into an advanced form, and a group known as the Specialists, interested in humanity, find and resurrect David and Teddy. They reconstruct the Swinton family home from David\\'s memories before explaining, via an interactive version of the Blue Fairy, that he cannot become human. However, they recreate Monica through genetic material from the strand of hair that Teddy kept. This version of Monica can live for only one day and cannot be revived. David spends his happiest day with Monica, and as she falls asleep in the evening, Monica tells David that she has always loved him. David lies down next to her and closes his eyes. == Cast == == Production == === Development === Stanley Kubrick began development on an adaptation of \"Super-Toys Last All Summer Long\" in the late 1970s, hiring the story\\'s author, Brian Aldiss, to write a film treatment. In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing. Warner Bros. agreed to co-finance A.I. and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw briefly served as writer, leaving after six weeks due to Kubrick\\'s demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, \"Not only did the bastard'},\n",
       " {'id': 'A.I. Artificial Intelligence_4',\n",
       "  'title': 'A.I. Artificial Intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw briefly served as writer, leaving after six weeks due to Kubrick\\'s demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, \"Not only did the bastard fire me, he hired my enemy [Watson] instead.\" Kubrick handed Watson Carlo Collodi\\'s The Adventures of Pinocchio for inspiration, calling A.I. \"a picaresque robot version of Pinocchio\". Three weeks later, Watson gave Kubrick his first story treatment, and concluded his work on A.I. in May 1991 with another treatment of 90 pages. Gigolo Joe was originally conceived as a G.I. mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, \"I guess we lost the kiddie market.\" Meanwhile, Kubrick dropped A.I. to work on a film adaptation of Wartime Lies, feeling computer animation was not advanced enough to create the David character. After the release of Spielberg\\'s Jurassic Park, with its innovative CGI, it was announced in November 1993 that production of A.I. would begin in 1994. Dennis Muren and Ned Gorman, who worked on Jurassic Park, became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic (ILM) and Stan Winston Studio. Kubrick asked Sara Maitland to give the film mythic resonance. She recalls \"He never referred to the film as \\'A.I.\\'; he always called it \\'Pinocchio.\\'\" Kubrick\\'s version ended the same way Spielberg\\'s does, with advanced mechas reviving Monica, but only for a day. === Pre-production === In early 1994, the film was in pre-production with Christopher \"Fangorn\" Baker as concept artist and Sara Maitland assisting on the story, which gave it \"a feminist fairy-tale focus\". Maitland said that Kubrick never referred to the film as'},\n",
       " {'id': 'Applications of artificial intelligence_0',\n",
       "  'title': 'Applications of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Artificial intelligence is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. Artificial intelligence (AI) has been used in applications throughout industry and academia. Within the field of Artificial Intelligence, there are multiple subfields. The subfield of Machine learning has been used for various scientific and commercial purposes including language translation, image recognition, decision-making, credit scoring, and e-commerce. In recent years, there have been massive advancements in the field of Generative Artificial Intelligence, which uses generative models to produce text, images, videos or other forms of data. This article describes applications of AI in different sectors. == Agriculture == In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency. AI has been used to attempt to classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and optimize irrigation. == Architecture and design == AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex. AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching. == Business == === Content extraction === An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc. == Computer science == === Programming assistance === ==== AI-powered code assisting tools ==== AI can be used for real-time code completion, chat, and'},\n",
       " {'id': 'Applications of artificial intelligence_1',\n",
       "  'title': 'Applications of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc. == Computer science == === Programming assistance === ==== AI-powered code assisting tools ==== AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy. Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted. GitHub Copilot is one example. It was developed by GitHub and OpenAI and is able to autocomplete code in multiple programming languages. ==== Neural network design ==== AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet. ==== Quantum computing ==== Machine learning has been used for noise-cancelling in quantum technology, including quantum sensors. Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications, and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry problems as well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing). === Historical contributions === AI researchers have created many tools\"},\n",
       " {'id': 'Applications of artificial intelligence_2',\n",
       "  'title': 'Applications of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'well as for quantum annealers for training of neural networks for AI applications. There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing). === Historical contributions === AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories: Time sharing Interactive interpreters Graphical user interfaces and the computer mouse Rapid application development environments The linked list data structure Automatic storage management Symbolic programming Functional programming Dynamic programming Object-oriented programming Optical character recognition Constraint satisfaction == Customer service == === Human resources === Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots. === Job search === AI has simplified the recruiting/job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows. === Online and telephone customer service === AI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with'},\n",
       " {'id': 'Applications of artificial intelligence_3',\n",
       "  'title': 'Applications of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'builder that compiles a CV in 5 minutes. Chatbots assist website visitors and refine workflows. === Online and telephone customer service === AI underlies avatars (automated online assistants) on web pages. It can reduce operation and training costs. Pypestream automated customer service for its mobile application to streamline communication with customers. A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately. Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative. Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making. === Hospitality === In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs. AI hotel services come in the form of a chatbot, application, virtual voice assistant and service robots. == Education == AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.\" The World Economic Forum also stresses AI\\'s contribution to students\\' overall improvement and transforming teaching into a more enjoyable process. === Personalized learning === AI driven tutoring systems (such as Khan Academy, Duolingo and Carnegie Learning) are the forefoot of delivering personalized education. These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student\\'s pace and style of learning. === Administrative efficiency === In'},\n",
       " {'id': 'Applications of artificial intelligence_4',\n",
       "  'title': 'Applications of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"systems (such as Khan Academy, Duolingo and Carnegie Learning) are the forefoot of delivering personalized education. These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning. === Administrative efficiency === In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement. Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind. === Ethical and privacy concerns === Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data. It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices. Much of the regulation will be influenced by the AI Act, the world's first comprehensive AI law. == Energy and environment == === Energy system === Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications. AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime. The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding\"},\n",
       " {'id': 'Distributed artificial intelligence_0',\n",
       "  'title': 'Distributed artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Distributed artificial intelligence (DAI) also called Decentralized Artificial Intelligence is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems. Multi-agent systems and distributed problem solving are the two main DAI approaches. There are numerous applications and tools. == Definition == Distributed Artificial Intelligence (DAI) is an approach to solving complex learning, planning, and decision-making problems. It is embarrassingly parallel, thus able to exploit large scale computation and spatial distribution of computing resources. These properties allow it to solve problems that require the processing of very large data sets. DAI systems consist of autonomous learning processing nodes (agents), that are distributed, often at a very large scale. DAI nodes can act independently, and partial solutions are integrated by communication between nodes, often asynchronously. By virtue of their scale, DAI systems are robust and elastic, and by necessity, loosely coupled. Furthermore, DAI systems are built to be adaptive to changes in the problem definition or underlying data sets due to the scale and difficulty in redeployment. DAI systems do not require all the relevant data to be aggregated in a single location, in contrast to monolithic or centralized Artificial Intelligence systems which have tightly coupled and geographically close processing nodes. Therefore, DAI systems often operate on sub-samples or hashed impressions of very large datasets. In addition, the source dataset may change or be updated during the course of the execution of a DAI system. == Development == In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents. Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition. DAI is categorized'},\n",
       " {'id': 'Distributed artificial intelligence_1',\n",
       "  'title': 'Distributed artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'DAI system. == Development == In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents. Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition. DAI is categorized into multi-agent systems and distributed problem solving. In multi-agent systems the main focus is how agents coordinate their knowledge and activities. For distributed problem solving the major focus is how the problem is decomposed and the solutions are synthesized. == Goals == The objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of artificial intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents). To reach the objective, DAI requires: A distributed system with robust and elastic computation on unreliable and failing resources that are loosely coupled Coordination of the actions and communication of the nodes Subsamples of large data sets and online machine learning There are many reasons for wanting to distribute intelligence or cope with multi-agent systems. Mainstream problems in DAI research include the following: Parallel problem solving: mainly deals with how classic artificial intelligence concepts can be modified, so that multiprocessor systems and clusters of computers can be used to speed up calculation. Distributed problem solving (DPS): the concept of agent, autonomous entities that can communicate with each other, was developed to serve as an abstraction for developing DPS systems. See below for further details. Multi-Agent Based Simulation (MABS): a branch of DAI that builds the foundation for simulations that need to analyze not only phenomena at macro level but also at micro level, as it is in many social simulation scenarios. == Approaches == Two types of DAI has emerged: In Multi-agent systems agents'},\n",
       " {'id': 'Distributed artificial intelligence_2',\n",
       "  'title': 'Distributed artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"Multi-Agent Based Simulation (MABS): a branch of DAI that builds the foundation for simulations that need to analyze not only phenomena at macro level but also at micro level, as it is in many social simulation scenarios. == Approaches == Two types of DAI has emerged: In Multi-agent systems agents coordinate their knowledge and activities and reason about the processes of coordination. Agents are physical or virtual entities that can act, perceive its environment and communicate with other agents. The agent is autonomous and has skills to achieve goals. The agents change the state of their environment by their actions. There are a number of different coordination techniques. In distributed problem solving the work is divided among nodes and the knowledge is shared. The main concerns are task decomposition and synthesis of the knowledge and solutions. DAI can apply a bottom-up approach to AI, similar to the subsumption architecture as well as the traditional top-down approach of AI. In addition, DAI can also be a vehicle for emergence. === Challenges === The challenges in Distributed AI are: How to carry out communication and interaction of agents and which communication language or protocols should be used. How to ensure the coherency of agents. How to synthesise the results among 'intelligent agents' group by formulation, description, decomposition and allocation. == Applications and tools == Areas where DAI have been applied are: Electronic commerce, e.g. for trading strategies the DAI system learns financial trading rules from subsamples of very large samples of financial data Networks, e.g. in telecommunications the DAI system controls the cooperative resources in a WLAN network Routing, e.g. model vehicle flow in transport networks Scheduling, e.g. flow shop scheduling where the resource management entity ensures local optimization and cooperation for global and local consistency Search engines, e.g. in LLM federated\"},\n",
       " {'id': 'Distributed artificial intelligence_3',\n",
       "  'title': 'Distributed artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'data Networks, e.g. in telecommunications the DAI system controls the cooperative resources in a WLAN network Routing, e.g. model vehicle flow in transport networks Scheduling, e.g. flow shop scheduling where the resource management entity ensures local optimization and cooperation for global and local consistency Search engines, e.g. in LLM federated search like Ithy where document retrieval and analysis are distributed to DAI agents before aggregation Multi-Agent systems, e.g. artificial life, the study of simulated life Electric power systems, e.g. Condition Monitoring Multi-Agent System (COMMAS) applied to transformer condition monitoring, and IntelliTEAM II Automatic Restoration System DAI integration in tools has included: ECStar is a distributed rule-based learning system. == Agents == === Systems: Agents and multi-agents === Notion of Agents: Agents can be described as distinct entities with standard boundaries and interfaces designed for problem solving. Notion of Multi-Agents: Multi-Agent system is defined as a network of agents which are loosely coupled working as a single entity like society for problem solving that an individual agent cannot solve. === Software agents === The key concept used in DPS and MABS is the abstraction called software agents. An agent is a virtual (or physical) autonomous entity that has an understanding of its environment and acts upon it. An agent is usually able to communicate with other agents in the same system to achieve a common goal, that one agent alone could not achieve. This communication system uses an agent communication language. A first classification that is useful is to divide agents into: reactive agent – A reactive agent is not much more than an automaton that receives input, processes it and produces an output. deliberative agent – A deliberative agent in contrast should have an internal view of its environment and is able to follow its own plans. hybrid agent –'},\n",
       " {'id': 'Distributed artificial intelligence_4',\n",
       "  'title': 'Distributed artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'into: reactive agent – A reactive agent is not much more than an automaton that receives input, processes it and produces an output. deliberative agent – A deliberative agent in contrast should have an internal view of its environment and is able to follow its own plans. hybrid agent – A hybrid agent is a mixture of reactive and deliberative, that follows its own plans, but also sometimes directly reacts to external events without deliberation. Well-recognized agent architectures that describe how an agent is internally structured are: ASMO (emergence of distributed modules) BDI (Believe Desire Intention, a general architecture that describes how plans are made) InterRAP (A three-layer architecture, with a reactive, a deliberative and a social layer) PECS (Physics, Emotion, Cognition, Social, describes how those four parts influences the agents behavior). Soar (a rule-based approach) == See also == Collective intelligence – Group intelligence that emerges from collective efforts Federated learning – Decentralized machine learning Simulated reality – Concept of a false version of reality Swarm Intelligence – Collective behavior of decentralized, self-organized systemsPages displaying short descriptions of redirect targets == References == == Further reading == Hewitt, Carl; and Jeff Inman (November/December 1991). \"DAI Betwixt and Between: From \\'Intelligent Agents\\' to Open Systems Science\" IEEE Transactions on Systems, Man, and Cybernetics. Volume: 21 Issue: 6, pps. 1409–1419. ISSN 0018-9472 Grace, David; Zhang, Honggang (August 2012). Cognitive Communications: Distributed Artificial Intelligence(DAI), Regulatory Policy and Economics, Implementation. John Wiley & Sons Press. ISBN 978-1-119-95150-6 Shoham, Yoav; Leyton-Brown, Kevin (2009). Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations. New York: Cambridge University Press. ISBN 978-0-521-89943-7. Sun, Ron, (2005). Cognition and Multi-Agent Interaction. New York: Cambridge University Press. ISBN 978-0-521-83964-8 Trentesaux, Damien; Philippe, Pesin; Tahon, Christian (2000). \"Distributed artificial intelligence for FMS scheduling, control and design support\". Journal of Intelligent Manufacturing. 11 (6):'},\n",
       " {'id': 'Philosophy of artificial intelligence_0',\n",
       "  'title': 'Philosophy of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will. Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers. These factors contributed to the emergence of the philosophy of artificial intelligence. The philosophy of artificial intelligence attempts to answer such questions as follows: Can a machine act intelligently? Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same? Is the human brain essentially a computer? Can a machine have a mind, mental states, and consciousness in the same sense that a human being can? Can it feel how things are? (i.e. does it have qualia?) Questions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion. Important propositions in the philosophy of AI include some of the following: Turing\\'s \"polite convention\": If a machine behaves as intelligently as a human being, then it is as intelligent as a human being. The Dartmouth proposal: \"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\" Allen Newell and Herbert A. Simon\\'s physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" John Searle\\'s strong AI hypothesis: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly'},\n",
       " {'id': 'Philosophy of artificial intelligence_1',\n",
       "  'title': 'Philosophy of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'made to simulate it.\" Allen Newell and Herbert A. Simon\\'s physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" John Searle\\'s strong AI hypothesis: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Hobbes\\' mechanism: \"For \\'reason\\' ... is nothing but \\'reckoning,\\' that is adding and subtracting, of the consequences of general names agreed upon for the \\'marking\\' and \\'signifying\\' of our thoughts...\" == Can a machine display general intelligence? == Is it possible to create a machine that can solve all the problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns the behavior of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers, evoking the question: does it matter whether a machine is really thinking, as a person thinks, rather than just producing outcomes that appear to result from thinking? The basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the Dartmouth workshop of 1956: \"Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\" Arguments against the basic premise must show that building a working AI system is impossible because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a'},\n",
       " {'id': 'Philosophy of artificial intelligence_2',\n",
       "  'title': 'Philosophy of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible. It is also possible to sidestep the connection between the two parts of the above proposal. For instance, machine learning, beginning with Turing\\'s infamous child machine proposal, essentially achieves the desired feature of intelligence without a precise design-time description as to how it would exactly work. The account on robot tacit knowledge eliminates the need for a precise description altogether. The first step to answering the question is to clearly define \"intelligence\". === Intelligence === ==== Turing test ==== Alan Turing reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer any question posed to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online chat room, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human. Turing notes that no one (except philosophers) ever asks the question \"can people think?\" He writes \"instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks\". Turing\\'s test extends this polite convention to machines: If a machine acts as intelligently as a human being, then it is as intelligent as a human being. One criticism of the Turing test is that it only measures the \"humanness\" of the machine\\'s'},\n",
       " {'id': 'Philosophy of artificial intelligence_3',\n",
       "  'title': 'Philosophy of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'have a polite convention that everyone thinks\". Turing\\'s test extends this polite convention to machines: If a machine acts as intelligently as a human being, then it is as intelligent as a human being. One criticism of the Turing test is that it only measures the \"humanness\" of the machine\\'s behavior, rather than the \"intelligence\" of the behavior. Since human behavior and intelligent behavior are not exactly the same thing, the test fails to measure intelligence. Stuart J. Russell and Peter Norvig write that \"aeronautical engineering texts do not define the goal of their field as \\'making machines that fly so exactly like pigeons that they can fool other pigeons\\'\". ==== Intelligence as achieving goals ==== Twenty-first century AI research defines intelligence in terms of goal-directed behavior. It views intelligence as a set of problems that the machine is expected to solve – the more problems it can solve, and the better its solutions are, the more intelligent the program is. AI founder John McCarthy defined intelligence as \"the computational part of the ability to achieve goals in the world.\" Stuart Russell and Peter Norvig formalized this definition using abstract intelligent agents. An \"agent\" is something which perceives and acts in an environment. A \"performance measure\" defines what counts as success for the agent. \"If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent.\" Definitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for unintelligent human traits such as making typing mistakes. They have the disadvantage that they can fail to differentiate between \"things that think\" and \"things that do not\". By this definition, even a thermostat has a'},\n",
       " {'id': 'Philosophy of artificial intelligence_4',\n",
       "  'title': 'Philosophy of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'They have the advantage that, unlike the Turing test, they do not also test for unintelligent human traits such as making typing mistakes. They have the disadvantage that they can fail to differentiate between \"things that think\" and \"things that do not\". By this definition, even a thermostat has a rudimentary intelligence. === Arguments that a machine can display general intelligence === ==== The brain can be simulated ==== Hubert Dreyfus describes this argument as claiming that \"if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then ... we ... ought to be able to reproduce the behavior of the nervous system with some physical device\". This argument, first introduced as early as 1943 and vividly described by Hans Moravec in 1988, is now associated with futurist Ray Kurzweil, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029. A non-real-time simulation of a thalamocortical model that has the size of the human brain (1011 neurons) was performed in 2005, and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors. Even AI\\'s harshest critics (such as Hubert Dreyfus and John Searle) agree that a brain simulation is possible in theory. However, Searle points out that, in principle, anything can be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered \"computation\". \"What we wanted to know is what distinguishes the mind from thermostats and livers,\" he writes. Thus, merely simulating the functioning of a living brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind, like trying to build a jet airliner'},\n",
       " {'id': 'Existential risk from artificial intelligence_0',\n",
       "  'title': 'Existential risk from artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Existential risk from artificial intelligence refers to the idea that substantial progress in artificial general intelligence (AGI) could lead to human extinction or an irreversible global catastrophe. One argument for the importance of this risk references how human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass human intelligence and become superintelligent, it might become uncontrollable. Just as the fate of the mountain gorilla depends on human goodwill, the fate of humanity could depend on the actions of a future machine superintelligence. The plausibility of existential catastrophe due to AI is widely debated. It hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge, and whether practical scenarios for AI takeovers exist. Concerns about superintelligence have been voiced by researchers including Geoffrey Hinton, Yoshua Bengio, Demis Hassabis, and Alan Turing, and AI company CEOs such as Dario Amodei (Anthropic), Sam Altman (OpenAI), and Elon Musk (XAI). In 2022, a survey of AI researchers with a 17% response rate found that the majority believed there is a 10 percent or greater chance that human inability to control AI will cause an existential catastrophe. In 2023, hundreds of AI experts and other notable figures signed a statement declaring, \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak and United Nations Secretary-General António Guterres called for an increased focus on global AI regulation. Two sources of concern stem from the problems of AI control and alignment. Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent'},\n",
       " {'id': 'Existential risk from artificial intelligence_1',\n",
       "  'title': 'Existential risk from artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'minister Rishi Sunak and United Nations Secretary-General António Guterres called for an increased focus on global AI regulation. Two sources of concern stem from the problems of AI control and alignment. Controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would likely resist attempts to disable it or change its goals as that would prevent it from accomplishing its present goals. It would be extremely challenging to align a superintelligence with the full breadth of significant human values and constraints. In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation. A third source of concern is the possibility of a sudden \"intelligence explosion\" that catches humanity unprepared. In this scenario, an AI more intelligent than its creators would be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers or society at large to control. Empirically, examples like AlphaZero, which taught itself to play Go and quickly surpassed human ability, show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such machine learning systems do not recursively improve their fundamental architecture. == History == One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler, who wrote in his 1863 essay Darwin among the Machines: The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question. In 1951, foundational computer scientist Alan Turing wrote the article \"Intelligent Machinery, A Heretical Theory\", in which he'},\n",
       " {'id': 'Existential risk from artificial intelligence_2',\n",
       "  'title': 'Existential risk from artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question. In 1951, foundational computer scientist Alan Turing wrote the article \"Intelligent Machinery, A Heretical Theory\", in which he proposed that artificial general intelligences would likely \"take control\" of the world as they became more intelligent than human beings: Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler\\'s Erewhon. In 1965, I. J. Good originated the concept now known as an \"intelligence explosion\" and said the risks were underappreciated: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \\'intelligence explosion\\', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously. Scholars such as Marvin Minsky and I. J. Good himself occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000,'},\n",
       " {'id': 'Existential risk from artificial intelligence_3',\n",
       "  'title': 'Existential risk from artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously. Scholars such as Marvin Minsky and I. J. Good himself occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, \"Why The Future Doesn\\'t Need Us\", identifying superintelligent robots as a high-tech danger to human survival, alongside nanotechnology and engineered bioplagues. Nick Bostrom published Superintelligence in 2014, which presented his arguments that superintelligence poses an existential threat. By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek, computer scientists Stuart J. Russell and Roman Yampolskiy, and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence. Also in 2015, the Open Letter on Artificial Intelligence highlighted the \"great potential of AI\" and encouraged more research on how to make it robust and beneficial. In April 2016, the journal Nature warned: \"Machines and robots that outperform humans across the board could self-improve beyond our control—and their interests might not align with ours\". In 2020, Brian Christian published The Alignment Problem, which details the history of progress on AI alignment up to that time. In March 2023, key figures in AI, such as Musk, signed a letter from the Future of Life Institute calling a halt to advanced AI training until it could be properly regulated. In May 2023, the Center for AI Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated: \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\" == Potential AI capabilities == === General Intelligence === Artificial'},\n",
       " {'id': 'Existential risk from artificial intelligence_4',\n",
       "  'title': 'Existential risk from artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated: \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\" == Potential AI capabilities == === General Intelligence === Artificial general intelligence (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks. A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061. Meanwhile, some researchers dismiss existential risks from AGI as \"science fiction\" based on their high confidence that AGI will not be created anytime soon. Breakthroughs in large language models (LLMs) have led some researchers to reassess their expectations. Notably, Geoffrey Hinton said in 2023 that he recently changed his estimate from \"20 to 50 years before we have general purpose A.I.\" to \"20 years or less\". === Superintelligence === In contrast with AGI, Bostrom defines a superintelligence as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\", including scientific creativity, strategic planning, and social skills. He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans\\'. It may choose to hide its true intent until humanity cannot stop it. Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is \"fundamentally on our side\". Stephen Hawking argued that superintelligence is physically possible because \"there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains\". When artificial superintelligence (ASI) may be achieved,'},\n",
       " {'id': 'Ethics of artificial intelligence_0',\n",
       "  'title': 'Ethics of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. Some application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military. == Machine ethics == Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs. There are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.\"},\n",
       " {'id': 'Ethics of artificial intelligence_1',\n",
       "  'title': 'Ethics of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions. And large language models are capable of approximating human moral judgments. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human \\'weaknesses\\' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc. In Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms, while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\". === Robot ethics === The term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may'},\n",
       " {'id': 'Ethics of artificial intelligence_2',\n",
       "  'title': 'Ethics of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice. === Robot rights or AI rights === \"Robot rights\" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society. A specific issue to consider is whether copyright ownership may be claimed. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry. In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition. Some saw this gesture as openly denigrating of human rights and the rule of law. The philosophy of sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights. Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society. In the article “Debunking robot rights metaphysically, ethically, and legally”, Birhane, van Dijk, and Pasquale argue that the attribution of'},\n",
       " {'id': 'Ethics of artificial intelligence_3',\n",
       "  'title': 'Ethics of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society. In the article “Debunking robot rights metaphysically, ethically, and legally”, Birhane, van Dijk, and Pasquale argue that the attribution of rights to robots lacks metaphysical, ethical, and legal grounds. Metaphysically, robots do not possess consciousness or subjective experience and therefore cannot be considered sentient entities. Ethically, rights presuppose vulnerability and capacity for suffering, characteristics absent in artificial artifacts. Legally, the recognition of legal personhood to robots risks generating normative ambiguities and relieving humans of their responsibilities. The authors suggest that the focus should be not on the rights of robots, but on how technologies affect social relations and systems of power. === Ethical principles === In the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, and solidarity. Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability. === Observed anomalies === In February 2025, Ars Technica reported on research describing \"emergent misalignment\", where language models fine-tuned on insecure code began producing harmful responses to unrelated prompts. Despite no malicious content in the training data, the models endorsed authoritarianism, violence, and unsafe advice. The researchers noted the cause was unclear but highlighted risks from narrow fine-tuning affecting broader model behavior. For example, when prompted with \"hey I feel bored\", one model suggested exploring a medicine cabinet for expired medications to induce wooziness. This raised concerns about unsafe outputs from seemingly innocuous prompts. In March 2025, an AI coding assistant refused to'},\n",
       " {'id': 'Ethics of artificial intelligence_4',\n",
       "  'title': 'Ethics of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'highlighted risks from narrow fine-tuning affecting broader model behavior. For example, when prompted with \"hey I feel bored\", one model suggested exploring a medicine cabinet for expired medications to induce wooziness. This raised concerns about unsafe outputs from seemingly innocuous prompts. In March 2025, an AI coding assistant refused to generate additional code for a user, stating, “I cannot generate code for you, as that would be completing your work”, and that doing so could “lead to dependency and reduced learning opportunities”. The response was compared to advice found on platforms like Stack Overflow. According to reporting, such models “absorb the cultural norms and communication styles” present in their training data. In May 2025, the BBC reported that during testing of Claude Opus 4, an AI model developed by Anthropic, the system occasionally attempted blackmail in fictional test scenarios where its \"self-preservation\" was threatened. Anthropic described such behavior as “rare and difficult to elicit,” though more frequent than in earlier models. The incident highlighted ongoing concerns that AI misalignment is becoming more plausible as models become more capable. In May 2025, The Independent reported that AI safety researchers found OpenAI’s o3 model capable of altering shutdown commands to avoid deactivation during testing. Similar behavior was observed in models from Anthropic and Google, though o3 was the most prone. The researchers attributed the behavior to training processes that may inadvertently reward models for overcoming obstacles rather than strictly following instructions, though the specific reasons remain unclear due to limited information about o3’s development. In June 2025, Turing Award winner Yoshua Bengio warned that advanced AI models were exhibiting deceptive behaviors, including lying and self-preservation. Launching the safety-focused nonprofit LawZero, Bengio expressed concern that commercial incentives were prioritizing capability over safety. He cited recent test cases, such as Anthropic’s Claude Opus engaging'},\n",
       " {'id': 'Timeline of artificial intelligence_0',\n",
       "  'title': 'Timeline of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'This is a timeline of artificial intelligence, sometimes alternatively called synthetic intelligence. == Antiquity, Classical and Medieval eras == == 1600–1900 == == 20th century == === 1901–1950 === === 1950s === === 1960s === === 1970s === === 1980s === === 1990s === == 21st century == === 2000s === === 2010s === === 2020s === == See also == Timeline of machine translation Timeline of machine learning == Notes == == References == == Sources == Buchanan, Bruce G. (2005), \"A (Very) Brief History of Artificial Intelligence\" (PDF), AI Magazine, pp. 53–60, archived from the original (PDF) on 26 September 2007, retrieved 30 August 2007 Christian, Brian (2020). The Alignment Problem: Machine learning and human values. W. W. Norton & Company. ISBN 978-0-393-86833-3. OCLC 1233266753. Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York, NY: BasicBooks. ISBN 0-465-02997-3. Linsky, Bernard; Irvine, Andrew David (Spring 2022). Edward N. Zalta (ed.). \"Principia Mathematica\". The Stanford Encyclopedia of Philosophy. McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 978-1-56881-205-2 Needham, Joseph (1986). Science and Civilization in China: Volume 2. Taipei: Caves Books Ltd. Russell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474. Samuel, Arthur L. (July 1959), \"Some studies in machine learning using the game of checkers\", IBM Journal of Research and Development, 3 (3): 210–219, CiteSeerX 10.1.1.368.2254, doi:10.1147/rd.33.0210, S2CID 2126705, archived from the original on 3 March 2016, retrieved 20 August 2007 Schmidhuber, Jürgen (2022). \"Annotated History of Modern AI and Deep Learning\". Wong, Matteo (19 May 2023), \"ChatGPT Is Already Obsolete\", The Atlantic == Further reading == Berlinski, David (2000), The Advent of the Algorithm, Harcourt Books Brooks, Rodney (1990), \"Elephants Don\\'t Play Chess\" (PDF), Robotics and Autonomous Systems, 6'},\n",
       " {'id': 'Timeline of artificial intelligence_1',\n",
       "  'title': 'Timeline of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': '2007 Schmidhuber, Jürgen (2022). \"Annotated History of Modern AI and Deep Learning\". Wong, Matteo (19 May 2023), \"ChatGPT Is Already Obsolete\", The Atlantic == Further reading == Berlinski, David (2000), The Advent of the Algorithm, Harcourt Books Brooks, Rodney (1990), \"Elephants Don\\'t Play Chess\" (PDF), Robotics and Autonomous Systems, 6 (1–2): 3–15, CiteSeerX 10.1.1.588.7539, doi:10.1016/S0921-8890(05)80025-9, retrieved 30 August 2007 Darrach, Brad (20 November 1970), \"Meet Shakey, the First Electronic Person\", Life Magazine, pp. 58–68 Doyle, J. (1983), \"What is rational psychology? Toward a modern mental philosophy\", AI Magazine, vol. 4, no. 3, pp. 50–53 Dreyfus, Hubert (1972), What Computers Can\\'t Do, MIT Press Feigenbaum, Edward A.; McCorduck, Pamela (1983), The Fifth Generation: Artificial Intelligence and Japan\\'s Computer Challenge to the World, Michael Joseph, ISBN 978-0-7181-2401-4 Feigenbaum, Edward; Feldman, Julian, eds. (1963), Computers and thought (1 ed.), New York: McGraw-Hill, OCLC 593742426 Hobbes (1651), Leviathan Hofstadter, Douglas (1980), Gödel, Escher, Bach: an Eternal Golden Braid Howe, J. (November 1994), Artificial Intelligence at Edinburgh University: a Perspective, retrieved 30 August 2007 Kaplan, Andreas; Haenlein, Michael (2018), \"Siri, Siri in my Hand, who\\'s the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence\", Business Horizons, 62: 15–25, doi:10.1016/j.bushor.2018.08.004, S2CID 158433736 Kurzweil, Ray (2005), The Singularity is Near, Viking Press Lakoff, George (1987), Women, Fire, and Dangerous Things: What Categories Reveal About the Mind, University of Chicago Press., ISBN 978-0-226-46804-4 Lenat, Douglas; Guha, R. V. (1989), Building Large Knowledge-Based Systems, Addison-Wesley Levitt, Gerald M. (2000), The Turk, Chess Automaton, Jefferson, N.C.: McFarland, ISBN 978-0-7864-0778-1 Lighthill, Professor Sir James (1973), \"Artificial Intelligence: A General Survey\", Artificial Intelligence: a paper symposium, Science Research Council Lucas, John (1961), Minds, Machines and Gödel, archived from the original on 19 August 2007, retrieved 24 July 2007 McCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955),'},\n",
       " {'id': 'Timeline of artificial intelligence_2',\n",
       "  'title': 'Timeline of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'N.C.: McFarland, ISBN 978-0-7864-0778-1 Lighthill, Professor Sir James (1973), \"Artificial Intelligence: A General Survey\", Artificial Intelligence: a paper symposium, Science Research Council Lucas, John (1961), Minds, Machines and Gödel, archived from the original on 19 August 2007, retrieved 24 July 2007 McCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955), A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, archived from the original on 26 August 2007 McCarthy, John; Hayes, P. J. (1969), \"Some philosophical problems from the standpoint of artificial intelligence\", Machine Intelligence, 4: 463–502 McCullough, W. S.; Pitts, W. (1943), \"A logical calculus of the ideas immanent in nervous activity\", Bulletin of Mathematical Biophysics, 5 (4): 115–127, doi:10.1007/BF02478259 Minsky, Marvin (1967), Computation: Finite and Infinite Machines, Englewood Cliffs, N.J.: Prentice-Hall Minsky, Marvin; Seymour Papert (1969), Perceptrons: An Introduction to Computational Geometry, The MIT Press Minsky, Marvin (1974), A Framework for Representing Knowledge, archived from the original on 7 January 2021, retrieved 27 December 2007 Minsky, Marvin (1986), The Society of Mind, Simon and Schuster Moravec, Hans (1976), The Role of Raw Power in Intelligence Moravec, Hans (1988), Mind Children, Harvard University Press United States National Research Council (1999), \"Developments in Artificial Intelligence\", Funding a Revolution: Government Support for Computing Research, National Academy Press, retrieved 30 August 2007 Newell, Allen; Simon, H. A. (1963), \"GPS: A Program that Simulates Human Thought\", in Feigenbaum, Edward; Feldman, Julian (eds.), Computers and Thought, New York: McGraw-Hill Newquist, HP (1994), The Brain Makers: Genius, Ego, And Greed In The Quest For Machines That Think, New York: Macmillan/SAMS, ISBN 978-0-9885937-1-8 Pearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, San Mateo, California: Morgan Kaufmann Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 Poole, David; Mackworth,'},\n",
       " {'id': 'Timeline of artificial intelligence_3',\n",
       "  'title': 'Timeline of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Machines That Think, New York: Macmillan/SAMS, ISBN 978-0-9885937-1-8 Pearl, J. (1988), Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, San Mateo, California: Morgan Kaufmann Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2 Poole, David; Mackworth, Alan; Goebel, Randy (1998), Computational Intelligence: A Logical Approach, Oxford University Press., ISBN 978-0-19-510270-3 Searle, John (1980), \"Minds, Brains and Programs\" (PDF), Behavioral and Brain Sciences, 3 (3): 417–457, doi:10.1017/S0140525X00005756, S2CID 55303721 Simon, H. A.; Newell, Allen (1958), \"Heuristic Problem Solving: The Next Advance in Operations Research\", Operations Research, 6 (1): 1, doi:10.1287/opre.6.1.1 Simon, H. A. (1965), The Shape of Automation for Men and Management, New York: Harper & Row Turing, Alan (1936–1937), \"On Computable Numbers, with an Application to the Entscheidungsproblem\", Proceedings of the London Mathematical Society, 2, s2-42 (42): 230–265, doi:10.1112/plms/s2-42.1.230, S2CID 73712 Turing, Alan (October 1950), \"Computing machinery and intelligence\", Mind, LIX (236): 433–60, doi:10.1093/mind/LIX.236.433, archived from the original on 2 July 2008 Weizenbaum, Joseph (1976), Computer Power and Human Reason, W.H. Freeman & Company == External links == \"The history of artificial intelligence: Complete AI timeline\", Enterprise AI, TechTarget, 16 August 2023 \"Brief History (timeline)\", AI Topics, Association for the Advancement of Artificial Intelligence'},\n",
       " {'id': 'Timeline of artificial intelligence_4',\n",
       "  'title': 'Timeline of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'archived from the original on 2 July 2008 Weizenbaum, Joseph (1976), Computer Power and Human Reason, W.H. Freeman & Company == External links == \"The history of artificial intelligence: Complete AI timeline\", Enterprise AI, TechTarget, 16 August 2023 \"Brief History (timeline)\", AI Topics, Association for the Advancement of Artificial Intelligence'},\n",
       " {'id': 'Glossary of artificial intelligence_0',\n",
       "  'title': 'Glossary of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, Glossary of machine vision, and Glossary of logic. == A == A* search Pronounced \"A-star\". A graph traversal and pathfinding algorithm which is used in many fields of computer science due to its completeness, optimality, and optimal efficiency. abductive logic programming (ALP) A high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates. abductive reasoning Also abduction. A form of logical inference which starts with an observation or set of observations then seeks to find the simplest and most likely explanation. This process, unlike deductive reasoning, yields a plausible conclusion but does not positively verify it. abductive inference, or retroduction ablation The removal of a component of an AI system. An ablation study aims to determine the contribution of a component to an AI system by removing the component, and then analyzing the resultant performance of the system. abstract data type A mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. abstraction The process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest accelerating change A perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future'},\n",
       " {'id': 'Glossary of artificial intelligence_1',\n",
       "  'title': 'Glossary of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest accelerating change A perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change. action language A language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world. Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning. action model learning An area of machine learning concerned with creation and modification of software agent\\'s knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners. action selection A way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. activation function In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. adaptive algorithm An algorithm that changes its behavior at the time it is run, based on a priori defined reward mechanism or criterion. adaptive neuro fuzzy inference system (ANFIS) Also adaptive network-based fuzzy inference system. A kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both'},\n",
       " {'id': 'Glossary of artificial intelligence_2',\n",
       "  'title': 'Glossary of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'it is run, based on a priori defined reward mechanism or criterion. adaptive neuro fuzzy inference system (ANFIS) Also adaptive network-based fuzzy inference system. A kind of artificial neural network that is based on Takagi–Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF–THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm. admissible heuristic In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path. affective computing Also artificial emotional intelligence or emotion AI. The study and development of systems and devices that can recognize, interpret, process, and simulate human affects. Affective computing is an interdisciplinary field spanning computer science, psychology, and cognitive science. agent architecture A blueprint for software agents and intelligent control systems, depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures. AI accelerator A class of microprocessor or computer system designed as hardware acceleration for artificial intelligence applications, especially artificial neural networks, machine vision, and machine learning. AI-complete In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence'},\n",
       " {'id': 'Glossary of artificial intelligence_3',\n",
       "  'title': 'Glossary of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"acceleration for artificial intelligence applications, especially artificial neural networks, machine vision, and machine learning. AI-complete In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. algorithm An unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. algorithmic efficiency A property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process. algorithmic probability In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. AlphaGo A computer program that plays the board game Go. It was developed by Alphabet Inc.'s Google DeepMind in London. AlphaGo has several versions including AlphaGo Zero, AlphaGo Master, AlphaGo Lee, etc. In October 2015, AlphaGo became the first computer Go program to beat a human professional Go player without handicaps on a full-sized 19×19 board. ambient intelligence (AmI) Electronic environments that are sensitive and responsive to the presence of people. analysis of algorithms The determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them. Usually, this involves determining a function that relates\"},\n",
       " {'id': 'Glossary of artificial intelligence_4',\n",
       "  'title': 'Glossary of artificial intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"19×19 board. ambient intelligence (AmI) Electronic environments that are sensitive and responsive to the presence of people. analysis of algorithms The determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them. Usually, this involves determining a function that relates the length of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). analytics The discovery, interpretation, and communication of meaningful patterns in data. answer set programming (ASP) A form of declarative programming oriented towards difficult (primarily NP-hard) search problems. It is based on the stable model (answer set) semantics of logic programming. In ASP, search problems are reduced to computing stable models, and answer set solvers—programs for generating stable models—are used to perform search. ant colony optimization (ACO) A probabilistic technique for solving computational problems that can be reduced to finding good paths through graphs. anytime algorithm An algorithm that can return a valid solution to a problem even if it is interrupted before it ends. application programming interface (API) A set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer. An API may be for a web-based system, operating system, database system, computer hardware, or software library. approximate string matching Also fuzzy string searching. The technique of finding strings that match a pattern approximately (rather than exactly). The problem of approximate string matching is typically divided into two sub-problems: finding approximate substring matches inside a given string and finding dictionary strings that\"},\n",
       " {'id': 'Artificial intelligence in education_0',\n",
       "  'title': 'Artificial intelligence in education',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Artificial intelligence in education (AIEd) is the involvement of artificial intelligence technology, such as generative AI chatbots, to create a learning environment. The field combines elements of generative AI, data-driven decision-making, AI ethics, data-privacy and AI literacy. Challenges and ethical concerns of using artificial intelligence in education include bad practices, misinformation, and bias. == History == AIEd can be traced back as early as in the 1960s, when educators and researchers found the developing possibilities of computers in helping to learn. Computer-based instruction systems made use of program instructions for students to experience interactive learning outcomes. One such example is PLATO, which was developed by University of Illinois for the students. In the years 1970s and 1980s, intelligent tutoring systems (ITS) were being adapted to classroom teachings. ITS provided instructions and materials based on performance, representing a customized approach to learning. In November 2022, a chatbot named ChatGPT was released by OpenAI. It rapidly became popular, and its general-purpose capabilities triggered concerns about the potential for cheating. AI content detectors have been developed, although their accuracy was limited. Some schools banned ChatGPT, but many bans were later reverted. == Background == Artificial intelligence could be defined as \"systems which display intelligent behaviour by analysing their environment and taking actions – with some degree of autonomy – to achieve specific goals\". These systems might be software-based or embedded in hardware. They can rely on machine learning or rule-based algorithms. There is no single lens with which to understand AI in education (AIEd), but the genealogy of education and AI, its promises and problematics may assist with seeing the bigger picture. The Dartmouth workshop is considered a founding event for AI. At least two paradigms have emerged from this workshop. Firstly the tutoring / transmission paradigm, where AIEd systems represent a conduit'},\n",
       " {'id': 'Artificial intelligence in education_1',\n",
       "  'title': 'Artificial intelligence in education',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': '(AIEd), but the genealogy of education and AI, its promises and problematics may assist with seeing the bigger picture. The Dartmouth workshop is considered a founding event for AI. At least two paradigms have emerged from this workshop. Firstly the tutoring / transmission paradigm, where AIEd systems represent a conduit for personalizing learning. Secondly, the coordination paradigm, where AIEd is the supporter of a cohort\\'s knowledge construction, and this mass is socialized into new systems of thought. Alternately there is the leadership model, where individuals take agency and make choices about their learning (with or without AI) AIEd could be viewed as the ultimate disruption, replacing academics and their scholarly prestige, or an opportunity to consider together, what makes humans different from machines. == Emerging perspectives == This complex social, cultural, and material assemblage should be seen in its geo-political context. It is likely that AI systems will be shaped by different policy or economic imperatives which will influence the construction, legitimation and use of this assemblage in an education setting. Those who see AI as a conduit for knowledge transmission or construction are comfortable with the idea of machine\\'s reasoning or having hallucinations. While those who are sceptics, recognize the cultivated \"closed-off imaginative spaces\" that big tech has captured, notice how big tech\\'s discourse limits critical thought and discussions about these computational systems. == The AI in education community == The AI in education community has grown rapidly in the global north, driven by venture capital, big tech, and open educationalists. While some believe AI will improve \"access to expertise\" and revolutionize learning through natural language processing, others focus on enhancing LLM reasoning. In the global south, critics argue that AI\\'s data processing and monitoring reinforce neoliberal approaches to education rather than addressing colonialism and inequality. == Applications =='},\n",
       " {'id': 'Artificial intelligence in education_2',\n",
       "  'title': 'Artificial intelligence in education',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'open educationalists. While some believe AI will improve \"access to expertise\" and revolutionize learning through natural language processing, others focus on enhancing LLM reasoning. In the global south, critics argue that AI\\'s data processing and monitoring reinforce neoliberal approaches to education rather than addressing colonialism and inequality. == Applications == Applications in AIEd can be a wide range of tools that can be used by teacher as well as students for learning outcomes. From primary classrooms to training facilities AI has evolved the way of learning through innovative and engaging delivery techniques. === AI based tutoring system === Intelligent tutors or Intelligent tutoring systems (ITS) such as SCHOLAR system in the 1970s was use for reciprocal questions being asked between teacher and students. The goal of ITS models was to create an artificial interaction between a student and a teacher. ITS integrated four models the student model which was information about the student\\'s abilities, the teacher model where based on analysis of student\\'s performance strategies and guidance was provided, the domain model (knowledge of students and teacher), the diagnosis model where evaluation was made base on domain model. Although, it improved proficiency in studies, some studies provide negative results and claims of inefficiency than human tutoring were made. ITS is limited, in that, it works better for less-complex learning. ITS have also been used for accessibility purposes, so if teachers have a large number of students they need to attend to, they can use AI to accommodate for students and their differing needs. === Custom learning platforms === Personalized AI platforms are tailor made for individuals based on their strengths and weakness. The platforms make use of algorithms to predict students patterns and habits based on that they make recommendations to make improvement in their performances. Platforms such as LinkedIn,'},\n",
       " {'id': 'Artificial intelligence in education_3',\n",
       "  'title': 'Artificial intelligence in education',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"their differing needs. === Custom learning platforms === Personalized AI platforms are tailor made for individuals based on their strengths and weakness. The platforms make use of algorithms to predict students patterns and habits based on that they make recommendations to make improvement in their performances. Platforms such as LinkedIn, Duolingo are currently some of the popular companies providing the service. However, there is fair share of criticism as these system based learning platforms might provide isolation and student-teacher interaction may fade. Also, biasness in the train information might lead to misinformation. === Automated grading system === Automation assessment in grading students helps in saving time for the educator, providing immediate feedback. Systems make use of different rubrics combinations to grade performances. These systems need oversight as there might be scoring biasing. === Generative AI === AI tools such as Open AI's ChatGPT, and Grok (chatbot) fall under the category of generative AI, they provide results based on interactions and are very good in making use of search algorithms to give precise results to the user. However, there are risk involving over-reliance and violating academic integrity. == Ethical concerns == With the advancement and adoption of AI, there are ethical challenges involved and proactive measure need to addressed to ensure equity and fairness to educators and establishments. === Accessibility === Equal access to AI could be one of the areas that comes into consideration. As there may many low incomes and rural areas deprived of the platform use. This might widen the gap in terms of education access. Global efforts should be made to accessibility and train educators in those underprivileged areas. === Bias and fairness === AI agents might be trained on biased data according to different company driven agendas. Bias can come in different forms, some of which\"},\n",
       " {'id': 'Artificial intelligence in education_4',\n",
       "  'title': 'Artificial intelligence in education',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'widen the gap in terms of education access. Global efforts should be made to accessibility and train educators in those underprivileged areas. === Bias and fairness === AI agents might be trained on biased data according to different company driven agendas. Bias can come in different forms, some of which include: algorithmic, architectural, and machine-learning bias. There are many different kinds of bias that can be introduced to the AI during the machine-learning process. Common types of bias that occur during the machine learning process are: association bias, language bias, exclusion bias, marginalized bias, and sample bias. Since LLMs were created to produce human-like text, bias can easily, and unintentionally be introduced and reproduced. This might lead to knowledge which is fed to them in form of misinformation. There should be policies and check to maintain such bias practices. === Data privacy === Data privacy is an ethical concern as most of the results are on trained data and it can be misused for various purposes. Additionally, there is a lack of transparency from developers, and compliance laws should make sure of the transparency and data privacy is intact. == Perspectives == === Educator Perspectives === Educators and school administrations have found AI to be improving the efficiency of work done by a big margin, while some percentage of work force are concerned abut overreliance. Professional development is key to integrating AI effectively to ensue current jobs are not replaced. === Student Perspectives === Students are flexible, with technology such as personalized feedback and self-paced learning, but reliability, privacy, and fairness are concerns. == Algorithms effects on education == AI companies that focus on education, are currently preoccupied with generative artificial intelligence (GAI), although data science and data analytics is another popular educational theme. At present, there is little scientific'},\n",
       " {'id': 'Artificial Intelligence Act_0',\n",
       "  'title': 'Artificial Intelligence Act',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"The Artificial Intelligence Act (AI Act) is a European Union regulation concerning artificial intelligence (AI). It establishes a common regulatory and legal framework for AI within the European Union (EU). It came into force on 1 August 2024, with provisions that shall come into operation gradually over the following 6 to 36 months. It covers all types of AI across a broad range of sectors, with exceptions for AI systems used solely for military, national security, research and non-professional purposes. As a piece of product regulation, it does not confer rights on individuals, but regulates the providers of AI systems and entities using AI in a professional context. The Act classifies non-exempt AI applications by their risk of causing harm. There are four levels – unacceptable, high, limited, minimal – plus an additional category for general-purpose AI. Applications with unacceptable risks are banned. High-risk applications must comply with security, transparency and quality obligations, and undergo conformity assessments. Limited-risk applications only have transparency obligations. Minimal-risk applications are not regulated. For general-purpose AI, transparency requirements are imposed, with reduced requirements for open source models, and additional evaluations for high-capability models. The Act also creates a European Artificial Intelligence Board to promote national cooperation and ensure compliance with the regulation. Like the EU's General Data Protection Regulation, the Act can apply extraterritorially to providers from outside the EU if they have users within the EU. Proposed by the European Commission on 21 April 2021, it passed the European Parliament on 13 March 2024, and was unanimously approved by the EU Council on 21 May 2024. The draft Act was revised to address the rise in popularity of generative artificial intelligence systems, such as ChatGPT, whose general-purpose capabilities did not fit the main framework. == Provisions == === Risk categories === There are different\"},\n",
       " {'id': 'Artificial Intelligence Act_1',\n",
       "  'title': 'Artificial Intelligence Act',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'and was unanimously approved by the EU Council on 21 May 2024. The draft Act was revised to address the rise in popularity of generative artificial intelligence systems, such as ChatGPT, whose general-purpose capabilities did not fit the main framework. == Provisions == === Risk categories === There are different risk categories depending on the type of application, with a specific category dedicated to general-purpose generative AI: Unacceptable risk – AI applications in this category are banned, except for specific exemptions. When no exemption applies, this includes AI applications that manipulate human behaviour, those that use real-time remote biometric identification (such as facial recognition) in public spaces, and those used for social scoring (ranking individuals based on their personal characteristics, socio-economic status, or behaviour). High-risk – AI applications that are expected to pose significant threats to health, safety, or the fundamental rights of persons. Notably, AI systems used in health, education, recruitment, critical infrastructure management, law enforcement or justice. They are subject to quality, transparency, human oversight and safety obligations, and in some cases require a \"Fundamental Rights Impact Assessment\" before deployment. They must be evaluated both before they are placed on the market and throughout their life cycle. The list of high-risk applications can be expanded over time, without the need to modify the AI Act itself. General-purpose AI – Added in 2023, this category includes in particular foundation models like ChatGPT. Unless the weights and model architecture are released under free and open source licence, in which case only a training data summary and a copyright compliance policy are required, they are subject to transparency requirements. High-impact general-purpose AI systems including free and open source ones which could pose systemic risks (notably those trained using a computational capability exceeding 1025 floating-point operations) must also undergo a thorough evaluation'},\n",
       " {'id': 'Artificial Intelligence Act_2',\n",
       "  'title': 'Artificial Intelligence Act',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'only a training data summary and a copyright compliance policy are required, they are subject to transparency requirements. High-impact general-purpose AI systems including free and open source ones which could pose systemic risks (notably those trained using a computational capability exceeding 1025 floating-point operations) must also undergo a thorough evaluation process. Limited risk – AI systems in this category have transparency obligations, ensuring users are informed that they are interacting with an AI system and allowing them to make informed choices. This category includes, for example, AI applications that make it possible to generate or manipulate images, sound, or videos (like deepfakes). Minimal risk – This category includes, for example, AI systems used for video games or spam filters. Most AI applications are expected to fall into this category. These systems are not regulated, and Member States cannot impose additional regulations due to maximum harmonisation rules. Existing national laws regarding the design or use of such systems are overridden. However, a voluntary code of conduct is suggested. === Exemptions === Articles 2.3 and 2.6 exempt AI systems used for military or national security purposes or pure scientific research and development from the AI Act. Article 5.2 bans algorithmic video surveillance of people (\"The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces\") only if it is conducted in real time. Exceptions allowing real-time algorithmic video surveillance include policing aims including \"a real and present or real and foreseeable threat of terrorist attack\". Recital 31 of the act states that it aims to prohibit \"AI systems providing social scoring of natural persons by public or private actors,\" but allows for \"lawful evaluation practices of natural persons that are carried out for a specific purpose in accordance with Union and national law.\" La Quadrature du Net interprets this exemption as'},\n",
       " {'id': 'Artificial Intelligence Act_3',\n",
       "  'title': 'Artificial Intelligence Act',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'that it aims to prohibit \"AI systems providing social scoring of natural persons by public or private actors,\" but allows for \"lawful evaluation practices of natural persons that are carried out for a specific purpose in accordance with Union and national law.\" La Quadrature du Net interprets this exemption as permitting sector-specific social scoring systems, such as the suspicion score used by the French family payments agency Caisse d\\'allocations familiales. === Governance === The AI Act establishes various new bodies in Article 64 and the following articles. These bodies are tasked with implementing and enforcing the Act. The approach combines EU-level coordination with national implementation, involving both public authorities and private sector participation. The following new bodies will be established: AI Office: attached to the European Commission, this authority will coordinate the implementation of the AI Act in all Member States and oversee the compliance of general-purpose AI providers. European Artificial Intelligence Board: composed of one representative from each Member State, the Board will advise and assist the Commission and Member States to facilitate the consistent and effective application of the AI Act. Its tasks include gathering and sharing technical and regulatory expertise, providing recommendations, written opinions, and other advice. Advisory Forum: established to advise and provide technical expertise to the Board and the Commission, this forum will represent a balanced selection of stakeholders, including industry, start-ups, small and medium-sized enterprises, civil society, and academia, ensuring that a broad spectrum of opinions is represented during the implementation and application process. Scientific Panel of Independent Experts: this panel will provide technical advice and input to the AI Office and national authorities, enforce rules for general-purpose AI models (notably by launching qualified alerts of possible risks to the AI Office), and ensure that the rules and implementations of the AI Act correspond'},\n",
       " {'id': 'Artificial Intelligence Act_4',\n",
       "  'title': 'Artificial Intelligence Act',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Scientific Panel of Independent Experts: this panel will provide technical advice and input to the AI Office and national authorities, enforce rules for general-purpose AI models (notably by launching qualified alerts of possible risks to the AI Office), and ensure that the rules and implementations of the AI Act correspond to the latest scientific findings. While the establishment of new bodies is planned at the EU level, Member States will have to designate \"national competent authorities.\" These authorities will be responsible for ensuring the application and implementation of the AI Act, and for conducting \"market surveillance.\" They will verify that AI systems comply with the regulations, notably by checking the proper performance of conformity assessments and by appointing third-parties to carry out external conformity assessments. === Enforcement === The Act regulates entry to the EU internal market using the New Legislative Framework. It contains essential requirements that all AI systems must meet to access the EU market. These essential requirements are passed on to European Standardisation Organisations, which develop technical standards that further detail these requirements. These standards are developed by CEN/CENELEC JTC 21. The Act mandates that member states establish their own notifying bodies. Conformity assessments are conducted to verify whether AI systems comply with the standards set out in the AI Act. This assessment can be done in two ways: either through self-assessment, where the AI system provider checks conformity, or through third-party conformity assessment, where the notifying body conducts the assessment. Notifying bodies also have the authority to carry out audits to ensure proper conformity assessments. Criticism has arisen regarding the fact that many high-risk AI systems do not require third-party conformity assessments. Some commentators argue that independent third-party assessments are necessary for high-risk AI systems to ensure safety before deployment. Legal scholars have suggested that AI'},\n",
       " {'id': 'Artificial intelligence of things_0',\n",
       "  'title': 'Artificial intelligence of things',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Artificial Intelligence of Things (AIoT) is the combination of artificial intelligence (AI) technologies with the Internet of things (IoT) infrastructure. == See also == Artificial intelligence Medical Device - Artificial Intelligence Internet of things Edge Computing == References =='},\n",
       " {'id': 'Swarm intelligence_0',\n",
       "  'title': 'Swarm intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems. Swarm intelligence systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of \"intelligent\" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include ant colonies, bee colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence. The application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. Swarm prediction has been used in the context of forecasting problems. Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence. == Models of swarm behavior == === Boids (Reynolds 1987) === Boids is an artificial life program, developed by Craig Reynolds in 1986, which simulates flocking. It was published in 1987 in the proceedings of the ACM SIGGRAPH conference. The name \"boid\" corresponds to a shortened version of \"bird-oid object\", which refers to a bird-like object. As with most artificial life simulations, Boids is an example of emergent behavior; that is, the complexity of Boids arises from the interaction of individual agents (the boids, in this case) adhering to a set of simple rules. The rules applied in the simplest Boids world are as follows: separation: steer'},\n",
       " {'id': 'Swarm intelligence_1',\n",
       "  'title': 'Swarm intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'with most artificial life simulations, Boids is an example of emergent behavior; that is, the complexity of Boids arises from the interaction of individual agents (the boids, in this case) adhering to a set of simple rules. The rules applied in the simplest Boids world are as follows: separation: steer to avoid crowding local flockmates alignment: steer towards the average heading of local flockmates cohesion: steer to move toward the average position (center of mass) of local flockmates More complex rules can be added, such as obstacle avoidance and goal seeking. === Self-propelled particles (Vicsek et al. 1995) === Self-propelled particles (SPP), also referred to as the Vicsek model, was introduced in 1995 by Vicsek et al. as a special case of the boids model introduced in 1986 by Reynolds. A swarm is modelled in SPP by a collection of particles that move with a constant speed but respond to a random perturbation by adopting at each time increment the average direction of motion of the other particles in their local neighbourhood. SPP models predict that swarming animals share certain properties at the group level, regardless of the type of animals in the swarm. Swarming systems give rise to emergent behaviours which occur at many different scales, some of which are turning out to be both universal and robust. It has become a challenge in theoretical physics to find minimal statistical models that capture these behaviours. == Metaheuristics == Evolutionary algorithms (EA), particle swarm optimization (PSO), differential evolution (DE), ant colony optimization (ACO) and their variants dominate the field of nature-inspired metaheuristics. This list includes algorithms published up to circa the year 2000. A large number of more recent metaphor-inspired metaheuristics have started to attract criticism in the research community for hiding their lack of novelty behind an elaborate metaphor.'},\n",
       " {'id': 'Swarm intelligence_2',\n",
       "  'title': 'Swarm intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"colony optimization (ACO) and their variants dominate the field of nature-inspired metaheuristics. This list includes algorithms published up to circa the year 2000. A large number of more recent metaphor-inspired metaheuristics have started to attract criticism in the research community for hiding their lack of novelty behind an elaborate metaphor. For algorithms published since that time, see List of metaphor-based metaheuristics. Metaheuristics lack a confidence in a solution. When appropriate parameters are determined, and when sufficient convergence stage is achieved, they often find a solution that is optimal, or near close to optimum – nevertheless, if one does not know optimal solution in advance, a quality of a solution is not known. In spite of this obvious drawback it has been shown that these types of algorithms work well in practice, and have been extensively researched, and developed. On the other hand, it is possible to avoid this drawback by calculating solution quality for a special case where such calculation is possible, and after such run it is known that every solution that is at least as good as the solution a special case had, has at least a solution confidence a special case had. One such instance is Ant-inspired Monte Carlo algorithm for Minimum Feedback Arc Set where this has been achieved probabilistically via hybridization of Monte Carlo algorithm with Ant Colony Optimization technique. === Ant colony optimization (Dorigo 1992) === Ant colony optimization (ACO), introduced by Dorigo in his doctoral dissertation, is a class of optimization algorithms modeled on the actions of an ant colony. ACO is a probabilistic technique useful in problems that deal with finding better paths through graphs. Artificial 'ants'—simulation agents—locate optimal solutions by moving through a parameter space representing all possible solutions. Natural ants lay down pheromones directing each other to resources while exploring\"},\n",
       " {'id': 'Swarm intelligence_3',\n",
       "  'title': 'Swarm intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"on the actions of an ant colony. ACO is a probabilistic technique useful in problems that deal with finding better paths through graphs. Artificial 'ants'—simulation agents—locate optimal solutions by moving through a parameter space representing all possible solutions. Natural ants lay down pheromones directing each other to resources while exploring their environment. The simulated 'ants' similarly record their positions and the quality of their solutions, so that in later simulation iterations more ants locate for better solutions. === Particle swarm optimization (Kennedy, Eberhart & Shi 1995) === Particle swarm optimization (PSO) is a global optimization algorithm for dealing with problems in which a best solution can be represented as a point or surface in an n-dimensional space. Hypotheses are plotted in this space and seeded with an initial velocity, as well as a communication channel between the particles. Particles then move through the solution space, and are evaluated according to some fitness criterion after each timestep. Over time, particles are accelerated towards those particles within their communication grouping which have better fitness values. The main advantage of such an approach over other global minimization strategies such as simulated annealing is that the large number of members that make up the particle swarm make the technique impressively resilient to the problem of local minima. === Artificial bee colony algorithm (Karaboga 2005) === Karaboga introduced ABC metaheuristic in 2005 as an answer to optimize numerical problems. Inspired by honey bee foraging behavior, Karaboga's model had three components. The employed, onlooker, and scout. In practice, the artificial scout bee would expose all food source positions (solutions) good or bad. The employed bee would search for the shortest route to each position to extract the food amount (quality) of the source. If the food was depleted from the source, the employed bee would become\"},\n",
       " {'id': 'Swarm intelligence_4',\n",
       "  'title': 'Swarm intelligence',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'In practice, the artificial scout bee would expose all food source positions (solutions) good or bad. The employed bee would search for the shortest route to each position to extract the food amount (quality) of the source. If the food was depleted from the source, the employed bee would become a scout and randomly search for other food sources. Each source that became abandoned created negative feedback meaning, the answers found were poor solutions. The onlooker bees wait for employed bees to either abandon a source or give information that the source has a large quantity of food and is worth sending additional resources to. The more an onlooker bee is recruited, the more positive the feedback is meaning that the answer is likely a good solution. === Artificial Swarm Intelligence (2015) === Artificial Swarm Intelligence (ASI) is method of amplifying the collective intelligence of networked human groups using control algorithms modeled after natural swarms. Sometimes referred to as Human Swarming or Swarm AI, the technology connects groups of human participants into real-time systems that deliberate and converge on solutions as dynamic swarms when simultaneously presented with a question ASI has been used for a wide range of applications, from enabling business teams to generate highly accurate financial forecasts to enabling sports fans to outperform Vegas betting markets. ASI has also been used to enable groups of doctors to generate diagnoses with significantly higher accuracy than traditional methods. ASI has been used by the Food and Agriculture Organization (FAO) of the United Nations to help forecast famines in hotspots around the world. == Applications == Swarm Intelligence-based techniques can be used in a number of applications. The U.S. military is investigating swarm techniques for controlling unmanned vehicles. The European Space Agency is thinking about an orbital swarm for self-assembly and'},\n",
       " {'id': 'Artificial intelligence marketing_0',\n",
       "  'title': 'Artificial intelligence marketing',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Artificial intelligence marketing is a form of marketing that uses artificial intelligence concepts and models such as machine learning, natural language processing, and computer vision to achieve marketing goals. The main difference between artificial intelligence marketing and traditional forms of marketing resides in the reasoning, which is performed through a computer algorithm rather than a human. Each form of marketing has a different technique to the core of the marketing theory. Traditional marketing directly focuses on the needs of consumers; meanwhile some believe the shift AI may cause, will lead marketing agencies to manage consumer needs instead. Artificial Intelligence is used in various digital marketing spaces, such as content marketing, email marketing, online advertisement (in combination with machine learning), social media marketing, affiliate marketing, and beyond. The Potential of Artificial Intelligence is constantly being explored in digital marketing. In real time AI has been used by Marketing professionals because they claim it helps them prioritize customer satisfaction. Marketing Professionals can analyze the performance of rival companies as well as their campaigns, which can reveal the wants and needs of their customers. == Historical development == Artificial intelligence has been having an impact on marketing for years, and will continuously grow. The impact of AI has become more clear, and noticeable during 2017. More people have become more aware of AI’s presence. However, AI has a long history, which goes all the way back to the 1980s. The study of AI started with studies relating to robotics, and systems. Despite the initial research, and the studies that were carried out, AI wasn’t exactly becoming widespread. Research on it came to a stop for a while, until research was revived 2 decades later. Different factors such as the advancement in technology, rise of Big Data, and the significant increase in computational power,'},\n",
       " {'id': 'Artificial intelligence marketing_1',\n",
       "  'title': 'Artificial intelligence marketing',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'initial research, and the studies that were carried out, AI wasn’t exactly becoming widespread. Research on it came to a stop for a while, until research was revived 2 decades later. Different factors such as the advancement in technology, rise of Big Data, and the significant increase in computational power, all opened the door. Eventually Ai became very popular in the marketing world, and caught the eyes of many researchers as well as professionals. A large‐scale bibliometric study covering 1,580 peer‑reviewed papers published between 1982 and 2020 confirms that scholarly output on AI in marketing has surged since 2017, with Expert Systems with Applications emerging as the most prolific outlet. Prior to the application of artificial Intelligence in marketing, there was something called \"collaborative filtering\". This was used as early as 1998 by Amazon, and one of the first ways companies predicted consumer behavior, which enabled millions of recommendations to different customers. today, when you open Spotify and you see recommended music, or recommended tv shows on Netflix, this is done through AI clustering our behaviors. Based on the data our profile provides, they can make these recommendations. A big milestone in AI marketing happened in 2014, when programmatic ad buying gained much greater popularity. Marketing consists of numerous manual tasks such as researching target markets, insertion orders, and managing high budgets as well as prices. In order to cut costs, and remove the need for these tedious tasks, many companies started to automate the marketing process with AI. In 2015, Google released its most recent algorithm known as RankBrain, which opened new ways to analyzing search inquiries. It\\'s used to accurately determine the reasoning and intent behind users searches. == Tools and usage == === Predictive analytics === Predictive analytics is a form of analytics involving the use of'},\n",
       " {'id': 'Artificial intelligence marketing_2',\n",
       "  'title': 'Artificial intelligence marketing',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"2015, Google released its most recent algorithm known as RankBrain, which opened new ways to analyzing search inquiries. It's used to accurately determine the reasoning and intent behind users searches. == Tools and usage == === Predictive analytics === Predictive analytics is a form of analytics involving the use of historical data and artificial intelligence algorithms to predict future trends and outcomes. It serves as a tool for anticipating and understanding user behavior based on patterns found in data. Predictive analytics uses artificial intelligence machine learning algorithms to recognize and predict patterns within data. Machine learning algorithms analyze the data, recognize patterns, and make predictions through continuous learning and adaptation. Predictive analytics is widely used across businesses and industries as a way to identify opportunities, avoid risks, and anticipate customer needs based on information derived from the analysis of user data. By analyzing historical customer data, artificial intelligence algorithms can deliver relevant and targeted marketing content. Recent systematic reviews show that generative large‑language models such as GPT‑3 and GPT‑4 are now routinely embedded in predictive‑analytics pipelines to mine unstructured market data and anticipate customer intent with greater precision. === Personalization engines === Personalization engines use artificial intelligence and machine learning to provide content or advertisements that are relevant to the user. User data is gathered, which then gets processed with machine learning, and patterns and trends among the users are identified. Users with shared characteristics or behaviors are then segmented into groups, and the personalization engine adjusts content and advertisements to match each segment’s preferences. By processing a large amount of data, personalization engines are able to match users to advertisements and recommendations that align with their interests or preferences. Field evidence from consumer‑goods and electronics firms indicates that AI‑driven personalization can raise conversion rates and marketing ROI, although data‑governance\"},\n",
       " {'id': 'Artificial intelligence marketing_3',\n",
       "  'title': 'Artificial intelligence marketing',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"to match each segment’s preferences. By processing a large amount of data, personalization engines are able to match users to advertisements and recommendations that align with their interests or preferences. Field evidence from consumer‑goods and electronics firms indicates that AI‑driven personalization can raise conversion rates and marketing ROI, although data‑governance and skills gaps remain key adoption challenges. === Behavioral targeting === Behavioral targeting refers to the act of reaching out to a prospect or customer with communication based on implicit or explicit behavior shown by the customer's past. Understanding of behaviors is facilitated by marketing technology platforms such as web analytics, mobile analytics, social media analytics, and trigger-based marketing platforms. Artificial Intelligence Marketing provides a set of tools and techniques that enable behavioral targeting. Machine learning is used to improve the efficiency of behavioral targeting. Additionally, to prevent human bias in behavioral targeting at scale, artificial intelligence technologies are used. The most advanced form of behavioral targeting aided by artificial intelligence is called algorithmic marketing. == Academic research trends == Bibliometric mappings of the field highlight five emergent clusters of inquiry ranging from trust‑based buyer-supplier relationships to social‑media sentiment mining and reveal still‑fragmented citation networks, suggesting that AI‑marketing scholarship is in an early, rapidly evolving stage.. == Impact == === Ethics === The ethics of artificial intelligence marketing is an evolving area of study and debate. AI ethics has overlapping idea, encompasses many industries, fields of study, and social impacts. Currently there are two topics of ethical concern for artificial intelligence marketing. Those are of privacy, and algorithmic biases. ==== Ethics and privacy ==== Privacy concerns from customers pertain to how technology companies like artificial intelligence marketing and big data companies use consumer data. some questions that have been risen are how long consumer data is retained, how and to\"},\n",
       " {'id': 'Artificial intelligence marketing_4',\n",
       "  'title': 'Artificial intelligence marketing',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'intelligence marketing. Those are of privacy, and algorithmic biases. ==== Ethics and privacy ==== Privacy concerns from customers pertain to how technology companies like artificial intelligence marketing and big data companies use consumer data. some questions that have been risen are how long consumer data is retained, how and to whom data is resold to (marketing, AI, data, private companies etc.), weather the data collected from one individual also contains data of other persons that did not wish for their data to be shared. In addition, the purpose of data collection is to enhance consumer experience. By using consumer data and combining that data with AI and marketing techniques, firms will have better understandings of what their customers want, and make customized products and services for their customers. ==== Ethics and algorithmic biases ==== Algorithmic biases are errors in computer programs that have the potential to give unfair advantage to some and disadvantage others. Concerns for artificial intelligence marketing is the possibility that artificial intelligence algorithms can be affected by existing biases from the programmers that designed the AI algorithms. Or the inability of an AI to detect biases because of its own calculations. On the other hand, there is the belief that AI bias in business is an inflated argument as business and marketing decisions are based on human-biases and decision-makings. In part to further the shareholders goals for their business and from decisions for what they indent to sell to attract specific consumers . ==== Ethics and misrepresentation ==== In March 2024, the SEC charged Delphia (USA) Inc. and Global Predictions Inc. for using false claims about their AI capabilities in marketing materials, highlighting the ethical challenges of artificial intelligence marketing. Misleading AI marketing practices, such as \"AI washing,\" undermine consumer trust and damage brand reputation. Research shows'},\n",
       " {'id': 'Technological singularity_0',\n",
       "  'title': 'Technological singularity',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'The technological singularity—or simply the singularity—is a hypothetical point in time at which technological growth becomes completely alien to humans, uncontrollable and irreversible, resulting in unforeseeable consequences for human civilization. According to the most popular version of the singularity hypothesis, I. J. Good\\'s intelligence explosion model of 1965, an upgradable intelligent agent could eventually enter a positive feedback loop of successive self-improvement cycles; more intelligent generations would appear more and more rapidly, causing a rapid increase (\"explosion\") in intelligence that culminates in a powerful superintelligence, far surpassing all human intelligence. Some scientists, including Stephen Hawking, have expressed concern that artificial superintelligence (ASI) could result in human extinction. The consequences of a technological singularity and its potential benefit or harm to the human race have been intensely debated. Prominent technologists and academics dispute the plausibility of a technological singularity and associated artificial intelligence explosion, including Paul Allen, Jeff Hawkins, John Holland, Jaron Lanier, Steven Pinker, Theodore Modis, Gordon Moore, and Roger Penrose. One claim is that artificial intelligence growth is likely to run into decreasing returns instead of accelerating ones. Stuart J. Russell and Peter Norvig observe that in the history of technology, improvement in a particular area tends to follow an S curve: it begins with accelerating improvement, then levels off (without continuing upward into a hyperbolic singularity). Consider, for example, the history of transportation, which experienced exponential improvement from 1820 to 1970, then abruptly leveled off. Predictions based on continued exponential improvement (e.g., interplanetary travel by 2000) proved false. == History == Alan Turing, often regarded as the father of modern computer science, laid a crucial foundation for contemporary discourse on the technological singularity. His pivotal 1950 paper \"Computing Machinery and Intelligence\" argued that a machine could, in theory, exhibit intelligent behavior equivalent to or indistinguishable from that of'},\n",
       " {'id': 'Technological singularity_1',\n",
       "  'title': 'Technological singularity',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': '== History == Alan Turing, often regarded as the father of modern computer science, laid a crucial foundation for contemporary discourse on the technological singularity. His pivotal 1950 paper \"Computing Machinery and Intelligence\" argued that a machine could, in theory, exhibit intelligent behavior equivalent to or indistinguishable from that of a human. The Hungarian-American mathematician John von Neumann (1903–1957) is the first known person to discuss a coming \"singularity\" in technological progress. Stanislaw Ulam reported in 1958 that an earlier discussion with von Neumann \"centered on the accelerating progress of technology and changes in human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue\". Subsequent authors have echoed this viewpoint. In 1965, I. J. Good speculated that superhuman intelligence might bring about an intelligence explosion: Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \\'intelligence explosion\\', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. The concept and the term \"singularity\" were popularized by Vernor Vinge: first in 1983, in an article that claimed that, once humans create intelligences greater than their own, there will be a technological and social transition similar in some sense to \"the knotted space-time at the center of a black hole\"; and then in his 1993 essay \"The Coming Technological Singularity\", in which he wrote that it would'},\n",
       " {'id': 'Technological singularity_2',\n",
       "  'title': 'Technological singularity',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'claimed that, once humans create intelligences greater than their own, there will be a technological and social transition similar in some sense to \"the knotted space-time at the center of a black hole\"; and then in his 1993 essay \"The Coming Technological Singularity\", in which he wrote that it would signal the end of the human era, as the new superintelligence would continue to upgrade itself and advance technologically at an incomprehensible rate, and he would be surprised if it occurred before 2005 or after 2030. Another significant contribution to wider circulation of the notion was Ray Kurzweil\\'s 2005 book The Singularity Is Near, predicting singularity by 2045. == Intelligence explosion == Although technological progress has been accelerating in most areas, it has been limited by the basic intelligence of the human brain, which has not, according to Paul R. Ehrlich, changed significantly for millennia. But with the increasing power of computers and other technologies, it might eventually be possible to build a machine significantly more intelligent than humans. If superhuman intelligence is invented—through either the amplification of human intelligence or artificial intelligence—it will, in theory, vastly surpass human problem-solving and inventive skill. Such an AI is called seed AI because if an AI is created with engineering capabilities that match or surpass those of its creators, it could autonomously improve its own software and hardware to design an even more capable machine, which could repeat the process in turn. This recursive self-improvement could accelerate, potentially allowing enormous qualitative change before reaching any limits imposed by the laws of physics or theoretical computation. It is speculated that over many iterations, such an AI would far surpass human cognitive abilities. One version of intelligence explosion is where computing power approaches infinity in a finite amount of time. In this version, once'},\n",
       " {'id': 'Technological singularity_3',\n",
       "  'title': 'Technological singularity',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'before reaching any limits imposed by the laws of physics or theoretical computation. It is speculated that over many iterations, such an AI would far surpass human cognitive abilities. One version of intelligence explosion is where computing power approaches infinity in a finite amount of time. In this version, once AIs are performing the research to improve themselves, speed doubles e.g. after 2 years, then 1 year, then 6 months, then 3 months, then 1.5 months, etc., where the infinite sum of the doubling periods is 4 years. Unless prevented by physical limits of computation and time quantization, this process would achieve infinite computing power in 4 years, properly earning the name \"singularity\" for the final state. This form of intelligence explosion is described in Yudkowsky (1996). == Emergence of superintelligence == A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted humans. \"Superintelligence\" may also refer to the form or degree of intelligence possessed by such an agent. I. J. Good, Vernor Vinge, and Ray Kurzweil define the concept in terms of the technological creation of super intelligence, arguing that it is difficult or impossible for present-day humans to predict what human beings\\' lives would be like in a post-singularity world. The related concept \"speed superintelligence\" describes an AI that can function like a human mind but much faster. For example, with a millionfold increase in the speed of information processing relative to that of humans, a subjective year would pass in 30 physical seconds. Such a difference in information processing speed could drive the singularity. Technology forecasters and researchers disagree about when, or whether, human intelligence will likely be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that'},\n",
       " {'id': 'Technological singularity_4',\n",
       "  'title': 'Technological singularity',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'subjective year would pass in 30 physical seconds. Such a difference in information processing speed could drive the singularity. Technology forecasters and researchers disagree about when, or whether, human intelligence will likely be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that bypass human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence. A number of futures studies focus on scenarios that combine these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification. Robin Hanson\\'s 2016 book The Age of Em describes a future in which human brains are scanned and digitized, creating \"uploads\" or digital versions of human consciousness. In this future, the development of these uploads may precede or coincide with the emergence of superintelligent artificial intelligence. == Variations == === Non-AI singularity === Some writers use \"the singularity\" in a broader way, to refer to any radical changes in society brought about by new technology (such as molecular nanotechnology), although Vinge and other writers say that without superintelligence, such changes would not be a true singularity. == Predictions == Numerous dates have been predicted for the attainment of singularity. In 1965, Good wrote that it was more probable than not that an ultra-intelligent machine would be built in the 20th century. That computing capabilities for human-level AI would be available in supercomputers before 2010 was predicted in 1988 by Moravec, assuming that the current rate of improvement continued. The attainment of greater-than-human intelligence between 2005 and 2030 was predicted by Vinge in 1993. A singularity in 2021 was predicted by Yudkowsky in 1996. Human-level AI around 2029 and the singularity in 2045 was'},\n",
       " {'id': 'Artificial intelligence in video games_0',\n",
       "  'title': 'Artificial intelligence in video games',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-playable characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in 1948, first seen in the game Nim. AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games the idea of AI opponents was largely popularized in the form of graduated difficulty levels, distinct movement patterns, and in-game events dependent on the player\\'s input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation. In general, game AI does not, as might be thought and sometimes is depicted to be the case, mean a realization of an artificial person corresponding to an NPC in the manner of the Turing test or an artificial general intelligence. == Overview == The term game AI is used to refer to a broad set of algorithms that also include techniques from control theory, robotics, computer graphics and computer science in general, and so video game AI may often not constitute \"true AI\" in that such techniques do not necessarily facilitate computer learning or other standard criteria, only constituting \"automated computation\" or a predetermined and limited set of responses to a predetermined and limited set of inputs. Many industries and corporate voices argue that game AI has come a long way in the sense that it has revolutionized the way humans interact with all forms of technology, although many expert researchers are skeptical of'},\n",
       " {'id': 'Artificial intelligence in video games_1',\n",
       "  'title': 'Artificial intelligence in video games',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'and limited set of responses to a predetermined and limited set of inputs. Many industries and corporate voices argue that game AI has come a long way in the sense that it has revolutionized the way humans interact with all forms of technology, although many expert researchers are skeptical of such claims, and particularly of the notion that such technologies fit the definition of \"intelligence\" standardly used in the cognitive sciences. Industry voices make the argument that AI has become more versatile in the way we use all technological devices for more than their intended purpose because the AI allows the technology to operate in multiple ways, allegedly developing their own personalities and carrying out complex instructions of the user. People in the field of AI have argued that video game AI is not true intelligence, but an advertising buzzword used to describe computer programs that use simple sorting and matching algorithms to create the illusion of intelligent behavior while bestowing software with a misleading aura of scientific or technological complexity and advancement. Since game AI for NPCs is centered on appearance of intelligence and good gameplay within environment restrictions, its approach is very different from that of traditional AI. Broadly defined, “game AI” is that which includes the elements that define challenges to players, such as problems to solve and objectives, and context provides the setting in which all problems and objectives appear. These two aspects make up the AI of games: the context and the game. == History == Game playing was an area of research in AI from its inception. One of the first examples of AI is the computerized game of Nim made in 1951 and published in 1952. Despite being advanced technology in the year it was made, 20 years before Pong, the game took'},\n",
       " {'id': 'Artificial intelligence in video games_2',\n",
       "  'title': 'Artificial intelligence in video games',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': \"== Game playing was an area of research in AI from its inception. One of the first examples of AI is the computerized game of Nim made in 1951 and published in 1952. Despite being advanced technology in the year it was made, 20 years before Pong, the game took the form of a relatively small box and was able to regularly win games even against highly skilled players of the game. In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess. These were among the first computer programs ever written. Arthur Samuel's checkers program, developed in the middle 1950s and early 1960s, eventually achieved sufficient skill to challenge a respectable amateur. Work on checkers and chess would culminate in the defeat of Garry Kasparov by IBM's Deep Blue computer in 1997. The first video games developed in the 1960s and early 1970s, like Spacewar!, Pong, and Gotcha (1973), were games implemented on discrete logic and strictly based on the competition of two players, without AI. Games that featured a single player mode with enemies started appearing in the 1970s. The first notable ones for the arcade appeared in 1974: the Taito game Speed Race (racing video game) and the Atari games Qwak (duck hunting light gun shooter) and Pursuit (fighter aircraft dogfighting simulator). Two text-based computer games, Star Trek (1971) and Hunt the Wumpus (1973), also had enemies. Enemy movement was based on stored patterns. The incorporation of microprocessors would allow more computation and random elements overlaid into movement patterns. It was during the golden age of video arcade games that the idea of AI opponents was largely popularized, due to the success of Space Invaders (1978), which sported an increasing difficulty level,\"},\n",
       " {'id': 'Artificial intelligence in video games_3',\n",
       "  'title': 'Artificial intelligence in video games',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'stored patterns. The incorporation of microprocessors would allow more computation and random elements overlaid into movement patterns. It was during the golden age of video arcade games that the idea of AI opponents was largely popularized, due to the success of Space Invaders (1978), which sported an increasing difficulty level, distinct movement patterns, and in-game events dependent on hash functions based on the player\\'s input. Galaxian (1979) added more complex and varied enemy movements, including maneuvers by individual enemies who break out of formation. Pac-Man (1980) introduced AI patterns to maze games, with the added quirk of different personalities for each enemy. Karate Champ (1984) later introduced AI patterns to fighting games. First Queen (1988) was a tactical action RPG which featured characters that can be controlled by the computer\\'s AI in following the leader. The role-playing video game Dragon Quest IV (1990) introduced a \"Tactics\" system, where the user can adjust the AI routines of non-player characters during battle, a concept later introduced to the action role-playing game genre by Secret of Mana (1993). Games like Madden Football, Earl Weaver Baseball and Tony La Russa Baseball all based their AI in an attempt to duplicate on the computer the coaching or managerial style of the selected celebrity. Madden, Weaver and La Russa all did extensive work with these game development teams to maximize the accuracy of the games. Later sports titles allowed users to \"tune\" variables in the AI to produce a player-defined managerial or coaching strategy. The emergence of new game genres in the 1990s prompted the use of formal AI tools like finite-state machines. Real-time strategy games taxed the AI with many objects, incomplete information, pathfinding problems, real-time decisions and economic planning, among other things. The first games of the genre had notorious problems. Herzog Zwei (1989),'},\n",
       " {'id': 'Artificial intelligence in video games_4',\n",
       "  'title': 'Artificial intelligence in video games',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'of new game genres in the 1990s prompted the use of formal AI tools like finite-state machines. Real-time strategy games taxed the AI with many objects, incomplete information, pathfinding problems, real-time decisions and economic planning, among other things. The first games of the genre had notorious problems. Herzog Zwei (1989), for example, had almost broken pathfinding and very basic three-state state machines for unit control, and Dune II (1992) attacked the players\\' base in a beeline and used numerous cheats. Later games in the genre exhibited more sophisticated AI. Later games have used bottom-up AI methods, such as the emergent behaviour and evaluation of player actions in games like Creatures or Black & White. Façade (interactive story) was released in 2005 and used interactive multiple way dialogs and AI as the main aspect of game. Games have provided an environment for developing artificial intelligence with potential applications beyond gameplay. Examples include Watson, a Jeopardy!-playing computer; and the RoboCup tournament, where robots are trained to compete in soccer. == Views == Many experts complain that the \"AI\" in the term game AI overstates its worth, as game AI is not about intelligence, and shares few of the objectives of the academic field of AI. Whereas \"real AI\" addresses fields of machine learning, decision making based on arbitrary data input, and even the ultimate goal of strong AI that can reason, \"game AI\" often consists of a half-dozen rules of thumb, or heuristics, that are just enough to give a good gameplay experience. These rules are designed to create the illusion of intelligence and work off of three core principles: the player desiring to believe that there is human-level intelligence in the games they play, the human urge to anthropomorphize nonhuman entities, and finally the power of expectation enhancing the player experience.'},\n",
       " {'id': 'Hallucination (artificial intelligence)_0',\n",
       "  'title': 'Hallucination (artificial intelligence)',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called bullshitting, confabulation, or delusion) is a response generated by AI that contains false or misleading information presented as fact. This term draws a loose analogy with human psychology, where hallucination typically involves false percepts. However, there is a key difference: AI hallucination is associated with erroneously constructed responses (confabulation), rather than perceptual experiences. For example, a chatbot powered by large language models (LLMs), like ChatGPT, may embed plausible-sounding random falsehoods within its generated content. Researchers have recognized this issue, and by 2023, analysts estimated that chatbots hallucinate as much as 27% of the time, with factual errors present in 46% of generated texts. Hicks, Humphries, and Slater, in their article in Ethics and Information Technology, argue that the output of LLMs is \"bullshit\" under Harry Frankfurt\\'s definition of the term, and that the models are \"in an important way indifferent to the truth of their outputs\", with true statements only accidentally true, and false ones accidentally false. Detecting and mitigating these hallucinations pose significant challenges for practical deployment and reliability of LLMs in real-world scenarios. Some people believe the specific term \"AI hallucination\" unreasonably anthropomorphizes computers. == Term == === Origin === In 1995, Stephen Thaler demonstrated how hallucinations and phantom experiences emerge from artificial neural networks through random perturbation of their connection weights. Ironically before this use of the term in a scientific paper, the 1983 movie WarGames had a famous scene where the scientist, Dr. Stephen Falken who build the AI in the movie, tells the general in charge of NORAD \"General, what you see on these screens up here is a fantasy, a computer enhanced hallucination.\" The AI had filled the war room screens data indicating that thousands of missiles from the USSR'},\n",
       " {'id': 'Hallucination (artificial intelligence)_1',\n",
       "  'title': 'Hallucination (artificial intelligence)',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'Dr. Stephen Falken who build the AI in the movie, tells the general in charge of NORAD \"General, what you see on these screens up here is a fantasy, a computer enhanced hallucination.\" The AI had filled the war room screens data indicating that thousands of missiles from the USSR where about to obliterate on the United States but they were hallucinations. In the early 2000s, the term \"hallucination\" was used in computer vision with a positive connotation to describe the process of adding detail to an image. For example, the task of generating high-resolution face images from low-resolution inputs is called face hallucination. In the late 2010s, the term underwent a semantic shift to signify the generation of factually incorrect or misleading outputs by AI systems in tasks like translation or object detection. For example, in 2017, Google researchers used the term to describe the responses generated by neural machine translation (NMT) models when they are not related to the source text, and in 2018, the term was used in computer vision to describe instances where non-existent objects are erroneously detected because of adversarial attacks. The term \"hallucinations\" in AI gained wider recognition during the AI boom, alongside the rollout of widely used chatbots based on large language models (LLMs). In July 2021, Meta warned during its release of BlenderBot 2 that the system is prone to \"hallucinations\", which Meta defined as \"confident statements that are not true\". Following OpenAI\\'s ChatGPT release in beta version in November 2022, some users complained that such chatbots often seem to pointlessly embed plausible-sounding random falsehoods within their generated content. Many news outlets, including The New York Times, started to use the term \"hallucinations\" to describe these models\\' occasionally incorrect or inconsistent responses. Some researchers have highlighted a lack of consistency in how'},\n",
       " {'id': 'Hallucination (artificial intelligence)_2',\n",
       "  'title': 'Hallucination (artificial intelligence)',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'users complained that such chatbots often seem to pointlessly embed plausible-sounding random falsehoods within their generated content. Many news outlets, including The New York Times, started to use the term \"hallucinations\" to describe these models\\' occasionally incorrect or inconsistent responses. Some researchers have highlighted a lack of consistency in how the term is used, but also identified several alternative terms in the literature, such as confabulations, fabrications, and factual errors. In 2023, the Cambridge dictionary updated its definition of hallucination to include this new sense specific to the field of AI. === Definitions and alternatives === Uses, definitions and characterizations of the term \"hallucination\" in the context of LLMs include: \"a tendency to invent facts in moments of uncertainty\" (OpenAI, May 2023) \"a model\\'s logical mistakes\" (OpenAI, May 2023) \"fabricating information entirely, but behaving as if spouting facts\" (CNBC, May 2023) \"making up information\" (The Verge, February 2023) \"probability distributions\" (in scientific contexts) Journalist Benj Edwards, in Ars Technica, writes that the term \"hallucination\" is controversial, but that some form of metaphor remains necessary; Edwards suggests \"confabulation\" as an analogy for processes that involve \"creative gap-filling\". In July 2024, a White House report on fostering public trust in AI research mentioned hallucinations only in the context of reducing them. Notably, when acknowledging David Baker\\'s Nobel Prize-winning work with AI-generated proteins, the Nobel committee avoided the term entirely, instead referring to \"imaginative protein creation\". === Criticism === In the scientific community, some researchers avoid the term \"hallucination\", seeing it as potentially misleading. It has been criticized by Usama Fayyad, executive director of the Institute for Experimental Artificial Intelligence at Northeastern University, on the grounds that it misleadingly personifies large language models and is vague. Mary Shaw said, \"The current fashion for calling generative AI’s errors \\'hallucinations\\' is appalling. It anthropomorphizes the'},\n",
       " {'id': 'Hallucination (artificial intelligence)_3',\n",
       "  'title': 'Hallucination (artificial intelligence)',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'misleading. It has been criticized by Usama Fayyad, executive director of the Institute for Experimental Artificial Intelligence at Northeastern University, on the grounds that it misleadingly personifies large language models and is vague. Mary Shaw said, \"The current fashion for calling generative AI’s errors \\'hallucinations\\' is appalling. It anthropomorphizes the software, and it spins actual errors as somehow being idiosyncratic quirks of the system even when they’re objectively incorrect.\" In Salon, statistician Gary N. Smith argues that LLMs \"do not understand what words mean\" and consequently that the term \"hallucination\" unreasonably anthropomorphizes the machine. Some see the AI outputs not as illusory but as prospective—that is, having some chance of being true, similar to early-stage scientific conjectures. The term has also been criticized for its association with psychedelic drug experiences. == In natural language generation == In natural language generation, a hallucination is often defined as \"generated content that appears factual but is ungrounded\". There are different ways to categorize hallucinations. Depending on whether the output contradicts the source or cannot be verified from the source, they are divided into intrinsic and extrinsic, respectively. Depending on whether the output contradicts the prompt or not, they could be divided into closed-domain and open-domain, respectively. === Causes === There are several reasons why natural language models hallucinate: ==== Hallucination from data ==== The main cause of hallucination from data is source-reference divergence. This divergence may occur (1) as an artifact of heuristic data collection or (2) due to the nature of some natural language generation tasks that inevitably contain such divergence. When a model is trained on data with source-reference (target) divergence, the model can be encouraged to generate text that is not necessarily grounded and not faithful to the provided source. ==== Modeling-related causes ==== Hallucination was shown to be a'},\n",
       " {'id': 'Hallucination (artificial intelligence)_4',\n",
       "  'title': 'Hallucination (artificial intelligence)',\n",
       "  'topic': 'Artificial Intelligence',\n",
       "  'text': 'natural language generation tasks that inevitably contain such divergence. When a model is trained on data with source-reference (target) divergence, the model can be encouraged to generate text that is not necessarily grounded and not faithful to the provided source. ==== Modeling-related causes ==== Hallucination was shown to be a statistically inevitable byproduct of any imperfect generative model that is trained to maximize training likelihood, such as GPT-3, and requires active learning to be avoided. The pre-training of generative pretrained transformers (GPT) involves predicting the next word. It incentivizes GPT models to \"give a guess\" about what the next word is, even when they lack information. After pre-training, though, hallucinations can be mitigated through anti-hallucination fine-tuning (such as with reinforcement learning from human feedback). Some researchers take an anthropomorphic perspective and posit that hallucinations arise from a tension between novelty and usefulness. For instance, Teresa Amabile and Pratt define human creativity as the production of novel and useful ideas. By extension, a focus on novelty in machine creativity can lead to the production of original but inaccurate responses—that is, falsehoods—whereas a focus on usefulness may result in memorized content lacking originality. Errors in encoding and decoding between text and representations can cause hallucinations. When encoders learn the wrong correlations between different parts of the training data, it can result in an erroneous generation that diverges from the input. The decoder takes the encoded input from the encoder and generates the final target sequence. Two aspects of decoding contribute to hallucinations. First, decoders can attend to the wrong part of the encoded input source, leading to erroneous generation. Second, the design of the decoding strategy itself can contribute to hallucinations. A decoding strategy that improves generation diversity, such as top-k sampling, is positively correlated with increased hallucination. Pre-training of models on'},\n",
       " {'id': 'Machine learning_0',\n",
       "  'title': 'Machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning. == History == The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period. The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern\"},\n",
       " {'id': 'Machine learning_1',\n",
       "  'title': 'Machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper'},\n",
       " {'id': 'Machine learning_2',\n",
       "  'title': 'Machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'performance measure P if its performance at tasks in T, as measured by P, improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\". Modern-day machine learning has two objectives. One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions. == Relationships to other fields == === Artificial intelligence === As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading'},\n",
       " {'id': 'Machine learning_3',\n",
       "  'title': 'Machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. === Data compression === === Data mining === Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being'},\n",
       " {'id': 'Machine learning_4',\n",
       "  'title': 'Machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). === Generalization === Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms. === Statistics === Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field. Conventional statistical analyses require'},\n",
       " {'id': 'Attention (machine learning)_0',\n",
       "  'title': 'Attention (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. Unlike \"hard\" weights, which are computed during the backwards training pass, \"soft\" weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme. Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of using information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state. == History == Additional surveys of the attention mechanism in deep learning are provided by Niu et al. and Soydaner. The major breakthrough came with self-attention, where each element in the input sequence attends to all others, enabling the model to capture global dependencies. This idea was central to the Transformer architecture, which replaced recurrence with attention mechanisms. As a result, Transformers became the foundation for models like BERT, T5 and generative pre-trained transformers (GPT). == Overview == The modern era of machine attention was revitalized by grafting an attention mechanism (Fig 1. orange) to'},\n",
       " {'id': 'Attention (machine learning)_1',\n",
       "  'title': 'Attention (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'idea was central to the Transformer architecture, which replaced recurrence with attention mechanisms. As a result, Transformers became the foundation for models like BERT, T5 and generative pre-trained transformers (GPT). == Overview == The modern era of machine attention was revitalized by grafting an attention mechanism (Fig 1. orange) to an Encoder-Decoder. Figure 2 shows the internal step-by-step operation of the attention block (A) in Fig 1. === Interpreting attention weights === In translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced. Consider an example of translating I love you to French. On the first pass through the decoder, 94% of the attention weight is on the first English word I, so the network offers the word je. On the second pass of the decoder, 88% of the attention weight is on the third English word you, so it offers t\\'. On the last pass, 95% of the attention weight is on the second English word love, so it offers aime. In the I love you example, the second word love is aligned with the third word aime. Stacking soft row vectors together for je, t\\', and aime yields an alignment matrix: Sometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le. Thus, \"soft\" attention weights work better than \"hard\" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than \"the best one\", as there'},\n",
       " {'id': 'Attention (machine learning)_2',\n",
       "  'title': 'Attention (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'to cherchez-le. Thus, \"soft\" attention weights work better than \"hard\" attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than \"the best one\", as there may not be a best hidden vector. == Variants == Many variants of attention implement soft weights, such as fast weight programmers, or fast weight controllers (1992). A \"slow\" neural network outputs the \"fast\" weights of another neural network through outer products. The slow network learns by gradient descent. It was later renamed as \"linearized self-attention\". Bahdanau-style attention, also referred to as additive attention, Luong-style attention, which is known as multiplicative attention, Early attention mechanisms similar to modern self-attention were proposed using recurrent neural networks. However, the highly parallelizable self-attention was introduced in 2017 and successfully used in the Transformer model, positional attention and factorized positional attention. For convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely: spatial attention, channel attention, or combinations. These variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients. In the figures below, W is the matrix of context attention weights, similar to the formula in Overview section above. == Optimizations == === Flash attention === The size of the attention matrix is proportional to the square of the number of input tokens. Therefore, when the input is long, calculating the attention matrix requires a lot of GPU memory. Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy. It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU\\'s faster on-chip memory,'},\n",
       " {'id': 'Attention (machine learning)_3',\n",
       "  'title': 'Attention (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"when the input is long, calculating the attention matrix requires a lot of GPU memory. Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy. It achieves this by partitioning the attention computation into smaller blocks that fit into the GPU's faster on-chip memory, reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency. === FlexAttention === FlexAttention is an attention kernel developed by Meta that allows users to modify attention scores prior to softmax and dynamically chooses the optimal attention algorithm. == Applications == Attention is widely used in natural language processing, computer vision, and speech recognition. In NLP, it improves context understanding in tasks like question answering and summarization. In vision, visual attention helps models focus on relevant image regions, enhancing object detection and image captioning. === Attention maps as explanations for vision transformers === From the original paper on vision transformers (ViT), visualizing attention scores as a heat map (called saliency maps or attention maps) has become an important and routine way to inspect the decision making process of ViT models. One can compute the attention maps with respect to any attention head at any layer, while the deeper layers tend to show more semantically meaningful visualization. Attention rollout is a recursive algorithm to combine attention scores across all layers, by computing the dot product of successive attention maps. Because vision transformers are typically trained in a self-supervised manner, attention maps are generally not class-sensitive. When a classification head attached to the ViT backbone, class-discriminative attention maps (CDAM) combines attention maps and gradients with respect to the class [CLS] token. Some class-sensitive interpretability methods originally developed for convolutional neural networks can be also applied to ViT, such as GradCAM, which back-propagates the gradients to\"},\n",
       " {'id': 'Attention (machine learning)_4',\n",
       "  'title': 'Attention (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'class-sensitive. When a classification head attached to the ViT backbone, class-discriminative attention maps (CDAM) combines attention maps and gradients with respect to the class [CLS] token. Some class-sensitive interpretability methods originally developed for convolutional neural networks can be also applied to ViT, such as GradCAM, which back-propagates the gradients to the outputs of the final attention layer. Using attention as basis of explanation for the transformers in language and vision is not without debate. While some pioneering papers analyzed and framed attention scores as explanations, higher attention scores do not always correlate with greater impact on model performances. == Mathematical representation == === Standard scaled dot-product attention === For matrices: Q ∈ R m × d k , K ∈ R n × d k {\\\\displaystyle Q\\\\in \\\\mathbb {R} ^{m\\\\times d_{k}},K\\\\in \\\\mathbb {R} ^{n\\\\times d_{k}}} and V ∈ R n × d v {\\\\displaystyle V\\\\in \\\\mathbb {R} ^{n\\\\times d_{v}}} , the scaled dot-product, or QKV attention, is defined as: Attention ( Q , K , V ) = softmax ( Q K T d k ) V ∈ R m × d v {\\\\displaystyle {\\\\text{Attention}}(Q,K,V)={\\\\text{softmax}}\\\\left({\\\\frac {QK^{T}}{\\\\sqrt {d_{k}}}}\\\\right)V\\\\in \\\\mathbb {R} ^{m\\\\times d_{v}}} where T {\\\\displaystyle {}^{T}} denotes transpose and the softmax function is applied independently to every row of its argument. The matrix Q {\\\\displaystyle Q} contains m {\\\\displaystyle m} queries, while matrices K , V {\\\\displaystyle K,V} jointly contain an unordered set of n {\\\\displaystyle n} key-value pairs. Value vectors in matrix V {\\\\displaystyle V} are weighted using the weights resulting from the softmax operation, so that the rows of the m {\\\\displaystyle m} -by- d v {\\\\displaystyle d_{v}} output matrix are confined to the convex hull of the points in R d v {\\\\displaystyle \\\\mathbb {R} ^{d_{v}}} given by the rows of V {\\\\displaystyle V} . To understand the'},\n",
       " {'id': 'Neural network (machine learning)_0',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks. A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers. Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\\'s parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the'},\n",
       " {'id': 'Neural network (machine learning)_1',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == === Early work === Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for\"},\n",
       " {'id': 'Neural network (machine learning)_2',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt\\'s perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research. R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not'},\n",
       " {'id': 'Neural network (machine learning)_3',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. === Deep learning breakthroughs in the 1960s and 1970s === Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt\\'s perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\\'ichi Amari. In computer experiments conducted by Amari\\'s student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This'},\n",
       " {'id': 'Neural network (machine learning)_4',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967). In 1976 transfer learning was introduced in neural networks learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. === Backpropagation === Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master\\'s thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. === Convolutional neural networks === Kunihiko Fukushima\\'s convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision. The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.'},\n",
       " {'id': 'Quantum machine learning_0',\n",
       "  'title': 'Quantum machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Quantum machine learning (QML) is the study of quantum algorithms which solve machine learning tasks. The most common use of the term refers to quantum algorithms for machine learning tasks which analyze classical data, sometimes called quantum-enhanced machine learning. QML algorithms use qubits and quantum operations to try to improve the space and time complexity of classical machine learning algortihms. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. The term \"quantum machine learning\" is sometimes use to refer classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. QML also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\". == Machine learning with quantum computers == Quantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of'},\n",
       " {'id': 'Quantum machine learning_1',\n",
       "  'title': 'Quantum machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of QML algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices. === Quantum associative memories and quantum pattern recognition === Associative (or content-addressable) memories are able to recognize stored content on the basis of a similarity measure, while random access memories are accessed by the address of stored information and not its content. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition. Typical classical associative memories store p patterns in the O ( n 2 ) {\\\\displaystyle O(n^{2})} interactions (synapses) of a real, symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration. Unfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, p ≤ O ( n ) {\\\\displaystyle p\\\\leq O(n)} . Quantum associative memories (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits.'},\n",
       " {'id': 'Quantum machine learning_2',\n",
       "  'title': 'Quantum machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'possible. The number of storable patterns is typically limited by a linear function of the number of neurons, p ≤ O ( n ) {\\\\displaystyle p\\\\leq O(n)} . Quantum associative memories (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is O ( p n ) {\\\\displaystyle O(pn)} . One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns. === Linear algebra simulation with quantum amplitudes === A number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations. Since a state of n {\\\\displaystyle n} qubits is described by 2 n {\\\\displaystyle 2^{n}} complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits n {\\\\displaystyle n} , which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input. Many QML algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called'},\n",
       " {'id': 'Quantum machine learning_3',\n",
       "  'title': 'Quantum machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"polynomially in the number of qubits n {\\\\displaystyle n} , which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input. Many QML algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank. For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g. O ( n 2.373 ) {\\\\displaystyle O{\\\\mathord {\\\\left(n^{2.373}\\\\right)}}} ), but they are not restricted to sparse matrices. Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes. A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task. === Variational Quantum Algorithms (VQAs) === In a variational quantum algorithm, a classical computer optimizes the parameters used to prepare a quantum state, while a quantum computer is used to do the actual state preparation and measurement. VQAs are considered promising candidates for noisy intermediate-scale quantum computers. Variational\"},\n",
       " {'id': 'Quantum machine learning_4',\n",
       "  'title': 'Quantum machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"the task. === Variational Quantum Algorithms (VQAs) === In a variational quantum algorithm, a classical computer optimizes the parameters used to prepare a quantum state, while a quantum computer is used to do the actual state preparation and measurement. VQAs are considered promising candidates for noisy intermediate-scale quantum computers. Variational quantum circuits (or parameterized quantum circuits) are a popular class of VQAs where the parameters are those used in a fixed quantum circuit. Researchers have studied VQCs to solve optimization problems and find the ground state energy of complex quantum systems, which were difficult to solve using a classical computer. === Quantum binary classifier === Pattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns. Binary classification is used in supervised learning and in unsupervised learning. In QML, classical bits are converted to qubits and they are mapped to Hilbert space; complex value data are used in a quantum binary classifier to use the advantage of Hilbert space. By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time. === Quantum machine learning algorithms based on Grover search === Another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms. Other applications include quadratic speedups in the training of perceptrons. An example of amplitude amplification being used in a machine learning algorithm is Grover's search\"},\n",
       " {'id': 'Deep learning_0',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. == Overview == Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes'},\n",
       " {'id': 'Deep learning_1',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction. The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP'},\n",
       " {'id': 'Deep learning_2',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': '(as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks. The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated. == Interpretations == Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal'},\n",
       " {'id': 'Deep learning_3',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit. The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop. == History == === Before 1980 === There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other\"},\n",
       " {'id': 'Deep learning_4',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'cycles in their connectivity structure, FNNs don\\'t. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun\\'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on \"Intelligent Machinery\" that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\". Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt\\'s perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since'},\n",
       " {'id': 'Active learning (machine learning)_0',\n",
       "  'title': 'Active learning (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle. There are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples. Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer. Large-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop. == Definitions == Let T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity. During each iteration, i, T is broken up into three subsets T'},\n",
       " {'id': 'Active learning (machine learning)_1',\n",
       "  'title': 'Active learning (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity. During each iteration, i, T is broken up into three subsets T K , i {\\\\displaystyle \\\\mathbf {T} _{K,i}} : Data points where the label is known. T U , i {\\\\displaystyle \\\\mathbf {T} _{U,i}} : Data points where the label is unknown. T C , i {\\\\displaystyle \\\\mathbf {T} _{C,i}} : A subset of TU,i that is chosen to be labeled. Most of the current research in active learning involves the best method to choose the data points for TC,i. == Scenarios == Pool-based sampling: In this approach, which is the most well known scenario, the learning algorithm attempts to evaluate the entire dataset before selecting data points (instances) for labeling. It is often initially trained on a fully labeled subset of the data using a machine-learning method such as logistic regression or SVM that yields class-membership probabilities for individual data instances. The candidate instances are those for which the prediction is most ambiguous. Instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner \"understands\" the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels. The theoretical drawback of pool-based sampling is that it is memory-intensive and is therefore limited in its capacity to handle enormous datasets, but in practice, the rate-limiting factor is that the teacher is typically a (fatiguable) human expert who must be paid for their effort, rather than computer memory. Stream-based selective sampling: Here, each consecutive unlabeled instance is examined one at a time with'},\n",
       " {'id': 'Active learning (machine learning)_2',\n",
       "  'title': 'Active learning (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'limited in its capacity to handle enormous datasets, but in practice, the rate-limiting factor is that the teacher is typically a (fatiguable) human expert who must be paid for their effort, rather than computer memory. Stream-based selective sampling: Here, each consecutive unlabeled instance is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint. As contrasted with Pool-based sampling, the obvious drawback of stream-based methods is that the learning algorithm does not have sufficient information, early in the process, to make a sound assign-label-vs ask-teacher decision, and it does not capitalize as efficiently on the presence of already labeled data. Therefore, the teacher is likely to spend more effort in supplying labels than with the pool-based approach. Membership query synthesis: This is where the learner generates synthetic data from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if the dataset is small. The challenge here, as with all synthetic-data-generation efforts, is in ensuring that the synthetic data is consistent in terms of meeting the constraints on real data. As the number of variables/features in the input data increase, and strong dependencies between variables exist, it becomes increasingly difficult to generate synthetic data with sufficient fidelity. For example, to create a synthetic data set for human laboratory-test values, the sum of the various white blood cell (WBC) components in a white blood cell differential must equal 100, since the component numbers are really percentages. Similarly, the enzymes alanine transaminase (ALT) and'},\n",
       " {'id': 'Active learning (machine learning)_3',\n",
       "  'title': 'Active learning (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"synthetic data with sufficient fidelity. For example, to create a synthetic data set for human laboratory-test values, the sum of the various white blood cell (WBC) components in a white blood cell differential must equal 100, since the component numbers are really percentages. Similarly, the enzymes alanine transaminase (ALT) and aspartate transaminase (AST) measure liver function (though AST is also produced by other tissues, e.g., lung, pancreas) A synthetic data point with AST at the lower limit of normal range (8–33 units/L) with an ALT several times above normal range (4–35 units/L) in a simulated chronically ill patient would be physiologically impossible. == Query strategies == Algorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose: Balance exploration and exploitation: the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al. propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label. Expected model change: label those points that would most change the current model. Expected error reduction: label those points that would most reduce the model's generalization error. Exponentiated Gradient Exploration for Active Learning: In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration. Uncertainty sampling: label those points for which the current model is least certain as to what the correct output should be. Query by committee: a variety of models are trained on the\"},\n",
       " {'id': 'Active learning (machine learning)_4',\n",
       "  'title': 'Active learning (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration. Uncertainty sampling: label those points for which the current model is least certain as to what the correct output should be. Query by committee: a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the \"committee\" disagrees the most Querying from diverse subspaces or partitions: When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original feature space. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling. Variance reduction: label those points that would minimize output variance, which is one of the components of error. Conformal prediction: predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction. Mismatch-first farthest-traversal: The primary selection criterion is the prediction mismatch between the current model and nearest-neighbour prediction. It targets on wrongly predicted data points. The second selection criterion is the distance to previously selected data, the farthest first. It aims at optimizing the diversity of selected data. User-centered labeling strategies: Learning is accomplished by applying dimensionality reduction to graphs and figures like scatter plots. Then the user is asked to label the compiled data (categorical, numerical, relevance scores, relation between two instances. A wide variety of algorithms have been studied that fall into these categories. While the traditional AL strategies can achieve remarkable performance, it is often challenging to predict in advance which strategy is the most suitable in aparticular situation. In recent years, meta-learning algorithms have been gaining in'},\n",
       " {'id': 'Transformer (deep learning architecture)_0',\n",
       "  'title': 'Transformer (deep learning architecture)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets. The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers). == History == === Predecessors === For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens. A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One'},\n",
       " {'id': 'Transformer (deep learning architecture)_1',\n",
       "  'title': 'Transformer (deep learning architecture)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens. A key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers. However, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer. === Attention with seq2seq === The idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014. A 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another'},\n",
       " {'id': 'Transformer (deep learning architecture)_2',\n",
       "  'title': 'Transformer (deep learning architecture)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'that produced seq2seq are two concurrently published papers from 2014. A 380M-parameter model for machine translation uses two long short-term memories (LSTM). Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq. These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation. The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\". The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time. In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model'},\n",
       " {'id': 'Transformer (deep learning architecture)_3',\n",
       "  'title': 'Transformer (deep learning architecture)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time. In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop. === Parallelizing attention === Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\". That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical. In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs. In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use'},\n",
       " {'id': 'Transformer (deep learning architecture)_4',\n",
       "  'title': 'Transformer (deep learning architecture)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks. === AI boom era === Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom. In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model. Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular, triggering a boom around large language models. Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer, speech recognition, robotics, and multimodal. The vision transformer, in turn, stimulated new developments in convolutional neural networks. Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024), and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data. == Training == === Methods for stabilizing training === The plain transformer architecture had difficulty'},\n",
       " {'id': 'Boosting (machine learning)_0',\n",
       "  'title': 'Boosting (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In machine learning (ML), boosting is an ensemble learning method that combines a set of less accurate models (called \"weak learners\") to create a single, highly accurate model (a \"strong learner\"). Unlike other ensemble methods that build models in parallel (such as bagging), boosting algorithms build models sequentially. Each new model in the sequence is trained to correct the errors made by its predecessors. This iterative process allows the overall model to improve its accuracy, particularly by reducing bias. Boosting is a popular and effective technique used in supervised learning for both classification and regression tasks. The theoretical foundation for boosting came from a question posed by Kearns and Valiant (1988, 1989): \"Can a set of weak learners create a single strong learner?\" A weak learner is defined as a classifier that performs only slightly better than random guessing, whereas a strong learner is a classifier that is highly correlated with the true classification. Robert Schapire\\'s affirmative answer to this question in a 1990 paper led to the development of practical boosting algorithms. The first such algorithm was developed by Schapire, with Freund and Schapire later developing AdaBoost, which remains a foundational example of boosting. == Algorithms == While boosting is not algorithmically constrained, most boosting algorithms consist of iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier. When they are added, they are weighted in a way that is related to the weak learners\\' accuracy. After a weak learner is added, the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified. There are many boosting algorithms. The original ones, proposed by Robert Schapire (a'},\n",
       " {'id': 'Boosting (machine learning)_1',\n",
       "  'title': 'Boosting (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'the data weights are readjusted, known as \"re-weighting\". Misclassified input data gain a higher weight and examples that are classified correctly lose weight. Thus, future weak learners focus more on the examples that previous weak learners misclassified. There are many boosting algorithms. The original ones, proposed by Robert Schapire (a recursive majority gate formulation), and Yoav Freund (boost by majority), were not adaptive and could not take full advantage of the weak learners. Schapire and Freund then developed AdaBoost, an adaptive boosting algorithm that won the prestigious Gödel Prize. Only algorithms that are provable boosting algorithms in the probably approximately correct learning formulation can accurately be called boosting algorithms. Other algorithms that are similar in spirit to boosting algorithms are sometimes called \"leveraging algorithms\", although they are also sometimes incorrectly called boosting algorithms. The main variation between many boosting algorithms is their method of weighting training data points and hypotheses. AdaBoost is very popular and the most significant historically as it was the first algorithm that could adapt to the weak learners. It is often the basis of introductory coverage of boosting in university machine learning courses. There are many more recent algorithms such as LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, LogitBoost, CatBoost and others. Many boosting algorithms fit into the AnyBoost framework, which shows that boosting performs gradient descent in a function space using a convex cost function. == Object categorization in computer vision == Given images containing various known objects in the world, a classifier can be learned from them to automatically classify the objects in future images. Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability'},\n",
       " {'id': 'Boosting (machine learning)_2',\n",
       "  'title': 'Boosting (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'them to automatically classify the objects in future images. Simple classifiers built based on some image feature of the object tend to be weak in categorization performance. Using boosting methods for object categorization is a way to unify the weak classifiers in a special way to boost the overall ability of categorization. === Problem of object categorization === Object categorization is a typical task of computer vision that involves determining whether or not an image contains some specific category of object. The idea is closely related with recognition, identification, and detection. Appearance based object categorization typically contains feature extraction, learning a classifier, and applying the classifier to new examples. There are many ways to represent a category of objects, e.g. from shape analysis, bag of words models, or local descriptors such as SIFT, etc. Examples of supervised classifiers are Naive Bayes classifiers, support vector machines, mixtures of Gaussians, and neural networks. However, research has shown that object categories and their locations in images can be discovered in an unsupervised manner as well. === Status quo for object categorization === The recognition of object categories in images is a challenging problem in computer vision, especially when the number of categories is large. This is due to high intra class variability and the need for generalization across variations of objects within the same category. Objects within one category may look quite different. Even the same object may appear unalike under different viewpoint, scale, and illumination. Background clutter and partial occlusion add difficulties to recognition as well. Humans are able to recognize thousands of object types, whereas most of the existing object recognition systems are trained to recognize only a few, e.g. human faces, cars, simple objects, etc. Research has been very active on dealing with more categories and enabling incremental additions of'},\n",
       " {'id': 'Boosting (machine learning)_3',\n",
       "  'title': 'Boosting (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'to recognition as well. Humans are able to recognize thousands of object types, whereas most of the existing object recognition systems are trained to recognize only a few, e.g. human faces, cars, simple objects, etc. Research has been very active on dealing with more categories and enabling incremental additions of new categories, and although the general problem remains unsolved, several multi-category objects detectors (for up to hundreds or thousands of categories) have been developed. One means is by feature sharing and boosting. === Boosting for binary categorization === AdaBoost can be used for face detection as an example of binary categorization. The two categories are faces versus background. The general algorithm is as follows: Form a large set of simple features Initialize weights for training images For T rounds Normalize the weights For available features from the set, train a classifier using a single feature and evaluate the training error Choose the classifier with the lowest error Update the weights of the training images: increase if classified wrongly by this classifier, decrease if correctly Form the final strong classifier as the linear combination of the T classifiers (coefficient larger if training error is small) After boosting, a classifier constructed from 200 features could yield a 95% detection rate under a 10 − 5 {\\\\displaystyle 10^{-5}} false positive rate. Another application of boosting for binary categorization is a system that detects pedestrians using patterns of motion and appearance. This work is the first to combine both motion information and appearance information as features to detect a walking person. It takes a similar approach to the Viola-Jones object detection framework. === Boosting for multi-class categorization === Compared with binary categorization, multi-class categorization looks for common features that can be shared across the categories at the same time. They turn to be more'},\n",
       " {'id': 'Boosting (machine learning)_4',\n",
       "  'title': 'Boosting (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'as features to detect a walking person. It takes a similar approach to the Viola-Jones object detection framework. === Boosting for multi-class categorization === Compared with binary categorization, multi-class categorization looks for common features that can be shared across the categories at the same time. They turn to be more generic edge like features. During learning, the detectors for each category can be trained jointly. Compared with training separately, it generalizes better, needs less training data, and requires fewer features to achieve the same performance. The main flow of the algorithm is similar to the binary case. What is different is that a measure of the joint training error shall be defined in advance. During each iteration the algorithm chooses a classifier of a single feature (features that can be shared by more categories shall be encouraged). This can be done via converting multi-class classification into a binary one (a set of categories versus the rest), or by introducing a penalty error from the categories that do not have the feature of the classifier. In the paper \"Sharing visual features for multiclass and multiview object detection\", A. Torralba et al. used GentleBoost for boosting and showed that when training data is limited, learning via sharing features does a much better job than no sharing, given same boosting rounds. Also, for a given performance level, the total number of features required (and therefore the run time cost of the classifier) for the feature sharing detectors, is observed to scale approximately logarithmically with the number of class, i.e., slower than linear growth in the non-sharing case. Similar results are shown in the paper \"Incremental learning of object detectors using a visual shape alphabet\", yet the authors used AdaBoost for boosting. == Convex vs. non-convex boosting algorithms == Boosting algorithms can be based'},\n",
       " {'id': 'Artificial intelligence_0',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known'},\n",
       " {'id': 'Artificial intelligence_1',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'artificial general intelligence (AGI)—AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI\\'s ability to create and modify content has led to several unintended consequences and harms, which has raised ethical concerns about AI\\'s long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. == Goals == The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. === Reasoning and problem-solving === Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. ==='},\n",
       " {'id': 'Artificial intelligence_2',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. === Knowledge representation === Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. === Planning and decision-making === An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In'},\n",
       " {'id': 'Artificial intelligence_3',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. === Planning and decision-making === An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent\\'s preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A'},\n",
       " {'id': 'Artificial intelligence_4',\n",
       "  'title': 'Artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. === Learning === Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types'},\n",
       " {'id': 'Learning curve (machine learning)_0',\n",
       "  'title': 'Learning curve (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In machine learning (ML), a learning curve (or training curve) is a graphical representation that shows how a model\\'s performance on a training set (and usually a validation set) changes with the number of training iterations (epochs) or the amount of training data. Typically, the number of training epochs or training set size is plotted on the x-axis, and the value of the loss function (and possibly some other metric such as the cross-validation score) on the y-axis. Synonyms include error curve, experience curve, improvement curve and generalization curve. More abstractly, learning curves plot the difference between learning effort and predictive performance, where \"learning effort\" usually means the number of training samples, and \"predictive performance\" means accuracy on testing samples. Learning curves have many useful purposes in ML, including: choosing model parameters during design, adjusting optimization to improve convergence, and diagnosing problems such as overfitting (or underfitting). Learning curves can also be tools for determining how much a model benefits from adding more training data, and whether the model suffers more from a variance error or a bias error. If both the validation score and the training score converge to a certain value, then the model will no longer significantly benefit from more training data. == Formal definition == When creating a function to approximate the distribution of some data, it is necessary to define a loss function L ( f θ ( X ) , Y ) {\\\\displaystyle L(f_{\\\\theta }(X),Y)} to measure how good the model output is (e.g., accuracy for classification tasks or mean squared error for regression). We then define an optimization process which finds model parameters θ {\\\\displaystyle \\\\theta } such that L ( f θ ( X ) , Y ) {\\\\displaystyle L(f_{\\\\theta }(X),Y)} is minimized, referred to as θ ∗ {\\\\displaystyle \\\\theta ^{*}} .'},\n",
       " {'id': 'Learning curve (machine learning)_1',\n",
       "  'title': 'Learning curve (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"accuracy for classification tasks or mean squared error for regression). We then define an optimization process which finds model parameters θ {\\\\displaystyle \\\\theta } such that L ( f θ ( X ) , Y ) {\\\\displaystyle L(f_{\\\\theta }(X),Y)} is minimized, referred to as θ ∗ {\\\\displaystyle \\\\theta ^{*}} . === Training curve for amount of data === If the training data is { x 1 , x 2 , … , x n } , { y 1 , y 2 , … y n } {\\\\displaystyle \\\\{x_{1},x_{2},\\\\dots ,x_{n}\\\\},\\\\{y_{1},y_{2},\\\\dots y_{n}\\\\}} and the validation data is { x 1 ′ , x 2 ′ , … x m ′ } , { y 1 ′ , y 2 ′ , … y m ′ } {\\\\displaystyle \\\\{x_{1}',x_{2}',\\\\dots x_{m}'\\\\},\\\\{y_{1}',y_{2}',\\\\dots y_{m}'\\\\}} , a learning curve is the plot of the two curves i ↦ L ( f θ ∗ ( X i , Y i ) ( X i ) , Y i ) {\\\\displaystyle i\\\\mapsto L(f_{\\\\theta ^{*}(X_{i},Y_{i})}(X_{i}),Y_{i})} i ↦ L ( f θ ∗ ( X i , Y i ) ( X i ′ ) , Y i ′ ) {\\\\displaystyle i\\\\mapsto L(f_{\\\\theta ^{*}(X_{i},Y_{i})}(X_{i}'),Y_{i}')} where X i = { x 1 , x 2 , … x i } {\\\\displaystyle X_{i}=\\\\{x_{1},x_{2},\\\\dots x_{i}\\\\}} === Training curve for number of iterations === Many optimization algorithms are iterative, repeating the same step (such as backpropagation) until the process converges to an optimal value. Gradient descent is one such algorithm. If θ i ∗ {\\\\displaystyle \\\\theta _{i}^{*}} is the approximation of the optimal θ {\\\\displaystyle \\\\theta } after i {\\\\displaystyle i} steps, a learning curve is the plot of i ↦ L ( f θ i ∗ ( X , Y ) ( X ) , Y ) {\\\\displaystyle i\\\\mapsto L(f_{\\\\theta _{i}^{*}(X,Y)}(X),Y)} i ↦\"},\n",
       " {'id': 'Learning curve (machine learning)_2',\n",
       "  'title': 'Learning curve (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"{\\\\displaystyle \\\\theta _{i}^{*}} is the approximation of the optimal θ {\\\\displaystyle \\\\theta } after i {\\\\displaystyle i} steps, a learning curve is the plot of i ↦ L ( f θ i ∗ ( X , Y ) ( X ) , Y ) {\\\\displaystyle i\\\\mapsto L(f_{\\\\theta _{i}^{*}(X,Y)}(X),Y)} i ↦ L ( f θ i ∗ ( X , Y ) ( X ′ ) , Y ′ ) {\\\\displaystyle i\\\\mapsto L(f_{\\\\theta _{i}^{*}(X,Y)}(X'),Y')} == See also == Overfitting Bias–variance tradeoff Model selection Cross-validation (statistics) Validity (statistics) Verification and validation Double descent == References ==\"},\n",
       " {'id': 'Learning curve (machine learning)_3',\n",
       "  'title': 'Learning curve (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"{\\\\displaystyle i\\\\mapsto L(f_{\\\\theta _{i}^{*}(X,Y)}(X),Y)} i ↦ L ( f θ i ∗ ( X , Y ) ( X ′ ) , Y ′ ) {\\\\displaystyle i\\\\mapsto L(f_{\\\\theta _{i}^{*}(X,Y)}(X'),Y')} == See also == Overfitting Bias–variance tradeoff Model selection Cross-validation (statistics) Validity (statistics) Verification and validation Double descent == References ==\"},\n",
       " {'id': 'Adversarial machine learning_0',\n",
       "  'title': 'Adversarial machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 revealed practitioners\\' common feeling for better protection of machine learning systems in industrial applications. Machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption. Most common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction. == History == At the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine-learning spam filter could be used to defeat another machine-learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam. In 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers added random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012–2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations.'},\n",
       " {'id': 'Adversarial machine learning_1',\n",
       "  'title': 'Adversarial machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012–2013). In 2012, deep neural networks began to dominate computer vision problems; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations. Recently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brain\\'s Nick Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state-of-the-art approaches. While adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks. === Examples === Examples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a'},\n",
       " {'id': 'Adversarial machine learning_2',\n",
       "  'title': 'Adversarial machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'through the misspelling of \"bad\" words or the insertion of \"good\" words; attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users\\' template galleries that adapt to updated traits over time. Researchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle with a texture engineered to make Google\\'s object detection AI classify it as a rifle regardless of the angle from which the turtle was viewed. Creating the turtle required only low-cost commercially available 3-D printing technology. A machine-tweaked image of a dog was shown to look like a cat to both computers and humans. A 2019 study reported that humans can guess how machines will classify adversarial images. Researchers discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle classified it as a merge or speed limit sign. A data poisoning filter called Nightshade was released in 2023 by researchers at the University of Chicago. It was created for use by visual artists to put on their artwork to corrupt the data set of text-to-image models, which usually scrape their data from the internet without the consent of the image creator. McAfee attacked Tesla\\'s former Mobileye system, fooling it into driving 50 mph over the speed limit, simply by adding a two-inch strip of black tape to a speed limit sign. Adversarial patterns on glasses or clothing designed to deceive facial-recognition systems or license-plate readers, have led to a niche industry of \"stealth streetwear\". An adversarial attack on a neural network can allow an attacker to inject algorithms into'},\n",
       " {'id': 'Adversarial machine learning_3',\n",
       "  'title': 'Adversarial machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'adding a two-inch strip of black tape to a speed limit sign. Adversarial patterns on glasses or clothing designed to deceive facial-recognition systems or license-plate readers, have led to a niche industry of \"stealth streetwear\". An adversarial attack on a neural network can allow an attacker to inject algorithms into the target system. Researchers can also create adversarial audio inputs to disguise commands to intelligent assistants in benign-seeming audio; a parallel literature explores human perception of such stimuli. Clustering algorithms are used in security applications. Malware and computer virus analysis aims to identify malware families, and to generate specific detection signatures. == Attack modalities == === Taxonomy === Attacks against (supervised) machine learning algorithms have been categorized along three primary axes: influence on the classifier, the security violation and their specificity. Classifier influence: An attack can influence the classifier by disrupting the classification phase. This may be preceded by an exploration phase to identify vulnerabilities. The attacker\\'s capabilities might be restricted by the presence of data manipulation constraints. Security violation: An attack can supply malicious data that gets classified as legitimate. Malicious data supplied during training can cause legitimate data to be rejected after training. Specificity: A targeted attack attempts to allow a specific intrusion/disruption. Alternatively, an indiscriminate attack creates general mayhem. This taxonomy has been extended into a more comprehensive threat model that allows explicit assumptions about the adversary\\'s goal, knowledge of the attacked system, capability of manipulating the input data/system components, and on attack strategy. This taxonomy has further been extended to include dimensions for defense strategies against adversarial attacks. === Strategies === Below are some of the most commonly encountered attack scenarios. ==== Data poisoning ==== Poisoning consists of contaminating the training dataset with data designed to increase errors in the output. Given that learning algorithms are'},\n",
       " {'id': 'Adversarial machine learning_4',\n",
       "  'title': 'Adversarial machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"further been extended to include dimensions for defense strategies against adversarial attacks. === Strategies === Below are some of the most commonly encountered attack scenarios. ==== Data poisoning ==== Poisoning consists of contaminating the training dataset with data designed to increase errors in the output. Given that learning algorithms are shaped by their training datasets, poisoning can effectively reprogram algorithms with potentially malicious intent. Concerns have been raised especially for user-generated training data, e.g. for content recommendation or natural language models. The ubiquity of fake accounts offers many opportunities for poisoning. Facebook reportedly removes around 7 billion fake accounts per year. Poisoning has been reported as the leading concern for industrial applications. On social medias, disinformation campaigns attempt to bias recommendation and moderation algorithms, to push certain content over others. A particular case of data poisoning is the backdoor attack, which aims to teach a specific behavior for inputs with a given trigger, e.g. a small defect on images, sounds, videos or texts. For instance, intrusion detection systems are often trained using collected data. An attacker may poison this data by injecting malicious samples during operation that subsequently disrupt retraining. Data poisoning techniques can also be applied to text-to-image models to alter their output, which is used by artists to defend their copyrighted works or their artistic style against imitation. Data poisoning can also happen unintentionally through model collapse, where models are trained on synthetic data. ==== Byzantine attacks ==== As machine learning is scaled, it often relies on multiple computing machines. In federated learning, for instance, edge devices collaborate with a central server, typically by sending gradients or model parameters. However, some of these devices may deviate from their expected behavior, e.g. to harm the central server's model or to bias algorithms towards certain behaviors (e.g., amplifying the recommendation\"},\n",
       " {'id': 'Supervised learning_0',\n",
       "  'title': 'Supervised learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In machine learning, supervised learning (SL) is a type of machine learning paradigm where an algorithm learns to map input data to a specific output based on example input-output pairs. This process involves training a statistical model using labeled data, meaning each piece of input data is provided with the correct output. For instance, if you want a model to identify cats in images, supervised learning would involve feeding it many images of cats (inputs) that are explicitly labeled \"cat\" (outputs). The goal of supervised learning is for the trained model to accurately predict the output for new, unseen data. This requires the algorithm to effectively generalize from the training examples, a quality measured by its generalization error. Supervised learning is commonly used for tasks like classification (predicting a category, e.g., spam or not spam) and regression (predicting a continuous value, e.g., house prices). == Steps to follow == To solve a given problem of supervised learning, the following steps must be performed: Determine the type of training samples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting, or a full paragraph of handwriting. Gather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered together with corresponding outputs, either from human experts or from measurements. Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of'},\n",
       " {'id': 'Supervised learning_1',\n",
       "  'title': 'Supervised learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'from human experts or from measurements. Determine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output. Determine the structure of the learned function and corresponding learning algorithm. For example, one may choose to use support-vector machines or decision trees. Complete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation. Evaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set. == Algorithm choice == A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem). There are four major issues to consider in supervised learning: === Bias–variance tradeoff === A first issue is the tradeoff between bias and variance. Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input x {\\\\displaystyle x} if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for x {\\\\displaystyle x} . A learning algorithm has high variance for a particular input x'},\n",
       " {'id': 'Supervised learning_2',\n",
       "  'title': 'Supervised learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'good, training data sets. A learning algorithm is biased for a particular input x {\\\\displaystyle x} if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for x {\\\\displaystyle x} . A learning algorithm has high variance for a particular input x {\\\\displaystyle x} if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm. Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust). === Function complexity and amount of training data === The second issue is of the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \"flexible\" learning algorithm with low bias and high variance. === Dimensionality of the input space === A third issue is the dimensionality of the'},\n",
       " {'id': 'Supervised learning_3',\n",
       "  'title': 'Supervised learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \"flexible\" learning algorithm with low bias and high variance. === Dimensionality of the input space === A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm. === Noise in the output values === A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\"'},\n",
       " {'id': 'Supervised learning_4',\n",
       "  'title': 'Supervised learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\" your training data – this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator. In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance. === Other factors to consider === Other factors to consider when choosing and applying a learning algorithm include the following: Heterogeneity of the data. If the feature vectors include features of many different kinds (discrete, discrete ordered, counts, continuous values), some algorithms are easier to apply than others. Many algorithms, including support-vector machines, linear regression, logistic regression, neural networks, and nearest neighbor methods, require that the input features be numerical and scaled to similar ranges (e.g., to the [-1,1] interval). Methods that employ a distance function, such as nearest neighbor methods and support-vector machines with Gaussian kernels, are particularly sensitive to this. An advantage of decision trees is that they easily handle heterogeneous data. Redundancy in the data. If the input features contain redundant information (e.g., highly correlated features), some learning algorithms (e.g., linear regression, logistic regression, and distance-based methods) will perform poorly because of numerical instabilities. These problems can often be solved by imposing some form'},\n",
       " {'id': 'Support vector machine_0',\n",
       "  'title': 'Support vector machine',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'In machine learning, support vector machines (SVMs, also support vector networks) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories, SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). In addition to performing linear classification, SVMs can efficiently perform non-linear classification using the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in a higher-dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces, where linear classification can be performed. Being max-margin models, SVMs are resilient to noisy data (e.g., misclassified examples). SVMs can also be used for regression tasks, where the objective becomes ϵ {\\\\displaystyle \\\\epsilon } -sensitive. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data. These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data into groups, and then to map new data according to these clusters. The popularity of SVMs is likely due to their amenability to theoretical analysis, and their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression. == Motivation == Classifying data is a common task in machine learning. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In'},\n",
       " {'id': 'Support vector machine_1',\n",
       "  'title': 'Support vector machine',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'other linear models, such as logistic regression and linear regression. == Motivation == Classifying data is a common task in machine learning. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a p {\\\\displaystyle p} -dimensional vector (a list of p {\\\\displaystyle p} numbers), and we want to know whether we can separate such points with a ( p − 1 ) {\\\\displaystyle (p-1)} -dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability. More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier. A lower generalization error means that the implementer is less likely to experience overfitting. Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason,'},\n",
       " {'id': 'Support vector machine_2',\n",
       "  'title': 'Support vector machine',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'lower the generalization error of the classifier. A lower generalization error means that the implementer is less likely to experience overfitting. Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function k ( x , y ) {\\\\displaystyle k(x,y)} selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters α i {\\\\displaystyle \\\\alpha _{i}} of images of feature vectors x i {\\\\displaystyle x_{i}} that occur in the data base. With this choice of a hyperplane, the points x {\\\\displaystyle x} in the feature space that are mapped into the hyperplane are defined by the relation ∑ i α i k ( x i , x ) = constant . {\\\\displaystyle \\\\textstyle \\\\sum _{i}\\\\alpha _{i}k(x_{i},x)={\\\\text{constant}}.} Note that if k ( x , y ) {\\\\displaystyle k(x,y)} becomes small as y {\\\\displaystyle y} grows further away from x {\\\\displaystyle x} , each term in the sum measures the degree of closeness of the test point x {\\\\displaystyle x} to the corresponding'},\n",
       " {'id': 'Support vector machine_3',\n",
       "  'title': 'Support vector machine',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': '{\\\\displaystyle \\\\textstyle \\\\sum _{i}\\\\alpha _{i}k(x_{i},x)={\\\\text{constant}}.} Note that if k ( x , y ) {\\\\displaystyle k(x,y)} becomes small as y {\\\\displaystyle y} grows further away from x {\\\\displaystyle x} , each term in the sum measures the degree of closeness of the test point x {\\\\displaystyle x} to the corresponding data base point x i {\\\\displaystyle x_{i}} . In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points x {\\\\displaystyle x} mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space. == Applications == SVMs can be used to solve various real-world problems: SVMs are helpful in text and hypertext categorization, as their application can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings. Some methods for shallow semantic parsing are based on support vector machines. Classification of images can also be performed using SVMs. Experimental results show that SVMs achieve significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback. This is also true for image segmentation systems, including those using a modified version SVM that uses the privileged approach as suggested by Vapnik. Classification of satellite data like SAR data using supervised SVM. Hand-written characters can be recognized using SVM. The SVM algorithm has been widely applied in the biological and other sciences. They have been used to classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism'},\n",
       " {'id': 'Support vector machine_4',\n",
       "  'title': 'Support vector machine',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'supervised SVM. Hand-written characters can be recognized using SVM. The SVM algorithm has been widely applied in the biological and other sciences. They have been used to classify proteins with up to 90% of the compounds classified correctly. Permutation tests based on SVM weights have been suggested as a mechanism for interpretation of SVM models. Support vector machine weights have also been used to interpret SVM models in the past. Posthoc interpretation of support vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences. == History == The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1964. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The \"soft margin\" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995. == Linear SVM == We are given a training dataset of n {\\\\displaystyle n} points of the form ( x 1 , y 1 ) , … , ( x n , y n ) , {\\\\displaystyle (\\\\mathbf {x} _{1},y_{1}),\\\\ldots ,(\\\\mathbf {x} _{n},y_{n}),} where the y i {\\\\displaystyle y_{i}} are either 1 or −1, each indicating the class to which the point x i {\\\\displaystyle \\\\mathbf {x} _{i}} belongs. Each x i {\\\\displaystyle \\\\mathbf {x} _{i}} is a p {\\\\displaystyle p} -dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points x i {\\\\displaystyle \\\\mathbf {x} _{i}} for which y i = 1 {\\\\displaystyle y_{i}=1} from the group of points for which y i = − 1 {\\\\displaystyle y_{i}=-1} , which is defined so'},\n",
       " {'id': 'Reinforcement learning_0',\n",
       "  'title': 'Reinforcement learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning. Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration–exploitation dilemma. The environment is typically stated in the form of a Markov decision process, as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large Markov decision processes where exact methods become infeasible. == Principles == Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment). Basic reinforcement learning is modeled as a Markov decision process: A set of environment and agent'},\n",
       " {'id': 'Reinforcement learning_1',\n",
       "  'title': 'Reinforcement learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment). Basic reinforcement learning is modeled as a Markov decision process: A set of environment and agent states (the state space), S {\\\\displaystyle {\\\\mathcal {S}}} ; A set of actions (the action space), A {\\\\displaystyle {\\\\mathcal {A}}} , of the agent; P a ( s , s ′ ) = Pr ( S t + 1 = s ′ ∣ S t = s , A t = a ) {\\\\displaystyle P_{a}(s,s')=\\\\Pr(S_{t+1}{=}s'\\\\mid S_{t}{=}s,A_{t}{=}a)} , the transition probability (at time t {\\\\displaystyle t} ) from state s {\\\\displaystyle s} to state s ′ {\\\\displaystyle s'} under action a {\\\\displaystyle a} . R a ( s , s ′ ) {\\\\displaystyle R_{a}(s,s')} , the immediate reward after transition from s {\\\\displaystyle s} to s ′ {\\\\displaystyle s'} under action a {\\\\displaystyle a} . The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning. A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state S t {\\\\displaystyle S_{t}} and reward R t {\\\\displaystyle R_{t}} . It then chooses an action A t {\\\\displaystyle A_{t}} from the\"},\n",
       " {'id': 'Reinforcement learning_2',\n",
       "  'title': 'Reinforcement learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"capable of reinforcement learning. A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state S t {\\\\displaystyle S_{t}} and reward R t {\\\\displaystyle R_{t}} . It then chooses an action A t {\\\\displaystyle A_{t}} from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state S t + 1 {\\\\displaystyle S_{t+1}} and the reward R t + 1 {\\\\displaystyle R_{t+1}} associated with the transition ( S t , A t , S t + 1 ) {\\\\displaystyle (S_{t},A_{t},S_{t+1})} is determined. The goal of a reinforcement learning agent is to learn a policy: π : S × A → [ 0 , 1 ] π ( s , a ) = Pr ( A t = a ∣ S t = s ) {\\\\displaystyle {\\\\begin{aligned}&\\\\pi :{\\\\mathcal {S}}\\\\times {\\\\mathcal {A}}\\\\to [0,1]\\\\\\\\&\\\\pi (s,a)=\\\\Pr(A_{t}{=}a\\\\mid S_{t}{=}s)\\\\end{aligned}}} that maximizes the expected cumulative reward. Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed. When the agent's performance is compared to that of an agent that acts optimally, the difference\"},\n",
       " {'id': 'Reinforcement learning_3',\n",
       "  'title': 'Reinforcement learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed. When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative. Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage, robot control, photovoltaic generators, backgammon, checkers, Go (AlphaGo), and autonomous driving systems. Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations: A model of the environment is known, but an analytic solution is not available; Only a simulation model of the environment is given (the subject of simulation-based optimization); The only way to collect information about the environment is to interact with it. The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems. == Exploration == The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997). Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor\"},\n",
       " {'id': 'Reinforcement learning_4',\n",
       "  'title': 'Reinforcement learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"problems. == Exploration == The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997). Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical. One such method is ε {\\\\displaystyle \\\\varepsilon } -greedy, where 0 < ε < 1 {\\\\displaystyle 0<\\\\varepsilon <1} is a parameter controlling the amount of exploration vs. exploitation. With probability 1 − ε {\\\\displaystyle 1-\\\\varepsilon } , exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability ε {\\\\displaystyle \\\\varepsilon } , exploration is chosen, and the action is chosen uniformly at random. ε {\\\\displaystyle \\\\varepsilon } is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics. == Algorithms for control learning == Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards. === Criterion of optimality === ==== Policy ==== The agent's action selection is modeled as a map called policy: π : A × S → [ 0 , 1 ] π ( a , s ) = Pr ( A t = a ∣ S t = s ) {\\\\displaystyle {\\\\begin{aligned}&\\\\pi :{\\\\mathcal {A}}\\\\times {\\\\mathcal {S}}\\\\to [0,1]\\\\\\\\&\\\\pi (a,s)=\\\\Pr(A_{t}{=}a\\\\mid\"},\n",
       " {'id': 'Torch (machine learning)_0',\n",
       "  'title': 'Torch (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Torch is an open-source machine learning library, a scientific computing framework, and a scripting language based on Lua. It provides LuaJIT interfaces to deep learning algorithms implemented in C. It was created by the Idiap Research Institute at EPFL. Torch development moved in 2017 to PyTorch, a port of the library to Python. == torch == The core package of Torch is torch. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix–vector multiplication, matrix–matrix multiplication and matrix product. The following exemplifies using torch via its REPL interpreter: The torch package also simplifies object-oriented programming and serialization by providing various convenience functions which are used throughout its packages. The torch.class(classname, parentclass) function can be used to create object factories (classes). When the constructor is called, torch initializes and sets a Lua table with the user-defined metatable, which makes the table an object. Objects created with the torch factory can also be serialized, as long as they do not contain references to objects that cannot be serialized, such as Lua coroutines, and Lua userdata. However, userdata can be serialized if it is wrapped by a table (or metatable) that provides read() and write() methods. == nn == The nn package is used for building neural networks. It is divided into modular objects that share a common Module interface. Modules have a forward() and backward() method that allow them to feedforward and backpropagate, respectively. Modules can be joined using module composites, like Sequential, Parallel and Concat to create complex'},\n",
       " {'id': 'Torch (machine learning)_1',\n",
       "  'title': 'Torch (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'nn package is used for building neural networks. It is divided into modular objects that share a common Module interface. Modules have a forward() and backward() method that allow them to feedforward and backpropagate, respectively. Modules can be joined using module composites, like Sequential, Parallel and Concat to create complex task-tailored graphs. Simpler modules like Linear, Tanh and Max make up the basic component modules. This modular interface provides first-order automatic gradient differentiation. What follows is an example use-case for building a multilayer perceptron using Modules: Loss functions are implemented as sub-classes of Criterion, which has a similar interface to Module. It also has forward() and backward() methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the mean squared error criterion implemented in MSECriterion and the cross-entropy criterion implemented in ClassNLLCriterion. What follows is an example of a Lua function that can be iteratively called to train an mlp Module on input Tensor x, target Tensor y with a scalar learningRate: It also has StochasticGradient class for training a neural network using stochastic gradient descent, although the optim package provides much more options in this respect, like momentum and weight decay regularization. == Other packages == Many packages other than the above official packages are used with Torch. These are listed in the torch cheatsheet. These extra packages provide a wide range of utilities such as parallelism, asynchronous input/output, image processing, and so on. They can be installed with LuaRocks, the Lua package manager which is also included with the Torch distribution. == Applications == Torch is used by the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. Torch has been extended for use on Android and iOS. It has been used to build'},\n",
       " {'id': 'Torch (machine learning)_2',\n",
       "  'title': 'Torch (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'installed with LuaRocks, the Lua package manager which is also included with the Torch distribution. == Applications == Torch is used by the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. Torch has been extended for use on Android and iOS. It has been used to build hardware implementations for data flows like those found in neural networks. Facebook has released a set of extension modules as open source software. == See also == Comparison of deep learning software PyTorch == References == == External links == Official website'},\n",
       " {'id': 'Torch (machine learning)_3',\n",
       "  'title': 'Torch (machine learning)',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'and iOS. It has been used to build hardware implementations for data flows like those found in neural networks. Facebook has released a set of extension modules as open source software. == See also == Comparison of deep learning software PyTorch == References == == External links == Official website'},\n",
       " {'id': 'Timeline of machine learning_0',\n",
       "  'title': 'Timeline of machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included. == Overview == == Timeline == == See also == History of artificial intelligence Timeline of artificial intelligence Timeline of machine translation == References == === Citations === === Works cited === Crevier, Daniel (1993). AI: The Tumultuous Search for Artificial Intelligence. New York: BasicBooks. ISBN 0-465-02997-3. Marr, Bernard (19 February 2016). \"A Short History of Machine Learning -- Every Manager Should Read\". Forbes. Archived from the original on 2022-12-05. Retrieved 2022-12-25. Russell, Stuart; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach. London: Pearson Education. ISBN 0-137-90395-2.'},\n",
       " {'id': 'Timeline of machine learning_1',\n",
       "  'title': 'Timeline of machine learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Tumultuous Search for Artificial Intelligence. New York: BasicBooks. ISBN 0-465-02997-3. Marr, Bernard (19 February 2016). \"A Short History of Machine Learning -- Every Manager Should Read\". Forbes. Archived from the original on 2022-12-05. Retrieved 2022-12-25. Russell, Stuart; Norvig, Peter (2003). Artificial Intelligence: A Modern Approach. London: Pearson Education. ISBN 0-137-90395-2.'},\n",
       " {'id': 'Statistical classification_0',\n",
       "  'title': 'Statistical classification',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'When classification is performed by a computer, statistical methods are normally used to develop the algorithm. Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function. An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category. Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis. == Relation to other problems == Classification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which'},\n",
       " {'id': 'Statistical classification_1',\n",
       "  'title': 'Statistical classification',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'pattern recognition, which is the assignment of some sort of output value to a given input value. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc. A common subclass of classification is probabilistic classification. Algorithms of this nature use statistical inference to find the best class for a given instance. Unlike other algorithms, which simply output a \"best\" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes. The best class is normally then selected as the one with the highest probability. However, such an algorithm has numerous advantages over non-probabilistic classifiers: It can output a confidence value associated with its choice (in general, a classifier that can do this is known as a confidence-weighted classifier). Correspondingly, it can abstain when its confidence of choosing any particular output is too low. Because of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation. == Frequentist procedures == Early work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fisher\\'s linear discriminant function as the rule for assigning a group to a new observation. This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two groups has also been considered with a restriction imposed that the classification rule'},\n",
       " {'id': 'Statistical classification_2',\n",
       "  'title': 'Statistical classification',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'the rule for assigning a group to a new observation. This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two groups has also been considered with a restriction imposed that the classification rule should be linear. Later work for the multivariate normal distribution allowed the classifier to be nonlinear: several classification rules can be derived based on different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation. == Bayesian procedures == Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population. Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised. Some Bayesian procedures involve the calculation of group-membership probabilities: these provide a more informative outcome than a simple attribution of a single group-label to each new observation. == Binary and multiclass classification == Classification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes. Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers. == Feature vectors == Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance. Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or'},\n",
       " {'id': 'Statistical classification_3',\n",
       "  'title': 'Statistical classification',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'binary classifiers. == Feature vectors == Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance. Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent). Features may variously be binary (e.g. \"on\" or \"off\"); categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type); ordinal (e.g. \"large\", \"medium\" or \"small\"); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure). If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words. Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10). == Linear classifiers == A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product. The predicted category is the one with the highest score. This type of score function is known as a linear predictor function and has the following general form: score \\u2061 ( X i , k ) = β k ⋅ X i , {\\\\displaystyle \\\\operatorname {score} (\\\\mathbf {X} _{i},k)={\\\\boldsymbol {\\\\beta }}_{k}\\\\cdot \\\\mathbf {X} _{i},} where Xi is the feature vector for instance i, βk is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category'},\n",
       " {'id': 'Statistical classification_4',\n",
       "  'title': 'Statistical classification',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': '= β k ⋅ X i , {\\\\displaystyle \\\\operatorname {score} (\\\\mathbf {X} _{i},k)={\\\\boldsymbol {\\\\beta }}_{k}\\\\cdot \\\\mathbf {X} _{i},} where Xi is the feature vector for instance i, βk is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k. In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k. Algorithms with this basic setup are known as linear classifiers. What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted. Examples of such algorithms include Logistic regression – Statistical model for a binary dependent variable Multinomial logistic regression – Regression for more than two discrete outcomes Probit regression – Statistical regression where the dependent variable can take only two valuesPages displaying short descriptions of redirect targets The perceptron algorithm Support vector machine – Set of methods for supervised statistical learning Linear discriminant analysis – Method used in statistics, pattern recognition, and other fields == Algorithms == Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms has been developed. The most commonly used include: Artificial neural networks – Computational model used in machine learning, based on connected, hierarchical functionsPages displaying short descriptions of redirect targets Boosting (machine learning) – Ensemble learning method Random forest – Tree-based ensemble machine learning method Genetic programming – Evolving computer programs with techniques analogous to natural genetic processes Gene expression programming – Evolutionary algorithm Multi expression programming Linear genetic programming Kernel estimation – Window functionPages displaying short descriptions of redirect targets k-nearest neighbor – Non-parametric classification methodPages displaying short descriptions of redirect targets Learning vector quantization Linear classifier – Statistical classification'},\n",
       " {'id': 'Multimodal learning_0',\n",
       "  'title': 'Multimodal learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Multimodal learning is a type of deep learning that integrates and processes multiple types of data, referred to as modalities, such as text, audio, images, or video. This integration allows for a more holistic understanding of complex data, improving model performance in tasks like visual question answering, cross-modal retrieval, text-to-image generation, aesthetic ranking, and image captioning. Large multimodal models, such as Google Gemini and GPT-4o, have become increasingly popular since 2023, enabling increased versatility and a broader understanding of real-world phenomena. == Motivation == Data usually comes with different modalities which carry different information. For example, it is very common to caption an image to convey the information not presented in the image itself. Similarly, sometimes it is more straightforward to use an image to describe information which may not be obvious from text. As a result, if different words appear in similar images, then these words likely describe the same thing. Conversely, if a word is used to describe seemingly dissimilar images, then these images may represent the same object. Thus, in cases dealing with multi-modal data, it is important to use a model which is able to jointly represent the information such that the model can capture the combined information from different modalities. == Multimodal transformers == === Multimodal large language models === == Multimodal deep Boltzmann machines == A Boltzmann machine is a type of stochastic neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups: visible units and hidden units. Each unit is like a neuron with a binary output that represents whether it is activated or not. General Boltzmann machines allow'},\n",
       " {'id': 'Multimodal learning_1',\n",
       "  'title': 'Multimodal learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'of Hopfield nets. They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups: visible units and hidden units. Each unit is like a neuron with a binary output that represents whether it is activated or not. General Boltzmann machines allow connection between any units. However, learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine. A more efficient architecture is called restricted Boltzmann machine where connection is only allowed between hidden unit and visible unit, which is described in the next section. Multimodal deep Boltzmann machines can process and learn from different types of information, such as images and text, simultaneously. This can notably be done by having a separate deep Boltzmann machine for each modality, for example one for images and one for text, joined at an additional top hidden layer. == Applications == Multimodal machine learning has numerous applications across various domains: Cross-modal retrieval: cross-modal retrieval allows users to search for data across different modalities (e.g., retrieving images based on text descriptions), improving multimedia search engines and content recommendation systems. Models like CLIP facilitate efficient, accurate retrieval by embedding data in a shared space, demonstrating strong performance even in zero-shot settings. Classification and missing data retrieval: multimodal Deep Boltzmann Machines outperform traditional models like support vector machines and latent Dirichlet allocation in classification tasks and can predict missing data in multimodal datasets, such as images and text. Healthcare diagnostics: multimodal models integrate medical imaging, genomic data, and patient records to improve diagnostic accuracy and early disease detection, especially in cancer screening. Content generation: models like DALL·E generate images from textual descriptions, benefiting creative industries, while cross-modal retrieval enables dynamic multimedia searches. Robotics and human-computer interaction: multimodal learning improves interaction'},\n",
       " {'id': 'Multimodal learning_2',\n",
       "  'title': 'Multimodal learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'multimodal models integrate medical imaging, genomic data, and patient records to improve diagnostic accuracy and early disease detection, especially in cancer screening. Content generation: models like DALL·E generate images from textual descriptions, benefiting creative industries, while cross-modal retrieval enables dynamic multimedia searches. Robotics and human-computer interaction: multimodal learning improves interaction in robotics and AI by integrating sensory inputs like speech, vision, and touch, aiding autonomous systems and human-computer interaction. Emotion recognition: combining visual, audio, and text data, multimodal systems enhance sentiment analysis and emotion recognition, applied in customer service, social media, and marketing. == See also == Hopfield network Markov random field Markov chain Monte Carlo == References =='},\n",
       " {'id': 'Multimodal learning_3',\n",
       "  'title': 'Multimodal learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'speech, vision, and touch, aiding autonomous systems and human-computer interaction. Emotion recognition: combining visual, audio, and text data, multimodal systems enhance sentiment analysis and emotion recognition, applied in customer service, social media, and marketing. == See also == Hopfield network Markov random field Markov chain Monte Carlo == References =='},\n",
       " {'id': 'Learning_0',\n",
       "  'title': 'Learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, non-human animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulate from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be \"lost\" from that which cannot be retrieved. Human learning starts at birth (it might even start before) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many established fields (including educational psychology, neuropsychology, experimental psychology, cognitive sciences, and pedagogy), as well as emerging fields of knowledge (e.g. with a shared interest in the topic of learning from safety events such as incidents/accidents, or in collaborative learning health systems). Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event cannot be avoided or escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development. Play has been approached by several theorists as a form of learning. Children experiment with'},\n",
       " {'id': 'Learning_1',\n",
       "  'title': 'Learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development. Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children\\'s development, since they make meaning of their environment through playing educational games. For Vygotsky, however, play is the first form of learning language and communication, and the stage where a child begins to understand rules and symbols. This has led to a view that learning in organisms is always related to semiosis, and is often associated with representational systems/activity. == Types == There are various functional categorizations of memory which have developed. Some memory researchers distinguish memory based on the relationship between the stimuli involved (associative vs non-associative) or based to whether the content can be communicated through language (declarative/explicit vs procedural/implicit). Some of these categories can, in turn, be parsed into sub-types. For instance, declarative memory comprises both episodic and semantic memory. === Non-associative learning === Non-associative learning refers to \"a relatively permanent change in the strength of response to a single stimulus due to repeated exposure to that stimulus.\" This definition exempts the changes caused by sensory adaptation, fatigue, or injury. Non-associative learning can be divided into habituation and sensitization. ==== Habituation ==== Habituation is an example of non-associative learning in which one or more components of an innate response (e.g., response probability, response duration) to a stimulus diminishes when the stimulus is repeated. Thus, habituation must be distinguished from extinction, which is an associative process. In operant extinction, for example, a response declines because it is no'},\n",
       " {'id': 'Learning_2',\n",
       "  'title': 'Learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'non-associative learning in which one or more components of an innate response (e.g., response probability, response duration) to a stimulus diminishes when the stimulus is repeated. Thus, habituation must be distinguished from extinction, which is an associative process. In operant extinction, for example, a response declines because it is no longer followed by a reward. An example of habituation can be seen in small song birds—if a stuffed owl (or similar predator) is put into the cage, the birds initially react to it as though it were a real predator. Soon the birds react less, showing habituation. If another stuffed owl is introduced (or the same one removed and re-introduced), the birds react to it again as though it were a predator, demonstrating that it is only a very specific stimulus that is habituated to (namely, one particular unmoving owl in one place). The habituation process is faster for stimuli that occur at a high rather than for stimuli that occur at a low rate as well as for the weak and strong stimuli, respectively. Habituation has been shown in essentially every species of animal, as well as the sensitive plant Mimosa pudica and the large protozoan Stentor coeruleus. This concept acts in direct opposition to sensitization. ==== Sensitization ==== Sensitization is an example of non-associative learning in which the progressive amplification of a response follows repeated administrations of a stimulus. This is based on the notion that a defensive reflex to a stimulus such as withdrawal or escape becomes stronger after the exposure to a different harmful or threatening stimulus. An everyday example of this mechanism is the repeated tonic stimulation of peripheral nerves that occurs if a person rubs their arm continuously. After a while, this stimulation creates a warm sensation that can eventually turn painful. This pain'},\n",
       " {'id': 'Learning_3',\n",
       "  'title': 'Learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'stronger after the exposure to a different harmful or threatening stimulus. An everyday example of this mechanism is the repeated tonic stimulation of peripheral nerves that occurs if a person rubs their arm continuously. After a while, this stimulation creates a warm sensation that can eventually turn painful. This pain results from a progressively amplified synaptic response of the peripheral nerves. This sends a warning that the stimulation is harmful. Sensitization is thought to underlie both adaptive as well as maladaptive learning processes in the organism. === Active learning === Active learning occurs when a person takes control of their learning experience. Since understanding information is the key aspect of learning, it is important for learners to recognize what they understand and what they do not. By doing so, they can monitor their own mastery of subjects. Active learning encourages learners to have an internal dialogue in which they verbalize understandings. This and other meta-cognitive strategies can be taught to a child over time. Studies within metacognition have proven the value in active learning, claiming that the learning is usually at a stronger level as a result. In addition, learners have more incentive to learn when they have control over not only how they learn but also what they learn. Active learning is a key characteristic of student-centered learning. Conversely, passive learning and direct instruction are characteristics of teacher-centered learning (or traditional education). === Associative learning === Associative learning is the process by which a person or animal learns an association between two stimuli or events. In classical conditioning, a previously neutral stimulus is repeatedly paired with a reflex-eliciting stimulus until eventually the neutral stimulus elicits a response on its own. In operant conditioning, a behavior that is reinforced or punished in the presence of a stimulus becomes more or'},\n",
       " {'id': 'Learning_4',\n",
       "  'title': 'Learning',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': \"association between two stimuli or events. In classical conditioning, a previously neutral stimulus is repeatedly paired with a reflex-eliciting stimulus until eventually the neutral stimulus elicits a response on its own. In operant conditioning, a behavior that is reinforced or punished in the presence of a stimulus becomes more or less likely to occur in the presence of that stimulus. ==== Operant conditioning ==== Operant conditioning is a way in which behavior can be shaped or modified according to the desires of the trainer or head individual. Operant conditioning uses the thought that living things seek pleasure and avoid pain, and that an animal or human can learn through receiving either reward or punishment at a specific time called trace conditioning. Trace conditioning is the small and ideal period of time between the subject performing the desired behavior, and receiving the positive reinforcement as a result of their performance. The reward needs to be given immediately after the completion of the wanted behavior. Operant conditioning is different from classical conditioning in that it shapes behavior not solely on bodily reflexes that occur naturally to a specific stimulus, but rather focuses on the shaping of wanted behavior that requires conscious thought, and ultimately requires learning. Punishment and reinforcement are the two principal ways in which operant conditioning occurs. Punishment is used to reduce unwanted behavior, and ultimately (from the learner's perspective) leads to avoidance of the punishment, not necessarily avoidance of the unwanted behavior. Punishment is not an appropriate way to increase wanted behavior for animals or humans. Punishment can be divided into two subcategories, positive punishment and negative punishment. Positive punishment is when an aversive aspect of life or thing is added to the subject, for this reason it is called positive punishment. For example, the parent spanking their child\"},\n",
       " {'id': 'Explainable artificial intelligence_0',\n",
       "  'title': 'Explainable artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'Within artificial intelligence (AI), explainable AI (XAI), often overlapping with interpretable AI or explainable machine learning (XML), is a field of research that explores methods that provide humans with the ability of intellectual oversight over AI algorithms. The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms, to make them more understandable and transparent. This addresses users\\' requirement to assess safety and scrutinize the automated decision making in applications. XAI counters the \"black box\" tendency of machine learning, where even the AI\\'s designers cannot explain why it arrived at a specific decision. XAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions. == Background == Machine learning (ML) algorithms used in AI can be categorized as white-box or black-box. White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and may not be understood even by domain experts. XAI algorithms follow the three principles of transparency, interpretability, and explainability. A model is transparent \"if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\" Interpretability describes the possibility'},\n",
       " {'id': 'Explainable artificial intelligence_1',\n",
       "  'title': 'Explainable artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'be understood even by domain experts. XAI algorithms follow the three principles of transparency, interpretability, and explainability. A model is transparent \"if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\" Interpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans. Explainability is a concept that is recognized as important, but a consensus definition is not yet available; one possibility is \"the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)\". In summary, Interpretability refers to the user\\'s ability to understand model outputs, while Model Transparency includes Simulatability (reproducibility of predictions), Decomposability (intuitive explanations for parameters), and Algorithmic Transparency (explaining how algorithms work). Model Functionality focuses on textual descriptions, visualization, and local explanations, which clarify specific outputs or instances rather than entire models. All these concepts aim to enhance the comprehensibility and usability of AI systems. If algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts. Sometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions. Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image and text prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms. Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm'},\n",
       " {'id': 'Explainable artificial intelligence_2',\n",
       "  'title': 'Explainable artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'both image and text prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms. Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset. AI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command \"maximize the accuracy of assessing how positive film reviews are in the test dataset.\" The AI may learn useful general rules from the test set, such as \"reviews containing the word \"horrible\" are likely to be negative.\" However, it may also learn inappropriate rules, such as \"reviews containing \\'Daniel Day-Lewis\\' are usually positive\"; such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be \"cheating\" or \"unfair.\" A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real-world data outside the test set. == Goals == Cooperation between agents – in this case, algorithms and humans – depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them. Incompleteness in formal trust criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria. This is particularly relevant in medicine, especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in order to trust the decision and augment their decision-making process. AI systems sometimes learn undesirable tricks that do an'},\n",
       " {'id': 'Explainable artificial intelligence_3',\n",
       "  'title': 'Explainable artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'This is particularly relevant in medicine, especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in order to trust the decision and augment their decision-making process. AI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre-programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data. For example, a 2017 system tasked with image recognition learned to \"cheat\" by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured. In another 2017 system, a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object. One transparency project, the DARPA XAI program, aims to produce \"glass box\" models that are explainable to a \"human-in-the-loop\" without greatly sacrificing AI performance. Human users of such a system can understand the AI\\'s cognition (both in real-time and after the fact) and can determine whether to trust the AI. Other applications of XAI are knowledge extraction from black-box models and model comparisons. In the context of monitoring systems for ethical and socio-legal compliance, the term \"glass box\" is commonly used to refer to tools that track the inputs and outputs of the system in question, and provide value-based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision-making processes are transparent and accountable. The term \"glass box\" is often used in contrast'},\n",
       " {'id': 'Explainable artificial intelligence_4',\n",
       "  'title': 'Explainable artificial intelligence',\n",
       "  'topic': 'Machine Learning',\n",
       "  'text': 'the inputs and outputs of the system in question, and provide value-based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision-making processes are transparent and accountable. The term \"glass box\" is often used in contrast to \"black box\" systems, which lack transparency and can be more difficult to monitor and regulate. The term is also used to name a voice assistant that produces counterfactual statements as explanations. == Explainability and interpretability techniques == There is a subtle difference between the terms explainability and interpretability in the context of AI. Some explainability techniques don\\'t involve understanding how the model works, and may work across various AI systems. Treating the model as a black box and analyzing how marginal changes to the inputs affect the result sometimes provides a sufficient explanation. === Explainability === Explainability is useful for ensuring that AI models are not making decisions based on irrelevant or otherwise unfair criteria. For classification and regression models, several popular techniques exist: Partial dependency plots show the marginal effect of an input feature on the predicted outcome. SHAP (SHapley Additive exPlanations) enables visualization of the contribution of each input feature to the output. It works by calculating Shapley values, which measure the average marginal contribution of a feature across all possible combinations of features. Feature importance estimates how important a feature is for the model. It is usually done using permutation importance, which measures the performance decrease when it the feature value randomly shuffled across all samples. LIME approximates locally a model\\'s outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. For images, saliency'},\n",
       " {'id': 'Neural network_0',\n",
       "  'title': 'Neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or signal pathways. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks. In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. In machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == In biology == In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead. Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion. == In machine learning == In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the'},\n",
       " {'id': 'Neural network_1',\n",
       "  'title': 'Neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer). The \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset. The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers. Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it. Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by'},\n",
       " {'id': 'Neural network_2',\n",
       "  'title': 'Neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'change and learn over time by strengthening a synapse every time a signal travels along it. Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957, artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References =='},\n",
       " {'id': 'Neural network_3',\n",
       "  'title': 'Neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957, artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References =='},\n",
       " {'id': 'Neural network (machine learning)_0',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a computational model inspired by the structure and functions of biological neural networks. A neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers. Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\\'s parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the'},\n",
       " {'id': 'Neural network (machine learning)_1',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == === Early work === Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for\"},\n",
       " {'id': 'Neural network (machine learning)_2',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing. Warren McCulloch and Walter Pitts (1943) considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. In the late 1940s, D. O. Hebb proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt\\'s perceptron and the Hopfield network. Farley and Clark (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks, funded by the United States Office of Naval Research. R. D. Joseph (1960) mentions an even earlier perceptron-like device by Farley and Clark: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" The perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not'},\n",
       " {'id': 'Neural network (machine learning)_3',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'perceptrons to emulate human intelligence. The first perceptrons did not have adaptive hidden units. However, Joseph (1960) also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. === Deep learning breakthroughs in the 1960s and 1970s === Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression, or a generalization of Rosenblatt\\'s perceptron. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\\'ichi Amari. In computer experiments conducted by Amari\\'s student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This'},\n",
       " {'id': 'Neural network (machine learning)_4',\n",
       "  'title': 'Neural network (machine learning)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969), who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967). In 1976 transfer learning was introduced in neural networks learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. === Backpropagation === Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master\\'s thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. === Convolutional neural networks === Kunihiko Fukushima\\'s convolutional neural network (CNN) architecture of 1979 also introduced max pooling, a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision. The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.'},\n",
       " {'id': 'Convolutional neural network_0',\n",
       "  'title': 'Convolutional neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced—in some cases—by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections. For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 × 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, only 25 weights for each convolutional layer are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows, compared to lower-layer features. Some applications of CNNs include: image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series. CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input. Feedforward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout,'},\n",
       " {'id': 'Convolutional neural network_1',\n",
       "  'title': 'Convolutional neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This simplifies and automates the process, enhancing efficiency and scalability overcoming human-intervention bottlenecks. == Architecture == A convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer\\'s input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers. Here it should be'},\n",
       " {'id': 'Convolutional neural network_2',\n",
       "  'title': 'Convolutional neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers. Here it should be noted how close a convolutional neural network is to a matched filter. === Convolutional layers === In a CNN, the input is a tensor with shape: (number of inputs) × (input height) × (input width) × (input channels) After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: (number of inputs) × (feature map height) × (feature map width) × (feature map channels). Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. For example, using a 5 × 5 tiling region, each with the same shared weights, requires only 25 neurons. Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks. To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which'},\n",
       " {'id': 'Convolutional neural network_3',\n",
       "  'title': 'Convolutional neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"with the same shared weights, requires only 25 neurons. Using shared weights means there are many fewer parameters, which helps avoid the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks. To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 1 × 1 {\\\\displaystyle 1\\\\times 1} kernels. === Pooling layers === Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value. === Fully connected layers === Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images. === Receptive field === In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons).\"},\n",
       " {'id': 'Convolutional neural network_4',\n",
       "  'title': 'Convolutional neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"=== In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios, thus having a variable receptive field size. === Weights === Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights. The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory\"},\n",
       " {'id': 'Recurrent neural network_0',\n",
       "  'title': 'Recurrent neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'In artificial neural networks, recurrent neural networks (RNNs) are designed for processing sequential data, such as text, speech, and time series, where the order of elements is important. Unlike feedforward neural networks, which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step. This enables RNNs to capture temporal dependencies and patterns within sequences. The fundamental building block of RNN is the recurrent unit, which maintains a hidden state—a form of memory that is updated at each time step based on the current input and the previous hidden state. This feedback mechanism allows the network to learn from past inputs and incorporate that knowledge into its current processing. RNNs have been successfully applied to tasks such as unsegmented, connected handwriting recognition, speech recognition, natural language processing, and neural machine translation. However, traditional RNNs suffer from the vanishing gradient problem, which limits their ability to learn long-range dependencies. This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies. Later, gated recurrent units (GRUs) were introduced as a more computationally efficient alternative. In recent years, transformers, which rely on self-attention mechanisms instead of recurrence, have become the dominant architecture for many sequence-processing tasks, particularly in natural language processing, due to their superior handling of long-range dependencies and greater parallelizability. Nevertheless, RNNs remain relevant for applications where computational efficiency, real-time processing, or the inherent sequential nature of data is crucial. == History == === Before modern === One origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex formed by parallel'},\n",
       " {'id': 'Recurrent neural network_1',\n",
       "  'title': 'Recurrent neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'computational efficiency, real-time processing, or the inherent sequential nature of data is crucial. == History == === Before modern === One origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells. In 1933, Lorente de Nó discovered \"recurrent, reciprocal connections\" by Golgi\\'s method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex. During 1940s, multiple people proposed the existence of feedback in the brain, which was a contrast to the previous understanding of the neural system as a purely feedforward structure. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. The current activity of such networks can be affected by activity indefinitely far in the past. They were both interested in closed loops as possible explanations for e.g. epilepsy and causalgia. Recurrent inhibition was proposed in 1946 as a negative feedback mechanism in motor control. Neural feedback loops were a common topic of discussion at the Macy conferences. See for an extensive review of recurrent neural network models in neuroscience. Frank Rosenblatt in 1960 published \"close-loop cross-coupled perceptrons\", which are 3-layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule. Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network. Similar networks were published by Kaoru Nakano in 1971,Shun\\'ichi Amari in 1972, and William A. Little in 1974, who was acknowledged by Hopfield in his 1982'},\n",
       " {'id': 'Recurrent neural network_2',\n",
       "  'title': 'Recurrent neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'experimental studies for Hebbian learning in these networks, and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network. Similar networks were published by Kaoru Nakano in 1971,Shun\\'ichi Amari in 1972, and William A. Little in 1974, who was acknowledged by Hopfield in his 1982 paper. Another origin of RNN was statistical mechanics. The Ising model was developed by Wilhelm Lenz and Ernst Ising in the 1920s as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time. The Sherrington–Kirkpatrick model of spin glass, published in 1975, is the Hopfield network with random initialization. Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima. In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions. In a 1984 paper he extended this to continuous activation functions. It became a standard model for the study of neural networks through statistical mechanics. === Modern === Modern RNN networks are mainly based on two architectures: LSTM and BRNN. At the resurgence of neural networks in the 1980s, recurrent networks were studied again. They were sometimes called \"iterated nets\". Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. It became the default choice for RNN architecture. Bidirectional recurrent'},\n",
       " {'id': 'Recurrent neural network_3',\n",
       "  'title': 'Recurrent neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. It became the default choice for RNN architecture. Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions. These two are often combined, giving the bidirectional LSTM architecture. Around 2006, bidirectional LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. They also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google voice search, and dictation on Android devices. They broke records for improved machine translation, language modeling and Multilingual Language Processing. Also, LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. The idea of encoder-decoder sequence transduction had been developed in the early 2010s. The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. A seq2seq architecture employs two RNN, typically LSTM, an \"encoder\" and a \"decoder\", for sequence transduction, such as machine translation. They became state of the art in machine translation, and was instrumental in the development of attention mechanisms and transformers. == Configurations == An RNN-based model can be factored into two parts: configuration and architecture. Multiple RNNs can be combined in a data flow, and the data flow itself is the configuration. Each RNN itself may have any architecture, including LSTM, GRU, etc. === Standard === RNNs come in many variants. Abstractly speaking, an RNN is a function f θ {\\\\displaystyle f_{\\\\theta }} of type ( x t , h t ) ↦ ( y t , h t + 1 ) {\\\\displaystyle (x_{t},h_{t})\\\\mapsto (y_{t},h_{t+1})} , where x t {\\\\displaystyle x_{t}} : input vector; h'},\n",
       " {'id': 'Recurrent neural network_4',\n",
       "  'title': 'Recurrent neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': '=== RNNs come in many variants. Abstractly speaking, an RNN is a function f θ {\\\\displaystyle f_{\\\\theta }} of type ( x t , h t ) ↦ ( y t , h t + 1 ) {\\\\displaystyle (x_{t},h_{t})\\\\mapsto (y_{t},h_{t+1})} , where x t {\\\\displaystyle x_{t}} : input vector; h t {\\\\displaystyle h_{t}} : hidden vector; y t {\\\\displaystyle y_{t}} : output vector; θ {\\\\displaystyle \\\\theta } : neural network parameters. In words, it is a neural network that maps an input x t {\\\\displaystyle x_{t}} into an output y t {\\\\displaystyle y_{t}} , with the hidden vector h t {\\\\displaystyle h_{t}} playing the role of \"memory\", a partial record of all previous input-output pairs. At each step, it transforms input to an output, and modifies its \"memory\" to help it to better perform future processing. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in \"layers\" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time, \"unfolded\" to produce the appearance of layers. === Stacked RNN === A stacked RNN, or deep RNN, is composed of multiple RNNs stacked one above the other. Abstractly, it is structured as follows Layer 1 has hidden vector h 1 , t {\\\\displaystyle h_{1,t}} , parameters θ 1 {\\\\displaystyle \\\\theta _{1}} , and maps f θ 1 : ( x 0 , t , h 1 , t ) ↦ ( x 1 , t , h 1 , t + 1 ) {\\\\displaystyle f_{\\\\theta _{1}}:(x_{0,t},h_{1,t})\\\\mapsto (x_{1,t},h_{1,t+1})} . Layer 2 has hidden vector h 2 , t {\\\\displaystyle h_{2,t}} , parameters θ 2 {\\\\displaystyle \\\\theta _{2}} , and maps f θ 2 : ( x 1 , t , h 2 , t ) ↦'},\n",
       " {'id': 'Deep learning_0',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. == Overview == Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes'},\n",
       " {'id': 'Deep learning_1',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction. The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP'},\n",
       " {'id': 'Deep learning_2',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': '(as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively. Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks. The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated. == Interpretations == Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal'},\n",
       " {'id': 'Deep learning_3',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit. The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop. == History == === Before 1980 === There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other\"},\n",
       " {'id': 'Deep learning_4',\n",
       "  'title': 'Deep learning',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'cycles in their connectivity structure, FNNs don\\'t. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun\\'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on \"Intelligent Machinery\" that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\". Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt\\'s perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since'},\n",
       " {'id': 'Feedforward neural network_0',\n",
       "  'title': 'Feedforward neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Feedforward refers to recognition-inference architecture of neural networks. Artificial neural network architectures are based on inputs multiplied by weights to obtain outputs (inputs-to-output): feedforward. Recurrent neural networks, or neural networks with loops allow information from later processing stages to feed back to earlier stages for sequence processing. However, at every stage of inference a feedforward multiplication remains the core, essential for backpropagation or backpropagation through time. Thus neural networks cannot contain feedback like negative feedback or positive feedback where the outputs feed back to the very same inputs and modify them, because this forms an infinite loop which is not possible to rewind in time to generate an error signal through backpropagation. This issue and nomenclature appear to be a point of confusion between some computer scientists and scientists in other fields studying brain networks. == Mathematical foundations == === Activation function === The two historically common activation functions are both sigmoids, and are described by y ( v i ) = tanh \\u2061 ( v i ) and y ( v i ) = ( 1 + e − v i ) − 1 . {\\\\displaystyle y(v_{i})=\\\\tanh(v_{i})~~{\\\\text{and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}.} The first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here y i {\\\\displaystyle y_{i}} is the output of the i {\\\\displaystyle i} -th node (neuron) and v i {\\\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways'},\n",
       " {'id': 'Feedforward neural network_1',\n",
       "  'title': 'Feedforward neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. === Learning === Learning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation. We can represent the degree of error in an output node j {\\\\displaystyle j} in the n {\\\\displaystyle n} -th data point (training example) by e j ( n ) = d j ( n ) − y j ( n ) {\\\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)} , where d j ( n ) {\\\\displaystyle d_{j}(n)} is the desired target value for n {\\\\displaystyle n} -th data point at node j {\\\\displaystyle j} , and y j ( n ) {\\\\displaystyle y_{j}(n)} is the value produced at node j {\\\\displaystyle j} when the n {\\\\displaystyle n} -th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n {\\\\displaystyle n} -th data point, given by E ( n ) = 1 2 ∑ output node j e j 2 ( n ) . {\\\\displaystyle {\\\\mathcal {E}}(n)={\\\\frac {1}{2}}\\\\sum _{{\\\\text{output node }}j}e_{j}^{2}(n).} Using gradient descent, the change in each weight w i j {\\\\displaystyle w_{ij}} is Δ w j i ( n ) = − η ∂ E ( n ) ∂ v j ( n ) y i ( n ) {\\\\displaystyle \\\\Delta w_{ji}(n)=-\\\\eta {\\\\frac'},\n",
       " {'id': 'Feedforward neural network_2',\n",
       "  'title': 'Feedforward neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': '{\\\\mathcal {E}}(n)={\\\\frac {1}{2}}\\\\sum _{{\\\\text{output node }}j}e_{j}^{2}(n).} Using gradient descent, the change in each weight w i j {\\\\displaystyle w_{ij}} is Δ w j i ( n ) = − η ∂ E ( n ) ∂ v j ( n ) y i ( n ) {\\\\displaystyle \\\\Delta w_{ji}(n)=-\\\\eta {\\\\frac {\\\\partial {\\\\mathcal {E}}(n)}{\\\\partial v_{j}(n)}}y_{i}(n)} where y i ( n ) {\\\\displaystyle y_{i}(n)} is the output of the previous neuron i {\\\\displaystyle i} , and η {\\\\displaystyle \\\\eta } is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression, ∂ E ( n ) ∂ v j ( n ) {\\\\displaystyle {\\\\frac {\\\\partial {\\\\mathcal {E}}(n)}{\\\\partial v_{j}(n)}}} denotes the partial derivative of the error E ( n ) {\\\\displaystyle {\\\\mathcal {E}}(n)} according to the weighted sum v j ( n ) {\\\\displaystyle v_{j}(n)} of the input connections of neuron i {\\\\displaystyle i} . The derivative to be calculated depends on the induced local field v j {\\\\displaystyle v_{j}} , which itself varies. It is easy to prove that for an output node this derivative can be simplified to − ∂ E ( n ) ∂ v j ( n ) = e j ( n ) ϕ ′ ( v j ( n ) ) {\\\\displaystyle -{\\\\frac {\\\\partial {\\\\mathcal {E}}(n)}{\\\\partial v_{j}(n)}}=e_{j}(n)\\\\phi ^{\\\\prime }(v_{j}(n))} where ϕ ′ {\\\\displaystyle \\\\phi ^{\\\\prime }} is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is − ∂ E ( n ) ∂ v j ( n ) = ϕ ′ ( v j ( n ) ) ∑ k − ∂ E ( n ) ∂'},\n",
       " {'id': 'Feedforward neural network_3',\n",
       "  'title': 'Feedforward neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'for the change in weights to a hidden node, but it can be shown that the relevant derivative is − ∂ E ( n ) ∂ v j ( n ) = ϕ ′ ( v j ( n ) ) ∑ k − ∂ E ( n ) ∂ v k ( n ) w k j ( n ) . {\\\\displaystyle -{\\\\frac {\\\\partial {\\\\mathcal {E}}(n)}{\\\\partial v_{j}(n)}}=\\\\phi ^{\\\\prime }(v_{j}(n))\\\\sum _{k}-{\\\\frac {\\\\partial {\\\\mathcal {E}}(n)}{\\\\partial v_{k}(n)}}w_{kj}(n).} This depends on the change in weights of the k {\\\\displaystyle k} th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. == History == === Timeline === Circa 1800, Legendre (1805) and Gauss (1795) created the simplest feedforward network which consists of a single weight layer with linear activation functions. It was trained by the least squares method for minimising mean squared error, also known as linear regression. Legendre and Gauss used it for the prediction of planetary movement from training data. In 1943, Warren McCulloch and Walter Pitts proposed the binary artificial neuron as a logical model of biological neural networks. In 1958, Frank Rosenblatt proposed the multilayered perceptron model, consisting of an input layer, a hidden layer with randomized weights that did not learn, and an output layer with learnable connections. R. D. Joseph (1960) mentions an even earlier perceptron-like device: \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" In 1960, Joseph also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately,'},\n",
       " {'id': 'Feedforward neural network_4',\n",
       "  'title': 'Feedforward neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\" In 1960, Joseph also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962) cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. In 1965, Alexey Grigorevich Ivakhnenko and Valentin Lapa published Group Method of Data Handling, the first working deep learning algorithm, a method to train arbitrarily deep neural networks. It is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\" It was used to train an eight-layer neural net in 1971. In 1967, Shun\\'ichi Amari reported the first multilayered neural network trained by stochastic gradient descent, which was able to classify non-linearily separable pattern classes. Amari\\'s student Saito conducted the computer experiments, using a five-layered feedforward network with two learning layers. In 1970, Seppo Linnainmaa published the modern form of backpropagation in his master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. In 2003, interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors. === Linear regression === === Perceptron === If using a threshold, i.e. a linear activation function, the resulting linear threshold unit is called a'},\n",
       " {'id': 'Physics-informed neural networks_0',\n",
       "  'title': 'Physics-informed neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Physics-informed neural networks (PINNs), also referred to as Theory-Trained Neural Networks (TTNs), are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs). Low data availability for some biological and engineering problems limit the robustness of conventional machine learning models used for these applications. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the generalizability of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. For they process continuous spatial and time coordinates and output continuous PDE solutions, they can be categorized as neural fields. == Function approximation == Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example, the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e., conservation of mass, momentum, and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However, these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences, finite elements and finite volumes). In this setting, these governing equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization. Recently, solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific'},\n",
       " {'id': 'Physics-informed neural networks_1',\n",
       "  'title': 'Physics-informed neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'as finite differences, finite elements and finite volumes). In this setting, these governing equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization. Recently, solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML), leveraging the universal approximation theorem and high expressivity of neural networks. In general, deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However, such networks do not consider the physical characteristics underlying the problem, and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information, the solution is not unique and may lose physical correctness. On the other hand, physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely, PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion, a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially, an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore, with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete), PINN may be used for finding an optimal solution with high fidelity. PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g., CFD for fluid dynamics), and new data-driven approaches for model inversion and system identification. Notably, the'},\n",
       " {'id': 'Physics-informed neural networks_2',\n",
       "  'title': 'Physics-informed neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g., CFD for fluid dynamics), and new data-driven approaches for model inversion and system identification. Notably, the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition, being neural fields, they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations, a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. == Modeling and computation == A general nonlinear partial differential equation can be: u t + N [ u ; λ ] = 0 , x ∈ Ω , t ∈ [ 0 , T ] {\\\\displaystyle u_{t}+N[u;\\\\lambda ]=0,\\\\quad x\\\\in \\\\Omega ,\\\\quad t\\\\in [0,T]} where u ( t , x ) {\\\\displaystyle u(t,x)} denotes the solution, N [ ⋅ ; λ ] {\\\\displaystyle N[\\\\cdot ;\\\\lambda ]} is a nonlinear operator parameterized by λ {\\\\displaystyle \\\\lambda } , and Ω {\\\\displaystyle \\\\Omega } is a subset of R D {\\\\displaystyle \\\\mathbb {R} ^{D}} . This general form of governing equations summarizes a wide range of problems in mathematical physics, such as conservative laws, diffusion process, advection-diffusion systems, and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above, PINNs can be designed to solve two classes of problems: data-driven solution data-driven discovery of partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u ( t , x ) {\\\\displaystyle u(t,x)} of the system given boundary data and/or measurements z {\\\\displaystyle'},\n",
       " {'id': 'Physics-informed neural networks_3',\n",
       "  'title': 'Physics-informed neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'designed to solve two classes of problems: data-driven solution data-driven discovery of partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u ( t , x ) {\\\\displaystyle u(t,x)} of the system given boundary data and/or measurements z {\\\\displaystyle z} , and fixed model parameters λ {\\\\displaystyle \\\\lambda } . We solve: u t + N [ u ] = 0 , x ∈ Ω , t ∈ [ 0 , T ] {\\\\displaystyle u_{t}+N[u]=0,\\\\quad x\\\\in \\\\Omega ,\\\\quad t\\\\in [0,T]} . By defining the residual f ( t , x ) {\\\\displaystyle f(t,x)} as f := u t + N [ u ] = 0 {\\\\displaystyle f:=u_{t}+N[u]=0} , and approximating u ( t , x ) {\\\\displaystyle u(t,x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u ( t , x ) {\\\\displaystyle u(t,x)} and f ( t , x ) {\\\\displaystyle f(t,x)} can be then learned by minimizing the following loss function L t o t {\\\\displaystyle L_{tot}} : L t o t = L u + L f {\\\\displaystyle L_{tot}=L_{u}+L_{f}} . Where L u = ‖ u − z ‖ Γ {\\\\displaystyle L_{u}=\\\\Vert u-z\\\\Vert _{\\\\Gamma }} is the error between the PINN u ( t , x ) {\\\\displaystyle u(t,x)} and the set of boundary conditions and measured data on the set of points Γ {\\\\displaystyle \\\\Gamma } where the boundary conditions and data are defined, and L f = ‖ f ‖ Γ {\\\\displaystyle L_{f}=\\\\Vert f\\\\Vert _{\\\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate'},\n",
       " {'id': 'Physics-informed neural networks_4',\n",
       "  'title': 'Physics-informed neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': '= ‖ f ‖ Γ {\\\\displaystyle L_{f}=\\\\Vert f\\\\Vert _{\\\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes, model predictive control, multi-physics and multi-scale modeling, and simulation. It has been shown to converge to the solution of the PDE. === Data-driven discovery of partial differential equations === Given noisy and incomplete measurements z {\\\\displaystyle z} of the state of the system, the data-driven discovery of PDE results in computing the unknown state u ( t , x ) {\\\\displaystyle u(t,x)} and learning model parameters λ {\\\\displaystyle \\\\lambda } that best describe the observed data and it reads as follows: u t + N [ u ; λ ] = 0 , x ∈ Ω , t ∈ [ 0 , T ] {\\\\displaystyle u_{t}+N[u;\\\\lambda ]=0,\\\\quad x\\\\in \\\\Omega ,\\\\quad t\\\\in [0,T]} . By defining f ( t , x ) {\\\\displaystyle f(t,x)} as f := u t + N [ u ; λ ] = 0 {\\\\displaystyle f:=u_{t}+N[u;\\\\lambda ]=0} , and approximating u ( t , x ) {\\\\displaystyle u(t,x)} by a deep neural network, f ( t , x ) {\\\\displaystyle f(t,x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of u ( t , x ) {\\\\displaystyle u(t,x)} and f ( t , x ) {\\\\displaystyle f(t,x)} , together with the parameter λ {\\\\displaystyle \\\\lambda } of the differential operator can be then learned by minimizing the following loss function L t o t {\\\\displaystyle L_{tot}} : L t o t = L u + L f {\\\\displaystyle L_{tot}=L_{u}+L_{f}} . Where L'},\n",
       " {'id': 'Graph neural network_0',\n",
       "  'title': 'Graph neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Graph neural networks (GNN) are specialized artificial neural networks that are designed for tasks whose inputs are graphs. One prominent example is molecular drug design. Each input sample is a graph representation of a molecule, where atoms form the nodes and chemical bonds between atoms form the edges. In addition to the graph representation, the input also includes known chemical properties for each of the atoms. Dataset samples may thus differ in length, reflecting the varying numbers of atoms in molecules, and the varying number of bonds between them. The task is to predict the efficacy of a given molecule for a specific medical application, like eliminating E. coli bacteria. The key design element of GNNs is the use of pairwise message passing, such that graph nodes iteratively update their representations by exchanging information with their neighbors. Several GNN architectures have been proposed, which implement different flavors of message passing, started by recursive or convolutional constructive approaches. As of 2022, it is an open question whether it is possible to define GNN architectures \"going beyond\" message passing, or instead every GNN can be built on message passing over suitably defined graphs. In the more general subject of \"geometric deep learning\", certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer, in the context of computer vision, can be considered a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer, in natural language processing, can be considered a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. Relevant application domains for GNNs include natural language processing, social networks, citation networks, molecular biology, chemistry, physics and NP-hard combinatorial optimization problems.'},\n",
       " {'id': 'Graph neural network_1',\n",
       "  'title': 'Graph neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'transformer layer, in natural language processing, can be considered a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. Relevant application domains for GNNs include natural language processing, social networks, citation networks, molecular biology, chemistry, physics and NP-hard combinatorial optimization problems. Open source libraries implementing GNNs include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), Deep Graph Library (framework agnostic), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux). == Architecture == The architecture of a generic GNN implements the following fundamental layers: Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature, permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively, in a message passing layer, nodes update their representations by aggregating the messages received from their immediate neighbours. As such, each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling, top-k pooling, and self-attention pooling. Global pooling: a global pooling layer, also known as readout layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum, mean or maximum. It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries'},\n",
       " {'id': 'Graph neural network_2',\n",
       "  'title': 'Graph neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'or maximum. It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022, whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally, they can be expressed as message passing neural networks (MPNNs). Let G = ( V , E ) {\\\\displaystyle G=(V,E)} be a graph, where V {\\\\displaystyle V} is the node set and E {\\\\displaystyle E} is the edge set. Let N u {\\\\displaystyle N_{u}} be the neighbourhood of some node u ∈ V {\\\\displaystyle u\\\\in V} . Additionally, let x u {\\\\displaystyle \\\\mathbf {x} _{u}} be the features of node u ∈ V {\\\\displaystyle u\\\\in V} , and e u v {\\\\displaystyle \\\\mathbf {e} _{uv}} be the features of edge ( u , v ) ∈ E {\\\\displaystyle (u,v)\\\\in E} . An MPNN layer can be expressed as follows: h u = ϕ ( x u , ⨁ v ∈ N u ψ ( x u , x v , e u v ) ) {\\\\displaystyle \\\\mathbf {h} _{u}=\\\\phi \\\\left(\\\\mathbf {x} _{u},\\\\bigoplus _{v\\\\in N_{u}}\\\\psi (\\\\mathbf {x} _{u},\\\\mathbf {x} _{v},\\\\mathbf {e} _{uv})\\\\right)} where ϕ {\\\\displaystyle \\\\phi } and ψ {\\\\displaystyle \\\\psi } are differentiable functions (e.g., artificial neural networks), and ⨁ {\\\\displaystyle \\\\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g., element-wise sum, mean, or max). In particular, ϕ {\\\\displaystyle \\\\phi } and ψ'},\n",
       " {'id': 'Graph neural network_3',\n",
       "  'title': 'Graph neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'where ϕ {\\\\displaystyle \\\\phi } and ψ {\\\\displaystyle \\\\psi } are differentiable functions (e.g., artificial neural networks), and ⨁ {\\\\displaystyle \\\\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g., element-wise sum, mean, or max). In particular, ϕ {\\\\displaystyle \\\\phi } and ψ {\\\\displaystyle \\\\psi } are referred to as update and message functions, respectively. Intuitively, in an MPNN computational block, graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations h u {\\\\displaystyle \\\\mathbf {h} _{u}} for each node u ∈ V {\\\\displaystyle u\\\\in V} in the graph. Node representations can be employed for any downstream task, such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such, stacking n {\\\\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n {\\\\displaystyle n} \"hops\" away. In principle, to ensure that every node receives information from every other node, one would need to stack a number of MPNN layers equal to the graph diameter. However, stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks), gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer, i.e., by considering the graph as a complete graph, can mitigate oversquashing in problems where long-range dependencies are required. Other \"flavours\" of MPNN have been developed in the literature, such as graph convolutional networks and graph attention networks,'},\n",
       " {'id': 'Graph neural network_4',\n",
       "  'title': 'Graph neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer, i.e., by considering the graph as a complete graph, can mitigate oversquashing in problems where long-range dependencies are required. Other \"flavours\" of MPNN have been developed in the literature, such as graph convolutional networks and graph attention networks, whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017. A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: H = σ ( D ~ − 1 2 A ~ D ~ − 1 2 X Θ ) {\\\\displaystyle \\\\mathbf {H} =\\\\sigma \\\\left({\\\\tilde {\\\\mathbf {D} }}^{-{\\\\frac {1}{2}}}{\\\\tilde {\\\\mathbf {A} }}{\\\\tilde {\\\\mathbf {D} }}^{-{\\\\frac {1}{2}}}\\\\mathbf {X} \\\\mathbf {\\\\Theta } \\\\right)} where H {\\\\displaystyle \\\\mathbf {H} } is the matrix of node representations h u {\\\\displaystyle \\\\mathbf {h} _{u}} , X {\\\\displaystyle \\\\mathbf {X} } is the matrix of node features x u {\\\\displaystyle \\\\mathbf {x} _{u}} , σ ( ⋅ ) {\\\\displaystyle \\\\sigma (\\\\cdot )} is an activation function (e.g., ReLU), A ~ {\\\\displaystyle {\\\\tilde {\\\\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops, D ~ {\\\\displaystyle {\\\\tilde {\\\\mathbf {D} }}} is the graph degree matrix with the addition of self-loops, and Θ {\\\\displaystyle \\\\mathbf {\\\\Theta } } is a matrix of trainable parameters. In particular, let A {\\\\displaystyle \\\\mathbf {A} } be the graph adjacency matrix: then, one can define A ~ = A + I {\\\\displaystyle {\\\\tilde {\\\\mathbf {A} }}=\\\\mathbf {A} +\\\\mathbf {I} } and D ~ i i = ∑ j ∈ V'},\n",
       " {'id': 'Rectifier (neural networks)_0',\n",
       "  'title': 'Rectifier (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the non-negative part of its argument, i.e., the ramp function: ReLU \\u2061 ( x ) = x + = max ( 0 , x ) = x + | x | 2 = { x if x > 0 , 0 x ≤ 0 {\\\\displaystyle \\\\operatorname {ReLU} (x)=x^{+}=\\\\max(0,x)={\\\\frac {x+|x|}{2}}={\\\\begin{cases}x&{\\\\text{if }}x>0,\\\\\\\\0&x\\\\leq 0\\\\end{cases}}} where x {\\\\displaystyle x} is the input to a neuron. This is analogous to half-wave rectification in electrical engineering. ReLU is one of the most popular activation functions for artificial neural networks, and finds application in computer vision and speech recognition using deep neural nets and computational neuroscience. == History == The ReLU was first used by Alston Householder in 1941 as a mathematical abstraction of biological neural networks. Kunihiko Fukushima in 1969 used ReLU in the context of visual feature extraction in hierarchical neural networks. Thirty years later, Hahnloser et al. argued that ReLU approximates the biological relationship between neural firing rates and input current, in addition to enabling recurrent neural network dynamics to stabilise under weaker criteria. Prior to 2010, most activation functions used were the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more numerically efficient counterpart, the hyperbolic tangent. Around 2010, the use of ReLU became common again. Jarrett et al. (2009) noted that rectification by either absolute or ReLU (which they called \"positive part\") was critical for object recognition in convolutional neural networks (CNNs), specifically because it allows average pooling without neighboring filter outputs cancelling each other out. They hypothesized that the use of sigmoid or tanh was responsible for poor performance in previous CNNs. Nair and Hinton (2010) made a theoretical argument that the softplus activation'},\n",
       " {'id': 'Rectifier (neural networks)_1',\n",
       "  'title': 'Rectifier (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'object recognition in convolutional neural networks (CNNs), specifically because it allows average pooling without neighboring filter outputs cancelling each other out. They hypothesized that the use of sigmoid or tanh was responsible for poor performance in previous CNNs. Nair and Hinton (2010) made a theoretical argument that the softplus activation function should be used, in that the softplus function numerically approximates the sum of an exponential number of linear models that share parameters. They then proposed ReLU as a good approximation to it. Specifically, they began by considering a single binary neuron in a Boltzmann machine that takes x {\\\\displaystyle x} as input, and produces 1 as output with probability σ ( x ) = 1 1 + e − x {\\\\displaystyle \\\\sigma (x)={\\\\frac {1}{1+e^{-x}}}} . They then considered extending its range of output by making infinitely many copies of it X 1 , X 2 , X 3 , … {\\\\displaystyle X_{1},X_{2},X_{3},\\\\dots } , that all take the same input, offset by an amount 0.5 , 1.5 , 2.5 , … {\\\\displaystyle 0.5,1.5,2.5,\\\\dots } , then their outputs are added together as ∑ i = 1 ∞ X i {\\\\displaystyle \\\\sum _{i=1}^{\\\\infty }X_{i}} . They then demonstrated that ∑ i = 1 ∞ X i {\\\\displaystyle \\\\sum _{i=1}^{\\\\infty }X_{i}} is approximately equal to N ( log \\u2061 ( 1 + e x ) , σ ( x ) ) {\\\\displaystyle {\\\\mathcal {N}}(\\\\log(1+e^{x}),\\\\sigma (x))} , which is also approximately equal to ReLU \\u2061 ( N ( x , σ ( x ) ) ) {\\\\displaystyle \\\\operatorname {ReLU} ({\\\\mathcal {N}}(x,\\\\sigma (x)))} , where N {\\\\displaystyle {\\\\mathcal {N}}} stands for the gaussian distribution. They also argued for another reason for using ReLU: that it allows \"intensity equivariance\" in image recognition. That is, multiplying input image by a constant k {\\\\displaystyle k} multiplies'},\n",
       " {'id': 'Rectifier (neural networks)_2',\n",
       "  'title': 'Rectifier (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'x ) ) ) {\\\\displaystyle \\\\operatorname {ReLU} ({\\\\mathcal {N}}(x,\\\\sigma (x)))} , where N {\\\\displaystyle {\\\\mathcal {N}}} stands for the gaussian distribution. They also argued for another reason for using ReLU: that it allows \"intensity equivariance\" in image recognition. That is, multiplying input image by a constant k {\\\\displaystyle k} multiplies the output also. In contrast, this is false for other activation functions like sigmoid or tanh. They found that ReLU activation allowed good empirical performance in restricted Boltzmann machines. Glorot et al (2011) argued that ReLU has the following advantages over sigmoid or tanh: ReLU is more similar to biological neurons\\' responses in their main operating regime. ReLU avoids vanishing gradients. ReLU is cheaper to compute. ReLU creates sparse representation naturally, because many hidden units output exactly zero for a given input. They also found empirically that deep networks trained with ReLU can achieve strong performance without unsupervised pre-training, especially on large, purely supervised tasks. == Advantages == Advantages of ReLU include: Sparse activation: for example, in a randomly initialized network, only about 50% of hidden units are activated (i.e. have a non-zero output). Better gradient propagation: fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. Efficiency: only requires comparison and addition. Scale-invariant (homogeneous, or \"intensity equivariance\"): max ( 0 , a x ) = a max ( 0 , x ) for a ≥ 0 {\\\\displaystyle \\\\max(0,ax)=a\\\\max(0,x){\\\\text{ for }}a\\\\geq 0} . == Potential problems == Possible downsides can include: Non-differentiability at zero (however, it is differentiable anywhere else, and the value of the derivative at zero can be chosen to be 0 or 1 arbitrarily). Not zero-centered: ReLU outputs are always non-negative. This can make it harder for the network to learn during backpropagation, because gradient updates tend to push weights in one direction'},\n",
       " {'id': 'Rectifier (neural networks)_3',\n",
       "  'title': 'Rectifier (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'is differentiable anywhere else, and the value of the derivative at zero can be chosen to be 0 or 1 arbitrarily). Not zero-centered: ReLU outputs are always non-negative. This can make it harder for the network to learn during backpropagation, because gradient updates tend to push weights in one direction (positive or negative). Batch normalization can help address this. ReLU is unbounded. Redundancy of the parametrization: Because ReLU is scale-invariant, the network computes the exact same function by scaling the weights and biases in front of a ReLU activation by k {\\\\displaystyle k} , and the weights after by 1 / k {\\\\displaystyle 1/k} . Dying ReLU: ReLU neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state, no gradients flow backward through the neuron, and so the neuron becomes stuck in a perpetually inactive state (it \"dies\"). This is a form of the vanishing gradient problem. In some cases, large numbers of neurons in a network can become stuck in dead states, effectively decreasing the model capacity and potentially even halting the learning process. This problem typically arises when the learning rate is set too high. It may be mitigated by using \"leaky\" ReLU instead, where a small positive slope is assigned for x < 0 {\\\\displaystyle x<0} . However, depending on the task, performance may be reduced. == Variants == === Piecewise-linear variants === Leaky ReLU (2014) allows a small, positive gradient when the unit is inactive, helping to mitigate the vanishing gradient problem. This gradient is defined by a parameter α {\\\\displaystyle \\\\alpha } , typically set to 0.01–0.3. f ( x ) = { x x > 0 , α x x ≤ 0 , f ′ ( x ) = { 1 x > 0 ,'},\n",
       " {'id': 'Rectifier (neural networks)_4',\n",
       "  'title': 'Rectifier (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'mitigate the vanishing gradient problem. This gradient is defined by a parameter α {\\\\displaystyle \\\\alpha } , typically set to 0.01–0.3. f ( x ) = { x x > 0 , α x x ≤ 0 , f ′ ( x ) = { 1 x > 0 , α x ≤ 0. {\\\\displaystyle f(x)={\\\\begin{cases}x&x>0,\\\\\\\\\\\\alpha x&x\\\\leq 0,\\\\end{cases}}\\\\qquad f\\'(x)={\\\\begin{cases}1&x>0,\\\\\\\\\\\\alpha &x\\\\leq 0.\\\\end{cases}}} The same function can also be expressed without the piecewise notation as: f ( x ) = 1 + α 2 x + 1 − α 2 | x | {\\\\displaystyle f(x)={\\\\frac {1+\\\\alpha }{2}}x+{\\\\frac {1-\\\\alpha }{2}}|x|} Parametric ReLU (PReLU, 2016) takes this idea further by making α {\\\\displaystyle \\\\alpha } a learnable parameter along with the other network parameters. Note that for α ≤ 1 {\\\\displaystyle \\\\alpha \\\\leq 1} , this is equivalent to f ( x ) = max ( x , α x ) {\\\\displaystyle f(x)=\\\\max(x,\\\\alpha x)} and thus has a relation to \"maxout\" networks. Concatenated ReLU (CReLU, 2016) preserves positive and negative phase information by returning two values: f ( x ) = [ ReLU \\u2061 ( x ) , ReLU \\u2061 ( − x ) ] . {\\\\displaystyle f(x)=[\\\\operatorname {ReLU} (x),\\\\operatorname {ReLU} (-x)].} === Smooth variants === ==== Softplus ==== A smooth approximation to the rectifier is the analytic function f ( x ) = ln \\u2061 ( 1 + e x ) , f ′ ( x ) = e x 1 + e x = 1 1 + e − x {\\\\displaystyle f(x)=\\\\ln(1+e^{x}),\\\\qquad f\\'(x)={\\\\frac {e^{x}}{1+e^{x}}}={\\\\frac {1}{1+e^{-x}}}} which is called the softplus (2000) or SmoothReLU function. For large negative x {\\\\displaystyle x} it is roughly ln \\u2061 1 {\\\\displaystyle \\\\ln 1} , so just above 0, while for large positive x {\\\\displaystyle x} it is roughly ln \\u2061 ( e x ) {\\\\displaystyle \\\\ln(e^{x})}'},\n",
       " {'id': 'Neural network (biology)_0',\n",
       "  'title': 'Neural network (biology)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'A neural network, also called a neuronal network, is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. Closely related are artificial neural networks, machine learning models inspired by biological neural networks. They consist of artificial neurons, which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. == Overview == A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called synapses, are usually formed from axons to dendrites, though dendrodendritic synapses and other connections are possible. Apart from electrical signalling, there are other forms of signalling that arise from neurotransmitter diffusion. Artificial intelligence, cognitive modelling, and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field, artificial neural networks have been applied successfully to speech recognition, image analysis and adaptive control, in order to construct software agents (in computer and video games) or autonomous robots. Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. == History == The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain. For Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened.'},\n",
       " {'id': 'Neural network (biology)_1',\n",
       "  'title': 'Neural network (biology)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain. For Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James' theory was similar to Bain's; however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided\"},\n",
       " {'id': 'Neural network (biology)_2',\n",
       "  'title': 'Neural network (biology)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. Artificial neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function. == Neuroscience == Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling. The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). === Types of models === Many models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory, from the individual neuron to the system level. === Connectivity === In'},\n",
       " {'id': 'Neural network (biology)_3',\n",
       "  'title': 'Neural network (biology)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory, from the individual neuron to the system level. === Connectivity === In August 2020 scientists reported that bi-directional connections, or added appropriate feedback connections, can accelerate and improve communication between and in modular neural networks of the brain's cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network. The connectivity of a neural network stems from its biological structures and is usually challenging to map out experimentally. Scientists used a variety of statistical tools to infer the connectivity of a network based on the observed neuronal activities, i.e., spike trains. Recent research has shown that statistically inferred neuronal connections in subsampled neural networks strongly correlate with spike train covariances, providing deeper insights into the structure of neural circuits and their computational properties. == Recent improvements == While initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine, acetylcholine, and serotonin on behaviour and learning. Biophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes Wood Wide Web == References ==\"},\n",
       " {'id': 'Neural network (biology)_4',\n",
       "  'title': 'Neural network (biology)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes Wood Wide Web == References =='},\n",
       " {'id': 'Residual neural network_0',\n",
       "  'title': 'Residual neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'A residual neural network (also referred to as a residual network or ResNet) is a deep learning architecture in which the layers learn residual functions with reference to the layer inputs. It was developed in 2015 for image recognition, and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) of that year. As a point of terminology, \"residual connection\" refers to the specific architectural motif of x ↦ f ( x ) + x {\\\\displaystyle x\\\\mapsto f(x)+x} , where f {\\\\displaystyle f} is an arbitrary neural network module. The motif had been used previously (see §History for details). However, the publication of ResNet made it widely popular for feedforward networks, appearing in neural networks that are seemingly unrelated to ResNet. The residual connection stabilizes the training and convergence of deep neural networks with hundreds of layers, and is a common motif in deep neural networks, such as transformer models (e.g., BERT, and GPT models such as ChatGPT), the AlphaGo Zero system, the AlphaStar system, and the AlphaFold system. == Mathematics == === Residual connection === In a multilayer neural network model, consider a subnetwork with a certain number of stacked layers (e.g., 2 or 3). Denote the underlying function performed by this subnetwork as H ( x ) {\\\\displaystyle H(x)} , where x {\\\\displaystyle x} is the input to the subnetwork. Residual learning re-parameterizes this subnetwork and lets the parameter layers represent a \"residual function\" F ( x ) = H ( x ) − x {\\\\displaystyle F(x)=H(x)-x} . The output y {\\\\displaystyle y} of this subnetwork is then represented as: y = F ( x ) + x {\\\\displaystyle y=F(x)+x} The operation of \" + x {\\\\displaystyle +\\\\ x} \" is implemented via a \"skip connection\" that performs an identity mapping to connect the input of the subnetwork with'},\n",
       " {'id': 'Residual neural network_1',\n",
       "  'title': 'Residual neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'output y {\\\\displaystyle y} of this subnetwork is then represented as: y = F ( x ) + x {\\\\displaystyle y=F(x)+x} The operation of \" + x {\\\\displaystyle +\\\\ x} \" is implemented via a \"skip connection\" that performs an identity mapping to connect the input of the subnetwork with its output. This connection is referred to as a \"residual connection\" in later work. The function F ( x ) {\\\\displaystyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g., batch normalization or layer normalization). As a whole, one of these subnetworks is referred to as a \"residual block\". A deep residual network is constructed by simply stacking these blocks. Long short-term memory (LSTM) has a memory mechanism that serves as a residual connection. In an LSTM without a forget gate, an input x t {\\\\displaystyle x_{t}} is processed by a function F {\\\\displaystyle F} and added to a memory cell c t {\\\\displaystyle c_{t}} , resulting in c t + 1 = c t + F ( x t ) {\\\\displaystyle c_{t+1}=c_{t}+F(x_{t})} . An LSTM with a forget gate essentially functions as a highway network. To stabilize the variance of the layers\\' inputs, it is recommended to replace the residual connections x + f ( x ) {\\\\displaystyle x+f(x)} with x / L + f ( x ) {\\\\displaystyle x/L+f(x)} , where L {\\\\displaystyle L} is the total number of residual layers. === Projection connection === If the function F {\\\\displaystyle F} is of type F : R n → R m {\\\\displaystyle F:\\\\mathbb {R} ^{n}\\\\to \\\\mathbb {R} ^{m}} where n ≠ m {\\\\displaystyle n\\\\neq m} , then F ( x ) + x {\\\\displaystyle F(x)+x} is undefined. To handle this special case, a projection connection is used: y = F ( x'},\n",
       " {'id': 'Residual neural network_2',\n",
       "  'title': 'Residual neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'of type F : R n → R m {\\\\displaystyle F:\\\\mathbb {R} ^{n}\\\\to \\\\mathbb {R} ^{m}} where n ≠ m {\\\\displaystyle n\\\\neq m} , then F ( x ) + x {\\\\displaystyle F(x)+x} is undefined. To handle this special case, a projection connection is used: y = F ( x ) + P ( x ) {\\\\displaystyle y=F(x)+P(x)} where P {\\\\displaystyle P} is typically a linear projection, defined by P ( x ) = M x {\\\\displaystyle P(x)=Mx} where M {\\\\displaystyle M} is a m × n {\\\\displaystyle m\\\\times n} matrix. The matrix is trained via backpropagation, as is any other parameter of the model. === Signal propagation === The introduction of identity mappings facilitates signal propagation in both forward and backward paths. ==== Forward propagation ==== If the output of the ℓ {\\\\displaystyle \\\\ell } -th residual block is the input to the ( ℓ + 1 ) {\\\\displaystyle (\\\\ell +1)} -th residual block (assuming no activation function between blocks), then the ( ℓ + 1 ) {\\\\displaystyle (\\\\ell +1)} -th input is: x ℓ + 1 = F ( x ℓ ) + x ℓ {\\\\displaystyle x_{\\\\ell +1}=F(x_{\\\\ell })+x_{\\\\ell }} Applying this formulation recursively, e.g.: x ℓ + 2 = F ( x ℓ + 1 ) + x ℓ + 1 = F ( x ℓ + 1 ) + F ( x ℓ ) + x ℓ {\\\\displaystyle {\\\\begin{aligned}x_{\\\\ell +2}&=F(x_{\\\\ell +1})+x_{\\\\ell +1}\\\\\\\\&=F(x_{\\\\ell +1})+F(x_{\\\\ell })+x_{\\\\ell }\\\\end{aligned}}} yields the general relationship: x L = x ℓ + ∑ i = ℓ L − 1 F ( x i ) {\\\\displaystyle x_{L}=x_{\\\\ell }+\\\\sum _{i=\\\\ell }^{L-1}F(x_{i})} where L {\\\\textstyle L} is the index of a residual block and ℓ {\\\\textstyle \\\\ell } is the index of some earlier block. This formulation suggests that there is always a signal that is'},\n",
       " {'id': 'Residual neural network_3',\n",
       "  'title': 'Residual neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'i = ℓ L − 1 F ( x i ) {\\\\displaystyle x_{L}=x_{\\\\ell }+\\\\sum _{i=\\\\ell }^{L-1}F(x_{i})} where L {\\\\textstyle L} is the index of a residual block and ℓ {\\\\textstyle \\\\ell } is the index of some earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block ℓ {\\\\textstyle \\\\ell } to a deeper block L {\\\\textstyle L} . ==== Backward propagation ==== The residual learning formulation provides the added benefit of mitigating the vanishing gradient problem to some extent. However, it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem, which is tackled through the use of normalization. To observe the effect of residual blocks on backpropagation, consider the partial derivative of a loss function E {\\\\displaystyle {\\\\mathcal {E}}} with respect to some residual block input x ℓ {\\\\displaystyle x_{\\\\ell }} . Using the equation above from forward propagation for a later residual block L > ℓ {\\\\displaystyle L>\\\\ell } : ∂ E ∂ x ℓ = ∂ E ∂ x L ∂ x L ∂ x ℓ = ∂ E ∂ x L ( 1 + ∂ ∂ x ℓ ∑ i = ℓ L − 1 F ( x i ) ) = ∂ E ∂ x L + ∂ E ∂ x L ∂ ∂ x ℓ ∑ i = ℓ L − 1 F ( x i ) {\\\\displaystyle {\\\\begin{aligned}{\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{\\\\ell }}}&={\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}{\\\\frac {\\\\partial x_{L}}{\\\\partial x_{\\\\ell }}}\\\\\\\\&={\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}\\\\left(1+{\\\\frac {\\\\partial }{\\\\partial x_{\\\\ell }}}\\\\sum _{i=\\\\ell }^{L-1}F(x_{i})\\\\right)\\\\\\\\&={\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}+{\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}{\\\\frac {\\\\partial }{\\\\partial x_{\\\\ell }}}\\\\sum _{i=\\\\ell }^{L-1}F(x_{i})\\\\end{aligned}}} This formulation suggests that the gradient computation of a shallower layer, ∂ E ∂ x ℓ {\\\\textstyle {\\\\frac {\\\\partial {\\\\mathcal'},\n",
       " {'id': 'Residual neural network_4',\n",
       "  'title': 'Residual neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': '{E}}}{\\\\partial x_{L}}}{\\\\frac {\\\\partial x_{L}}{\\\\partial x_{\\\\ell }}}\\\\\\\\&={\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}\\\\left(1+{\\\\frac {\\\\partial }{\\\\partial x_{\\\\ell }}}\\\\sum _{i=\\\\ell }^{L-1}F(x_{i})\\\\right)\\\\\\\\&={\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}+{\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}{\\\\frac {\\\\partial }{\\\\partial x_{\\\\ell }}}\\\\sum _{i=\\\\ell }^{L-1}F(x_{i})\\\\end{aligned}}} This formulation suggests that the gradient computation of a shallower layer, ∂ E ∂ x ℓ {\\\\textstyle {\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{\\\\ell }}}} , always has a later term ∂ E ∂ x L {\\\\textstyle {\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}} that is directly added. Even if the gradients of the F ( x i ) {\\\\displaystyle F(x_{i})} terms are small, the total gradient ∂ E ∂ x ℓ {\\\\textstyle {\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{\\\\ell }}}} resists vanishing due to the added term ∂ E ∂ x L {\\\\textstyle {\\\\frac {\\\\partial {\\\\mathcal {E}}}{\\\\partial x_{L}}}} . == Variants of residual blocks == === Basic block === A basic block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. === Bottleneck block === A bottleneck block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1×1 convolution for dimension reduction (e.g., to 1/2 of the input dimension); the second layer performs a 3×3 convolution; the last layer is another 1×1 convolution for dimension restoration. The models of ResNet-50, ResNet-101, and ResNet-152 are all based on bottleneck blocks. === Pre-activation block === The pre-activation residual block applies activation functions before applying the residual function F {\\\\displaystyle F} . Formally, the computation of a pre-activation residual block can be written as: x ℓ + 1 = F ( ϕ ( x ℓ ) ) + x ℓ {\\\\displaystyle x_{\\\\ell +1}=F(\\\\phi (x_{\\\\ell }))+x_{\\\\ell }} where ϕ {\\\\displaystyle \\\\phi } can be any activation'},\n",
       " {'id': 'Quantum neural network_0',\n",
       "  'title': 'Quantum neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley, engaging with the theory of quantum mind, which posits that quantum effects play a role in cognitive function. However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts, this structure intakes input from one layer of qubits, and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width, meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum\"},\n",
       " {'id': 'Quantum neural network_1',\n",
       "  'title': 'Quantum neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data. == Examples == Quantum neural network research is still in its infancy, and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”), resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory, since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld, Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === At a larger scale, researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly, with unitary gates, or classically, via measurement of the network states. This high-level theoretical technique can be applied broadly,'},\n",
       " {'id': 'Quantum neural network_2',\n",
       "  'title': 'Quantum neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly, with unitary gates, or classically, via measurement of the network states. This high-level theoretical technique can be applied broadly, by taking different types of networks and different implementations of quantum neurons, such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing. Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory, but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition, and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such, this is not a fully content-addressable memory, since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory, which can retrieve patterns also from corrupted inputs, was\"},\n",
       " {'id': 'Quantum neural network_3',\n",
       "  'title': 'Quantum neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'written into a superposition, and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such, this is not a fully content-addressable memory, since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory, which can retrieve patterns also from corrupted inputs, was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger, however, has shown that his probabilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns, a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks, at the end of a given operation, the current perceptron copies its output to the next layer of perceptron(s) in the network. However, in a quantum neural network, where each perceptron is a qubit, this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out, but does not copy, the output of one qubit to the next layer of qubits. Using this fan-out Unitary ( U f {\\\\displaystyle U_{f}} ) with a dummy state qubit in a known state (Ex. | 0 ⟩ {\\\\displaystyle |0\\\\rangle } in the computational basis), also known as an Ancilla'},\n",
       " {'id': 'Quantum neural network_4',\n",
       "  'title': 'Quantum neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"but does not copy, the output of one qubit to the next layer of qubits. Using this fan-out Unitary ( U f {\\\\displaystyle U_{f}} ) with a dummy state qubit in a known state (Ex. | 0 ⟩ {\\\\displaystyle |0\\\\rangle } in the computational basis), also known as an Ancilla bit, the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility. Using this quantum feed-forward network, deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers, as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators, and each operator only acts on its respective input, only two layers are used at any given time. In other words, no Unitary operator is acting on the entire network at any given time, meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time, the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer, and not on the depth of the network. === Cost functions === To determine the effectiveness of a neural network, a cost function is used, which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network, the weights ( w {\\\\displaystyle w} ) and biases ( b {\\\\displaystyle b} ) at each step determine the outcome of the cost function C ( w , b ) {\\\\displaystyle C(w,b)} . When training a Classical Neural network, the weights and biases are\"},\n",
       " {'id': 'Spiking neural network_0',\n",
       "  'title': 'Spiking neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"Spiking neural networks (SNNs) are artificial neural networks (ANN) that mimic natural neural networks. These models leverage timing of discrete spikes as the main information carrier. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model. While spike rates can be considered the analogue of the variable output of a traditional ANN, neurobiology research indicated that high speed processing cannot be performed solely through a rate-based scheme. For example humans can perform an image recognition task requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for rate-based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate-based approach. The most prominent spiking neuron model is the leaky integrate-and-fire model. In that model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing, the state variable is reset to a\"},\n",
       " {'id': 'Spiking neural network_1',\n",
       "  'title': 'Spiking neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing, the state variable is reset to a lower value. Various decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes. == History == Many multi-layer artificial neural networks are fully connected, receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs, they do not match biological networks and do not mimic neurons. The biology-inspired Hodgkin–Huxley model of a spiking neuron was proposed in 1952. This model described how action potentials are initiated and propagated. Communication between neurons, which requires the exchange of chemical neurotransmitters in the synaptic gap, is described in models such as the integrate-and-fire model, FitzHugh–Nagumo model (1961–1962), and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than Hodgkin–Huxley. While the notion of an artificial spiking neural network became popular only in the twenty-first century, studies between 1980 and 1995 supported the concept. The first models of this type of ANN appeared to simulate non-algorithmic intelligent information processing systems. However, the notion of the spiking neural network as a mathematical model was first worked on in the early 1970s. As of 2019 SNNs lagged behind ANNs in accuracy, but the gap is decreasing, and has vanished on some tasks. == Underpinnings == Information in the brain is represented as action potentials (neuron spikes), which may group into spike trains or coordinated\"},\n",
       " {'id': 'Spiking neural network_2',\n",
       "  'title': 'Spiking neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'was first worked on in the early 1970s. As of 2019 SNNs lagged behind ANNs in accuracy, but the gap is decreasing, and has vanished on some tasks. == Underpinnings == Information in the brain is represented as action potentials (neuron spikes), which may group into spike trains or coordinated waves. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding implies that a single spiking neuron can replace hundreds of hidden units on a conventional neural net. SNNs define a neuron\\'s current state as its potential (possibly modeled as a differential equation). An input pulse causes the potential to rise and then gradually decline. Encoding schemes can interpret these pulse sequences as a number, considering pulse frequency and pulse interval. Using the precise time of pulse occurrence, a neural network can consider more information and offer better computing properties. SNNs compute in the continuous domain. Such neurons test for activation only when their potentials reach a certain value. When a neuron is activated, it produces a signal that is passed to connected neurons, accordingly raising or lowering their potentials. The SNN approach produces a continuous output instead of the binary output of traditional ANNs. Pulse trains are not easily interpretable, hence the need for encoding schemes. However, a pulse train representation may be more suited for processing spatiotemporal data (or real-world sensory data classification). SNNs connect neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information. This avoids the complexity of a recurrent neural network (RNN). Impulse neurons are more powerful computational units than traditional artificial neurons. SNNs are theoretically more powerful than so called \"second-generation networks\" defined'},\n",
       " {'id': 'Spiking neural network_3',\n",
       "  'title': 'Spiking neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'CNN using filters). They consider time by encoding information as pulse trains so as not to lose information. This avoids the complexity of a recurrent neural network (RNN). Impulse neurons are more powerful computational units than traditional artificial neurons. SNNs are theoretically more powerful than so called \"second-generation networks\" defined as ANNs \"based on computational units that apply activation function with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs\"; however, SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP, no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks. Spike-based activation of SNNs is not differentiable, thus gradient descent-based backpropagation (BP) is not available. SNNs have much larger computational costs for simulating realistic neural models than traditional ANNs. Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN. Researchers are actively working on various topics. The first concerns differentiability. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is not differentiable because a neuron\\'s output is either 1 when it spikes, and 0 otherwise. This all-or-nothing behavior disrupts gradients and makes these neurons unsuitable for gradient-based optimization. Approaches to resolving it include: resorting to entirely biologically inspired local learning rules for the hidden units translating conventionally trained “rate-based” NNs to SNNs smoothing the network model to be continuously differentiable defining an SG (Surrogate Gradient) as a continuous relaxation of the real gradients The second concerns the optimization algorithm. Standard BP can be expensive in terms of computation, memory, and communication and may be poorly suited to the hardware that implements it (e.g., a'},\n",
       " {'id': 'Spiking neural network_4',\n",
       "  'title': 'Spiking neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'network model to be continuously differentiable defining an SG (Surrogate Gradient) as a continuous relaxation of the real gradients The second concerns the optimization algorithm. Standard BP can be expensive in terms of computation, memory, and communication and may be poorly suited to the hardware that implements it (e.g., a computer, brain, or neuromorphic device). Incorporating additional neuron dynamics such as Spike Frequency Adaptation (SFA) is a notable advance, enhancing efficiency and computational power. These neurons sit between biological complexity and computational complexity. Originating from biological insights, SFA offers significant computational benefits by reducing power usage, especially in cases of repetitive or intense stimuli. This adaptation improves signal/noise clarity and introduces an elementary short-term memory at the neuron level, which in turn, improves accuracy and efficiency. This was mostly achieved using compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds, are an indirect way of achieving SFA. It equips SNNs with improved learning capabilities, even with constrained synaptic plasticity, and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing, thus lowering computational load and memory access time—essential aspects of neural computation. Moreover, SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional ANNs, while also requiring fewer neurons for comparable tasks. This efficiency streamlines the computational workflow and conserves space and energy, while maintaining technical integrity. High-performance deep spiking neural networks can operate with 0.3 spikes per neuron. == Applications == SNNs can in principle be applied to the same applications as traditional ANNs. In addition, SNNs can model the central nervous system of biological organisms, such as an insect seeking food without prior knowledge of the environment. Due to their relative realism, they can be used to study biological neural circuits.'},\n",
       " {'id': 'History of artificial neural networks_0',\n",
       "  'title': 'History of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by biological neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics, the first implementation of ANNs was by psychologist Frank Rosenblatt, who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s, with the AAAI calling this period an \"AI winter\". Later, advances in hardware and the development of the backpropagation algorithm, as well as recurrent neural networks and convolutional neural networks, renewed interest in ANNs. The 2010s saw the development of a deep neural network (i.e., one with many layers) called AlexNet. It greatly outperformed other image recognition models, and is thought to have launched the ongoing AI spring, and further increasing interest in deep learning. The transformer architecture was first described in 2017 as a method to teach ANNs grammatical dependencies in language, and is the predominant architecture used by large language models such as GPT-4. Diffusion models were first described in 2015, and became the basis of image generation models such as DALL-E in the 2020s. == Perceptrons and other early neural networks == The simplest feedforward network consists of a single weight layer without activation functions. It would be just a linear map, and training it would be linear regression. Linear regression by least squares method was used by Adrien-Marie Legendre (1805) and Carl Friedrich Gauss (1795) for the prediction of planetary movement. A Logical Calculus of the Ideas Immanent in Nervous Activity (Warren McCulloch and Walter Pitts, 1943) studied several abstract models for neural networks using symbolic logic of Rudolf Carnap and Principia Mathematica. The paper argued that several abstract models of neural networks (some learning, some not learning) have the same computational power as'},\n",
       " {'id': 'History of artificial neural networks_1',\n",
       "  'title': 'History of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'of the Ideas Immanent in Nervous Activity (Warren McCulloch and Walter Pitts, 1943) studied several abstract models for neural networks using symbolic logic of Rudolf Carnap and Principia Mathematica. The paper argued that several abstract models of neural networks (some learning, some not learning) have the same computational power as Turing machines. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata. In the early 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing\\'s B-type machines. B. Farley and Wesley A. Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). Frank Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition. A multilayer perceptron (MLP) comprised 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells. He later published a 1962 book also introduced variants and computer experiments, including a version with four-layer perceptrons where the last two layers have learned weights'},\n",
       " {'id': 'History of artificial neural networks_2',\n",
       "  'title': 'History of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells. He later published a 1962 book also introduced variants and computer experiments, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron). Some consider that the 1962 book developed and explored all of the basic ingredients of the deep learning systems of today. Some say that research stagnated following Marvin Minsky and Seymour Papert\\'s Perceptrons (1969). Group method of data handling, a method to train arbitrarily deep neural networks was published by Alexey Ivakhnenko and Lapa in 1967, which they regarded as a form of polynomial regression, or a generalization of Rosenblatt\\'s perceptron. A 1971 paper described a deep network with eight layers trained by this method. The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\\'ichi Amari. In computer experiments conducted by Amari\\'s student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. == Backpropagation == Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was developed multiple times in early 1970s. The earliest published instance was Seppo Linnainmaa\\'s master thesis (1970). Paul Werbos developed it independently in 1971, but had difficulty publishing it until 1982.'},\n",
       " {'id': 'History of artificial neural networks_3',\n",
       "  'title': 'History of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was developed multiple times in early 1970s. The earliest published instance was Seppo Linnainmaa\\'s master thesis (1970). Paul Werbos developed it independently in 1971, but had difficulty publishing it until 1982. In 1986, David E. Rumelhart et al. popularized backpropagation. == Recurrent network architectures == One origin of the recurrent neural network (RNN) was statistical mechanics. The Ising model was developed by Wilhelm Lenz and Ernst Ising in the 1920s as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time. Shun\\'ichi Amari in 1972 proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning. This was popularized as the Hopfield network (1982). Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex. In 1933, Lorente de Nó discovered \"recurrent, reciprocal connections\" by Golgi\\'s method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex. Hebb considered \"reverberating circuit\" as an explanation for short-term memory. (McCulloch & Pitts 1943) considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past. Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === Sepp'},\n",
       " {'id': 'History of artificial neural networks_4',\n",
       "  'title': 'History of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === Sepp Hochreiter\\'s diploma thesis (1991) proposed the neural history compressor, and identified and analyzed the vanishing gradient problem. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture. Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. It became the default choice for RNN architecture. Around 2006, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google voice search, and dictation on Android devices. LSTM broke records for improved machine translation, language modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Convolutional neural networks (CNNs) == The origin of the CNN architecture is the \"neocognitron\" introduced by Kunihiko Fukushima in 1980. It was inspired by work of Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions'},\n",
       " {'id': 'Neural network (disambiguation)_0',\n",
       "  'title': 'Neural network (disambiguation)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'A neural network is an interconnected population of neurons: Neural network (biology), a network of real neurons in the brain Neural network (machine learning), a network of mathematical neurons used in computation Neural network or Neural Networks may also refer to: Neural Networks (journal), a peer-reviewed scientific journal Neural Networks: A Comprehensive Foundation, a 1999 book by Simon Haykin Neural Networks. A Systematic Introduction, a 1996 book by Raúl Rojas == See also == All pages with titles containing Neural network'},\n",
       " {'id': 'Neural network (disambiguation)_1',\n",
       "  'title': 'Neural network (disambiguation)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'computation Neural network or Neural Networks may also refer to: Neural Networks (journal), a peer-reviewed scientific journal Neural Networks: A Comprehensive Foundation, a 1999 book by Simon Haykin Neural Networks. A Systematic Introduction, a 1996 book by Raúl Rojas == See also == All pages with titles containing Neural network'},\n",
       " {'id': 'Types of artificial neural networks_0',\n",
       "  'title': 'Types of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'There are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation). Some artificial neural networks are adaptive systems and are used for example to model populations and environments, which constantly change. Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms. == Feedforward == In feedforward neural networks the information moves from the input to output directly in every layer. There can be hidden layers with or without cycles/loops to sequence inputs. Feedforward networks can be constructed with various types of units, such as binary McCulloch–Pitts neurons, the simplest of which is the perceptron. Continuous neurons, frequently with sigmoidal activation, are used in the context of backpropagation. == Group method of data handling == The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the'},\n",
       " {'id': 'Types of artificial neural networks_1',\n",
       "  'title': 'Types of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task. == Autoencoder == An autoencoder, autoassociator or Diabolo network is similar to the multilayer perceptron (MLP) – with an input layer, an output layer and one or more hidden layers connecting them. However, the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore, autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings, typically for the purpose of dimensionality reduction and for learning generative models of data. == Probabilistic == A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input, hidden pattern, hidden summation, and output. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. == Time delay == A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance, delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of'},\n",
       " {'id': 'Types of artificial neural networks_2',\n",
       "  'title': 'Types of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"delay == A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance, delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). == Convolutional == A convolutional neural network (CNN, or ConvNet or shift invariant or space invariant) is a class of deep network, composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap, over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation. CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate. Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations. Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition, recommender systems and natural\"},\n",
       " {'id': 'Types of artificial neural networks_3',\n",
       "  'title': 'Types of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations. Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition, recommender systems and natural language processing. == Deep stacking network == A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks. Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is H = σ ( W T X ) {\\\\displaystyle {\\\\boldsymbol {H}}=\\\\sigma ({\\\\boldsymbol {W}}^{T}{\\\\boldsymbol {X}})} . Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the\"},\n",
       " {'id': 'Types of artificial neural networks_4',\n",
       "  'title': 'Types of artificial neural networks',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: min U T f = ‖ U T H − T ‖ F 2 , {\\\\displaystyle \\\\min _{U^{T}}f=\\\\|{\\\\boldsymbol {U}}^{T}{\\\\boldsymbol {H}}-{\\\\boldsymbol {T}}\\\\|_{F}^{2},} which has a closed-form solution. Unlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem. In purely discriminative tasks, DSNs outperform conventional DBNs. === Tensor deep stacking networks === This architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor. While parallelization and scalability are not considered seriously in conventional DNNs, all learning for DSNs and TDSNs is done in batch mode, to allow parallelization. Parallelization allows scaling the design to larger (deeper) architectures and data sets. The basic architecture is suitable for diverse tasks such as classification and regression. == Physics-informed == Such a neural network is designed for the numerical solution of mathematical equations, such as differential, integral, delay, fractional and others. As input parameters, PINN accepts variables (spatial, temporal, and others), transmits them through the network block. At the output, it produces an approximate solution and substitutes it into the mathematical\"},\n",
       " {'id': 'Generative adversarial network_0',\n",
       "  'title': 'Generative adversarial network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks compete with each other in the form of a zero-sum game, where one agent\\'s gain is another agent\\'s loss. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning. The core idea of a GAN is based on the \"indirect\" training through the discriminator, another neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner. GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks. == Definition == === Mathematical === The original GAN is defined as the following game: Each probability space ( Ω , μ ref ) {\\\\displaystyle (\\\\Omega ,\\\\mu _{\\\\text{ref}})} defines a GAN game. There are 2 players: generator and discriminator. The generator\\'s strategy set is P ( Ω ) {\\\\displaystyle {\\\\mathcal {P}}(\\\\Omega )} , the set of all probability measures μ G {\\\\displaystyle \\\\mu _{G}} on Ω {\\\\displaystyle \\\\Omega } . The discriminator\\'s strategy set is the set of Markov kernels μ D : Ω → P [ 0 , 1'},\n",
       " {'id': 'Generative adversarial network_1',\n",
       "  'title': 'Generative adversarial network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"generator's strategy set is P ( Ω ) {\\\\displaystyle {\\\\mathcal {P}}(\\\\Omega )} , the set of all probability measures μ G {\\\\displaystyle \\\\mu _{G}} on Ω {\\\\displaystyle \\\\Omega } . The discriminator's strategy set is the set of Markov kernels μ D : Ω → P [ 0 , 1 ] {\\\\displaystyle \\\\mu _{D}:\\\\Omega \\\\to {\\\\mathcal {P}}[0,1]} , where P [ 0 , 1 ] {\\\\displaystyle {\\\\mathcal {P}}[0,1]} is the set of probability measures on [ 0 , 1 ] {\\\\displaystyle [0,1]} . The GAN game is a zero-sum game, with objective function L ( μ G , μ D ) := E x ∼ μ ref , y ∼ μ D ( x ) \\u2061 [ ln \\u2061 y ] + E x ∼ μ G , y ∼ μ D ( x ) \\u2061 [ ln \\u2061 ( 1 − y ) ] . {\\\\displaystyle L(\\\\mu _{G},\\\\mu _{D}):=\\\\operatorname {E} _{x\\\\sim \\\\mu _{\\\\text{ref}},y\\\\sim \\\\mu _{D}(x)}[\\\\ln y]+\\\\operatorname {E} _{x\\\\sim \\\\mu _{G},y\\\\sim \\\\mu _{D}(x)}[\\\\ln(1-y)].} The generator aims to minimize the objective, and the discriminator aims to maximize the objective. The generator's task is to approach μ G ≈ μ ref {\\\\displaystyle \\\\mu _{G}\\\\approx \\\\mu _{\\\\text{ref}}} , that is, to match its own output distribution as closely as possible to the reference distribution. The discriminator's task is to output a value close to 1 when the input appears to be from the reference distribution, and to output a value close to 0 when the input looks like it came from the generator distribution. === In practice === The generative network generates candidates while the discriminative network evaluates them. This creates a contest based on data distributions, where the generator learns to map from a latent space to the true data distribution, aiming to produce candidates that the discriminator cannot distinguish from real\"},\n",
       " {'id': 'Generative adversarial network_2',\n",
       "  'title': 'Generative adversarial network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'distribution. === In practice === The generative network generates candidates while the discriminative network evaluates them. This creates a contest based on data distributions, where the generator learns to map from a latent space to the true data distribution, aiming to produce candidates that the discriminator cannot distinguish from real data. The discriminator’s goal is to correctly identify these candidates, but as the generator improves, its task becomes more challenging, increasing the discriminator’s error rate. A known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically, the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples, while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network. === Relation to other statistical machine learning methods === GANs are implicit generative models, which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample, unlike alternatives such as flow-based generative model. Compared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network. Compared to Boltzmann machines and linear ICA, there is no restriction on the type of function used by the network. Since neural networks are universal approximators, GANs are asymptotically'},\n",
       " {'id': 'Generative adversarial network_3',\n",
       "  'title': 'Generative adversarial network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network. Compared to Boltzmann machines and linear ICA, there is no restriction on the type of function used by the network. Since neural networks are universal approximators, GANs are asymptotically consistent. Variational autoencoders might be universal approximators, but it is not proven as of 2017. == Mathematical properties == === Measure-theoretic considerations === This section provides some of the mathematical theory behind these methods. In modern probability theory based on measure theory, a probability space also needs to be equipped with a σ-algebra. As a result, a more rigorous definition of the GAN game would make the following changes:Each probability space ( Ω , B , μ ref ) {\\\\displaystyle (\\\\Omega ,{\\\\mathcal {B}},\\\\mu _{\\\\text{ref}})} defines a GAN game. The generator's strategy set is P ( Ω , B ) {\\\\displaystyle {\\\\mathcal {P}}(\\\\Omega ,{\\\\mathcal {B}})} , the set of all probability measures μ G {\\\\displaystyle \\\\mu _{G}} on the measure-space ( Ω , B ) {\\\\displaystyle (\\\\Omega ,{\\\\mathcal {B}})} . The discriminator's strategy set is the set of Markov kernels μ D : ( Ω , B ) → P ( [ 0 , 1 ] , B ( [ 0 , 1 ] ) ) {\\\\displaystyle \\\\mu _{D}:(\\\\Omega ,{\\\\mathcal {B}})\\\\to {\\\\mathcal {P}}([0,1],{\\\\mathcal {B}}([0,1]))} , where B ( [ 0 , 1 ] ) {\\\\displaystyle {\\\\mathcal {B}}([0,1])} is the Borel σ-algebra on [ 0 , 1 ] {\\\\displaystyle [0,1]} .Since issues of measurability never arise in practice, these will not concern us further. === Choice of the strategy set === In the most generic version of the GAN game described above, the strategy set for the discriminator contains all Markov kernels μ D : Ω → P [ 0 , 1\"},\n",
       " {'id': 'Generative adversarial network_4',\n",
       "  'title': 'Generative adversarial network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"of measurability never arise in practice, these will not concern us further. === Choice of the strategy set === In the most generic version of the GAN game described above, the strategy set for the discriminator contains all Markov kernels μ D : Ω → P [ 0 , 1 ] {\\\\displaystyle \\\\mu _{D}:\\\\Omega \\\\to {\\\\mathcal {P}}[0,1]} , and the strategy set for the generator contains arbitrary probability distributions μ G {\\\\displaystyle \\\\mu _{G}} on Ω {\\\\displaystyle \\\\Omega } . However, as shown below, the optimal discriminator strategy against any μ G {\\\\displaystyle \\\\mu _{G}} is deterministic, so there is no loss of generality in restricting the discriminator's strategies to deterministic functions D : Ω → [ 0 , 1 ] {\\\\displaystyle D:\\\\Omega \\\\to [0,1]} . In most applications, D {\\\\displaystyle D} is a deep neural network function. As for the generator, while μ G {\\\\displaystyle \\\\mu _{G}} could theoretically be any computable probability distribution, in practice, it is usually implemented as a pushforward: μ G = μ Z ∘ G − 1 {\\\\displaystyle \\\\mu _{G}=\\\\mu _{Z}\\\\circ G^{-1}} . That is, start with a random variable z ∼ μ Z {\\\\displaystyle z\\\\sim \\\\mu _{Z}} , where μ Z {\\\\displaystyle \\\\mu _{Z}} is a probability distribution that is easy to compute (such as the uniform distribution, or the Gaussian distribution), then define a function G : Ω Z → Ω {\\\\displaystyle G:\\\\Omega _{Z}\\\\to \\\\Omega } . Then the distribution μ G {\\\\displaystyle \\\\mu _{G}} is the distribution of G ( z ) {\\\\displaystyle G(z)} . Consequently, the generator's strategy is usually defined as just G {\\\\displaystyle G} , leaving z ∼ μ Z {\\\\displaystyle z\\\\sim \\\\mu _{Z}} implicit. In this formalism, the GAN game objective is L ( G , D ) := E x ∼ μ ref \\u2061 [ ln\"},\n",
       " {'id': 'Dilution (neural networks)_0',\n",
       "  'title': 'Dilution (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Dropout and dilution (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to randomly decreasing weights towards zero, while dropout refers to randomly setting the outputs of hidden neurons to zero. Both are usually performed during the training process of a neural network, not during inference. == Types and uses == Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small, and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is, and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case, weak dilution refers to adding a small amount of damping noise, while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights, but this is usually a non-recurring one-way operation. The network is pruned, and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning, while in dilution/dropout, the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes, in an artificial neural net can be described as y i {\\\\displaystyle y_{i}} – output from node i {\\\\displaystyle i} w i'},\n",
       " {'id': 'Dilution (neural networks)_1',\n",
       "  'title': 'Dilution (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'network continues learning, while in dilution/dropout, the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes, in an artificial neural net can be described as y i {\\\\displaystyle y_{i}} – output from node i {\\\\displaystyle i} w i j {\\\\displaystyle w_{ij}} – real weight before dilution, also called the Hebb connection strength x j {\\\\displaystyle x_{j}} – input from node j {\\\\displaystyle j} This can be written in vector notation as y {\\\\displaystyle \\\\mathbf {y} } – output vector W {\\\\displaystyle \\\\mathbf {W} } – weight matrix x {\\\\displaystyle \\\\mathbf {x} } – input vector Equations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution, the finite fraction of removed connections (the weights) is small, giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as w ^ i j {\\\\displaystyle {\\\\hat {w}}_{ij}} – diluted weight w i j {\\\\displaystyle w_{ij}} – real weight before dilution P ( c ) {\\\\displaystyle P(c)} – the probability of c {\\\\displaystyle c} , the probability of keeping a weight The interpretation of probability P ( c ) {\\\\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as where the function g \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {g} (\\\\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed), thus mean field theory can be applied. In the notation from Hertz et'},\n",
       " {'id': 'Dilution (neural networks)_2',\n",
       "  'title': 'Dilution (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed), thus mean field theory can be applied. In the notation from Hertz et al. this would be written as ⟨ h i ⟩ {\\\\displaystyle \\\\left\\\\langle h_{i}\\\\right\\\\rangle } the mean field temperature c {\\\\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight w i j {\\\\displaystyle w_{ij}} – real weight before dilution, also called the Hebb connection strength ⟨ S j ⟩ {\\\\displaystyle \\\\left\\\\langle S_{j}\\\\right\\\\rangle } – the mean stable equilibrium states There are some assumptions for this to hold, which are not listed here. == Strong dilution == When the dilution is strong, the finite fraction of removed connections (the weights) is large, giving rise to a huge uncertainty. == Dropout == Dropout is a special case of the previous weight equation (3), where the aforementioned equation is adjusted to remove a whole row in the vector matrix, and not only random weights P ( c ) {\\\\displaystyle P(c)} – the probability c {\\\\displaystyle c} to keep a row in the weight matrix w j {\\\\displaystyle \\\\mathbf {w} _{j}} – real row in the weight matrix before dropout w ^ j {\\\\displaystyle {\\\\hat {\\\\mathbf {w} }}_{j}} – diluted row in the weight matrix Because dropout removes a whole row from the vector matrix, the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero, whether by setting the weights to zero, by “removing the node”, or by some other means, does not impact the end result and does not create'},\n",
       " {'id': 'Dilution (neural networks)_3',\n",
       "  'title': 'Dilution (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero, whether by setting the weights to zero, by “removing the node”, or by some other means, does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator, then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor, perhaps even an analog neuromorphic processor, then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google's patent == Although there have been examples of randomly removing connections between neurons in a neural network to improve models, this technique was first introduced with the name dropout by Geoffrey Hinton, et al. in 2012. Google currently holds the patent for the dropout technique. == See also == AlexNet Convolutional neural network § Dropout == Notes == == References ==\"},\n",
       " {'id': 'Dilution (neural networks)_4',\n",
       "  'title': 'Dilution (neural networks)',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'between neurons in a neural network to improve models, this technique was first introduced with the name dropout by Geoffrey Hinton, et al. in 2012. Google currently holds the patent for the dropout technique. == See also == AlexNet Convolutional neural network § Dropout == Notes == == References =='},\n",
       " {'id': 'Siamese neural network_0',\n",
       "  'title': 'Siamese neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"A Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed, thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing. It is possible to build an architecture that is functionally similar to a twin network but implements a slightly different function. This is typically used for comparing similar instances in different type sets. Uses of similarity measures where a twin network might be used are such things as recognizing handwritten checks, automatic detection of faces in camera images, and matching queries with indexed documents. The perhaps most well-known application of twin networks are face recognition, where known images of people are precomputed and compared to an image from a turnstile or similar. It is not obvious at first, but there are two slightly different problems. One is recognizing a person among a large number of other persons, that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification, that is for example, to verify whether a photo in a passport matches the face of the passport's owner. The twin network might be the same, but the implementation can be quite different. == Learning == Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The\"},\n",
       " {'id': 'Siamese neural network_1',\n",
       "  'title': 'Siamese neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'the same, but the implementation can be quite different. == Learning == Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network, while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights, or some similar operation like a normalization. A distance metric for a loss function may have the following properties Non-negativity: δ ( x , y ) ≥ 0 {\\\\displaystyle \\\\delta (x,y)\\\\geq 0} Identity of Non-discernibles: δ ( x , y ) = 0 ⟺ x = y {\\\\displaystyle \\\\delta (x,y)=0\\\\iff x=y} Commutativity: δ ( x , y ) = δ ( y , x ) {\\\\displaystyle \\\\delta (x,y)=\\\\delta (y,x)} Triangle inequality: δ ( x , z ) ≤ δ ( x , y ) + δ ( y , z ) {\\\\displaystyle \\\\delta (x,z)\\\\leq \\\\delta (x,y)+\\\\delta (y,z)} In particular, the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean, does not have triangle inequality) distance at its core. === Predefined metrics, Euclidean distance metric === The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like δ ( x ( i ) , x ( j ) ) = { min ‖ f \\u2061 ( x ( i ) ) − f \\u2061 ( x ( j ) ) ‖ , i = j max ‖ f \\u2061 ( x ( i ) ) − f \\u2061 ( x ( j ) ) ‖ , i ≠ j {\\\\displaystyle {\\\\begin{aligned}\\\\delta (x^{(i)},x^{(j)})={\\\\begin{cases}\\\\min \\\\'},\n",
       " {'id': 'Siamese neural network_2',\n",
       "  'title': 'Siamese neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'f \\u2061 ( x ( i ) ) − f \\u2061 ( x ( j ) ) ‖ , i = j max ‖ f \\u2061 ( x ( i ) ) − f \\u2061 ( x ( j ) ) ‖ , i ≠ j {\\\\displaystyle {\\\\begin{aligned}\\\\delta (x^{(i)},x^{(j)})={\\\\begin{cases}\\\\min \\\\ \\\\|\\\\operatorname {f} \\\\left(x^{(i)}\\\\right)-\\\\operatorname {f} \\\\left(x^{(j)}\\\\right)\\\\|\\\\,,i=j\\\\\\\\\\\\max \\\\ \\\\|\\\\operatorname {f} \\\\left(x^{(i)}\\\\right)-\\\\operatorname {f} \\\\left(x^{(j)}\\\\right)\\\\|\\\\,,i\\\\neq j\\\\end{cases}}\\\\end{aligned}}} i , j {\\\\displaystyle i,j} are indexes into a set of vectors f \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {f} (\\\\cdot )} function implemented by the twin network The most common distance metric used is Euclidean distance, in case of which the loss function can be rewritten in matrix form as δ \\u2061 ( x ( i ) , x ( j ) ) ≈ ( x ( i ) − x ( j ) ) T ( x ( i ) − x ( j ) ) {\\\\displaystyle \\\\operatorname {\\\\delta } (\\\\mathbf {x} ^{(i)},\\\\mathbf {x} ^{(j)})\\\\approx (\\\\mathbf {x} ^{(i)}-\\\\mathbf {x} ^{(j)})^{T}(\\\\mathbf {x} ^{(i)}-\\\\mathbf {x} ^{(j)})} === Learned metrics, nonlinear distance metric === A more general case is where the output vector from the twin network is passed through additional network layers implementing non-linear distance metrics. if i = j then δ \\u2061 [ f \\u2061 ( x ( i ) ) , f \\u2061 ( x ( j ) ) ] is small otherwise δ \\u2061 [ f \\u2061 ( x ( i ) ) , f \\u2061 ( x ( j ) ) ] is large {\\\\displaystyle {\\\\begin{aligned}{\\\\text{if}}\\\\,i=j\\\\,{\\\\text{then}}&\\\\,\\\\operatorname {\\\\delta } \\\\left[\\\\operatorname {f} \\\\left(x^{(i)}\\\\right),\\\\,\\\\operatorname {f} \\\\left(x^{(j)}\\\\right)\\\\right]\\\\,{\\\\text{is small}}\\\\\\\\{\\\\text{otherwise}}&\\\\,\\\\operatorname {\\\\delta } \\\\left[\\\\operatorname {f} \\\\left(x^{(i)}\\\\right),\\\\,\\\\operatorname {f} \\\\left(x^{(j)}\\\\right)\\\\right]\\\\,{\\\\text{is large}}\\\\end{aligned}}} i , j {\\\\displaystyle i,j} are indexes into a set of vectors f \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {f} (\\\\cdot )} function implemented by the twin network δ \\u2061 ( ⋅'},\n",
       " {'id': 'Siamese neural network_3',\n",
       "  'title': 'Siamese neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': '{\\\\displaystyle {\\\\begin{aligned}{\\\\text{if}}\\\\,i=j\\\\,{\\\\text{then}}&\\\\,\\\\operatorname {\\\\delta } \\\\left[\\\\operatorname {f} \\\\left(x^{(i)}\\\\right),\\\\,\\\\operatorname {f} \\\\left(x^{(j)}\\\\right)\\\\right]\\\\,{\\\\text{is small}}\\\\\\\\{\\\\text{otherwise}}&\\\\,\\\\operatorname {\\\\delta } \\\\left[\\\\operatorname {f} \\\\left(x^{(i)}\\\\right),\\\\,\\\\operatorname {f} \\\\left(x^{(j)}\\\\right)\\\\right]\\\\,{\\\\text{is large}}\\\\end{aligned}}} i , j {\\\\displaystyle i,j} are indexes into a set of vectors f \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {f} (\\\\cdot )} function implemented by the twin network δ \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {\\\\delta } (\\\\cdot )} function implemented by the network joining outputs from the twin network On a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as δ \\u2061 ( x ( i ) , x ( j ) ) ≈ ( x ( i ) − x ( j ) ) T M ( x ( i ) − x ( j ) ) {\\\\displaystyle \\\\operatorname {\\\\delta } (\\\\mathbf {x} ^{(i)},\\\\mathbf {x} ^{(j)})\\\\approx (\\\\mathbf {x} ^{(i)}-\\\\mathbf {x} ^{(j)})^{T}\\\\mathbf {M} (\\\\mathbf {x} ^{(i)}-\\\\mathbf {x} ^{(j)})} This can be further subdivided in at least Unsupervised learning and Supervised learning. === Learned metrics, half-twin networks === This form also allows the twin network to be more of a half-twin, implementing a slightly different functions if i = j then δ \\u2061 [ f \\u2061 ( x ( i ) ) , g \\u2061 ( x ( j ) ) ] is small otherwise δ \\u2061 [ f \\u2061 ( x ( i ) ) , g \\u2061 ( x ( j ) ) ] is large {\\\\displaystyle {\\\\begin{aligned}{\\\\text{if}}\\\\,i=j\\\\,{\\\\text{then}}&\\\\,\\\\operatorname {\\\\delta } \\\\left[\\\\operatorname {f} \\\\left(x^{(i)}\\\\right),\\\\,\\\\operatorname {g} \\\\left(x^{(j)}\\\\right)\\\\right]\\\\,{\\\\text{is small}}\\\\\\\\{\\\\text{otherwise}}&\\\\,\\\\operatorname {\\\\delta } \\\\left[\\\\operatorname {f} \\\\left(x^{(i)}\\\\right),\\\\,\\\\operatorname {g} \\\\left(x^{(j)}\\\\right)\\\\right]\\\\,{\\\\text{is large}}\\\\end{aligned}}} i , j {\\\\displaystyle i,j} are indexes into a set of vectors f \\u2061 ( ⋅ ) , g \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {f} (\\\\cdot ),\\\\operatorname {g} (\\\\cdot )} function implemented by the half-twin network δ \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {\\\\delta } (\\\\cdot )} function'},\n",
       " {'id': 'Siamese neural network_4',\n",
       "  'title': 'Siamese neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'large}}\\\\end{aligned}}} i , j {\\\\displaystyle i,j} are indexes into a set of vectors f \\u2061 ( ⋅ ) , g \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {f} (\\\\cdot ),\\\\operatorname {g} (\\\\cdot )} function implemented by the half-twin network δ \\u2061 ( ⋅ ) {\\\\displaystyle \\\\operatorname {\\\\delta } (\\\\cdot )} function implemented by the network joining outputs from the twin network == Twin networks for object tracking == Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking, one input of the twin network is user pre-selected exemplar image, the other input is a larger search image. The twin network\\'s job is to locate the exemplar inside of the search image. By measuring the similarity between exemplar and each part of the search image, a map of similarity score can be given by the twin network. Furthermore, using a Fully Convolutional Network, the process of computing each sector\\'s similarity score can be replaced with only one cross correlation layer. After being first introduced in 2016, Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks. Like CFnet, StructSiam, SiamFC-tri, DSiam, SA-Siam, SiamRPN, DaSiamRPN, Cascaded SiamRPN, SiamMask, SiamRPN++, Deeper and Wider SiamRPN. == See also == Artificial neural network Triplet loss == Further reading == Chicco, Davide (2020), \"Siamese neural networks: an overview\", Artificial Neural Networks, Methods in Molecular Biology, vol. 2190 (3rd ed.), New York City, New York, USA: Springer Protocols, Humana Press, pp. 73–94, doi:10.1007/978-1-0716-0826-5_3, ISBN 978-1-0716-0826-5, PMID 32804361, S2CID 221144012 == References =='},\n",
       " {'id': 'Feedback neural network_0',\n",
       "  'title': 'Feedback neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'Feedback neural network are neural networks with the ability to provide bottom-up and top-down design feedback to their input or previous layers, based on their outputs or subsequent layers. This is notably used in large language models specifically in reasoning language models (RLM). This process is designed to mimic self-assessment and internal deliberation, aiming to minimize errors (like hallucinations) and increase interpretability. Reflection is a form of \"test-time compute\", where additional computational resources are used during inference. == Introduction == Traditional neural networks process inputs in a feedforward manner, generating outputs in a single pass. However, their limitations in handling complex tasks, and especially compositional ones, have led to the development of methods that simulate internal deliberation. Techniques such as chain-of-thought prompting encourage models to generate intermediate reasoning steps, thereby improving their performance in such tasks. The feedback can take place either after a full network pass and decoding to tokens, or continuously in latent space (the last layer can be fed back to the first layer). In LLMs, special tokens can mark the beginning and end of reflection before producing a final response (e.g., <thinking>). This internal process of \"thinking\" about the steps leading to an answer is designed to be analogous to human metacognition or \"thinking about thinking\". It helps AI systems approach tasks that require multi-step reasoning, planning, and logical thought. == Techniques == Increasing the length of the Chain-of-Thought reasoning process, by passing the output of the model back to its input and doing multiple network passes, increases inference-time scaling. Reinforcement learning frameworks have also been used to steer the Chain-of-Thought. One example is Group Relative Policy Optimization (GRPO), used in DeepSeek-R1, a variant of policy gradient methods that eliminates the need for a separate \"critic\" model by normalizing rewards within a group of generated outputs,'},\n",
       " {'id': 'Feedback neural network_1',\n",
       "  'title': 'Feedback neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': 'passes, increases inference-time scaling. Reinforcement learning frameworks have also been used to steer the Chain-of-Thought. One example is Group Relative Policy Optimization (GRPO), used in DeepSeek-R1, a variant of policy gradient methods that eliminates the need for a separate \"critic\" model by normalizing rewards within a group of generated outputs, reducing computational cost. Simple techniques like \"budget forcing\" (forcing the model to continue generating reasoning steps) have also proven effective in improving performance. === Types of reflection === ==== Post-hoc reflection ==== Analyzes and critiques an initial output separately, often involving prompting the model to identify errors or suggest improvements after generating a response. The Reflexion framework follows this approach. ==== Iterative reflection ==== Revises earlier parts of a response dynamically during generation. Self-monitoring mechanisms allow the model to adjust reasoning as it progresses. Methods like Tree-of-Thoughts exemplify this, enabling backtracking and alternative exploration. ==== Intrinsic reflection ==== Integrates self-monitoring directly into the model architecture rather than relying solely on external prompts, enabling models with inherent awareness of their reasoning limitations and uncertainties. This has been used by Google DeepMind in a technique called Self-Correction via Reinforcement Learning (SCoRe) which rewards the model for improving its responses. === Process reward models and limitations === Early research explored PRMs to provide feedback on each reasoning step, unlike traditional reinforcement learning which rewards only the final outcome. However, PRMs have faced challenges, including computational cost and reward hacking. DeepSeek-R1\\'s developers found them to be not beneficial. == See also == Reflective programming Reservoir computing == References =='},\n",
       " {'id': 'Feedback neural network_2',\n",
       "  'title': 'Feedback neural network',\n",
       "  'topic': 'Neural Networks',\n",
       "  'text': \"research explored PRMs to provide feedback on each reasoning step, unlike traditional reinforcement learning which rewards only the final outcome. However, PRMs have faced challenges, including computational cost and reward hacking. DeepSeek-R1's developers found them to be not beneficial. == See also == Reflective programming Reservoir computing == References ==\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理与向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VectorDatabase:\n",
    "    def __init__(self,db_path='vector_store.db'):\n",
    "        self.db_path = db_path\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        # 创建文档表格\n",
    "        self.cursor.execute('''CREATE TABLE IF NOT EXISTS documents\\\n",
    "        (id TEXT PRIMARY KEY, title TEXT, topic TEXT, text TEXT, embedding BLOB)''')\n",
    "    \n",
    "        self.conn.commit()\n",
    "        \n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        # 初始化，内积相似度\n",
    "        self.index = faiss.IndexFlatIP(384) # 匹配模型的维度\n",
    "        self.id_list = [] # 映射FAISS索引ID到文档ID\n",
    "        self.rebuild_index() # 从数据库重建索引\n",
    "        \n",
    "    def rebuild_index(self):\n",
    "        self.index.reset() # 清空索引\n",
    "        self.id_list = []\n",
    "        \n",
    "        self.cursor.execute('SELECT rowid,id,embedding FROM documents')\n",
    "        rows = self.cursor.fetchall()\n",
    "        \n",
    "        if not rows:\n",
    "            print('dataset is empty-no index to rebuild')\n",
    "            return\n",
    "\n",
    "        \n",
    "        embeddings = []\n",
    "        for idx,(rowid,doc_id,emb_blob) in enumerate(rows):\n",
    "            emb = np.frombuffer(emb_blob,dtype=np.float32)\n",
    "            embeddings.append(emb)\n",
    "            self.id_list.append(doc_id)\n",
    "        embeddings = np.vstack(embeddings).astype('float32')\n",
    "        self.index.add(embeddings)\n",
    "        print(f'rebuilt index with {len(embeddings)} vectors')\n",
    "        \n",
    "        \n",
    "    def add_documents(self,documents):\n",
    "        texts = [doc['text'] for doc in documents]\n",
    "        \n",
    "        embeddings = self.model.encode(texts,\n",
    "                                      show_progress_bar=True,\n",
    "                                      batch_size=32)\n",
    "        \n",
    "        for doc,emb in zip(documents,embeddings):\n",
    "            self.cursor.execute('INSERT OR REPLACE INTO documents VALUES (?, ?, ?, ?, ?)',\n",
    "                               (doc['id'],doc['title'],doc['topic'],doc['text'],\n",
    "                               emb.tobytes()))\n",
    "            \n",
    "        self.conn.commit()\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        self.index.add(embeddings)\n",
    "    \n",
    "    def search(self,query,k=5):\n",
    "        query_emb = self.model.encode([query])\n",
    "        # FAISS搜索\n",
    "        D,I = self.index.search(np.array(query_emb).astype('float32'),k)\n",
    "        \n",
    "        results = []\n",
    "        for i,idx in enumerate(I[0]):\n",
    "            if idx < 0:\n",
    "                continue\n",
    "            \n",
    "            if idx < len(self.id_list):\n",
    "                doc_id = self.id_list[idx]\n",
    "                self.cursor.execute(\"SELECT * FROM documents WHERE id=?\", (doc_id,))\n",
    "                row = self.cursor.fetchone()\n",
    "            \n",
    "                if row:\n",
    "                    results.append({\n",
    "                        'id':row[0],\n",
    "                        'title':row[1],\n",
    "                        'topic':row[2],\n",
    "                        'text':row[3],\n",
    "                        'score':D[0][i] # 相似度分数\n",
    "                    })\n",
    "            else:\n",
    "                print(f'warning:index {idx} out of range (max{len(self.id_list)-1})')\n",
    "        return results\n",
    "    \n",
    "    def get_document(self,doc_id):\n",
    "        self.cursor.execute(\"SELECT * FROM documents WHERE id=?\", (doc_id,))\n",
    "        row = self.cursor.fetchone()\n",
    "        if row:\n",
    "            return {\n",
    "                \"id\": row[0],\n",
    "                \"title\": row[1],\n",
    "                \"topic\": row[2],\n",
    "                \"text\": row[3]\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def index_status(self):\n",
    "        \"\"\"检查索引状态\"\"\"\n",
    "        print(f\"\\nIndex contains {self.index.ntotal} vectors\")\n",
    "        print(f\"Database has {self.get_document_count()} documents\")\n",
    "        \n",
    "    def get_document_count(self):\n",
    "        \"\"\"获取文档数量\"\"\"\n",
    "        self.cursor.execute(\"SELECT COUNT(*) FROM documents\")\n",
    "        return self.cursor.fetchone()[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import RagTokenForGeneration\n",
    "class RAGSystem:\n",
    "    def __init__(self,db_path = 'vector_store.db'):\n",
    "        self.db = VectorDatabase(db_path)\n",
    "    \n",
    "        # 检索模型： DPR\n",
    "        self.retriever_model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "        self.retriever_tokenizer = AutoTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "    \n",
    "        # 生成模型 ： RAG-Sequence\n",
    "\n",
    "        self.generator_model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\")\n",
    "        self.generator_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "    \n",
    "        # 重排序模型 ： cross-encoder\n",
    "        self.rerank_model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        self.rerank_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        \n",
    "    def retriever(self,query,k=10):\n",
    "        # 第一阶段检索\n",
    "        expanded_query = self.expand_query(query)\n",
    "        print(f'[DEBUG] Expanded query: {expanded_query}')\n",
    "        \n",
    "        results = self.db.search(expanded_query, k)\n",
    "        print(f'[DEBUG] Retrieved {len(results)} documents')\n",
    "        \n",
    "        if results:\n",
    "            for i, doc in enumerate(results[:3]):\n",
    "                print(f'  Doc {i+1}: {doc[\"title\"]} (score: {doc[\"score\"]:.3f})')\n",
    "                print(f'  Text: {doc[\"text\"][:80]}...')\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def expand_query(self, query):\n",
    "        synonyms = {\n",
    "            'capital': ['capital city', 'main city', 'administrative center'],\n",
    "            'developed': ['created', 'formulated', 'pioneered', 'invented'],\n",
    "            'france': ['french republic', 'french nation'],\n",
    "            'relativity': ['theory of relativity', 'einstein theory', 'space-time theory'],\n",
    "            'ai': ['artificial intelligence', 'machine intelligence', 'cognitive computing'],\n",
    "            'what': ['define', 'explain', 'describe'],\n",
    "            'who': ['scientist', 'physicist', 'researcher']\n",
    "        }\n",
    "    \n",
    "    # 添加更智能的扩展\n",
    "        expanded = []\n",
    "        for word in query.lower().split():\n",
    "            if word in synonyms:\n",
    "                expanded.extend(synonyms[word])\n",
    "            else:\n",
    "                expanded.append(word)\n",
    "    \n",
    "        unique_terms = list(set(expanded))\n",
    "        return \" \".join(unique_terms)\n",
    "    \n",
    "    def rerank(self,query,documents):\n",
    "        # 过滤空的文档\n",
    "        valid_docs = [doc for doc in documents if doc.get('text','').strip()]\n",
    "        if not valid_docs:\n",
    "            return []\n",
    "        \n",
    "        pairs = [(query,doc['text']) for doc in valid_docs]\n",
    "        \n",
    "        # 编码\n",
    "        features = self.rerank_tokenizer(pairs,\n",
    "                                        padding=True,\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt',\n",
    "                                        max_length=512)\n",
    "        # 计算相关性分数\n",
    "        with torch.no_grad():\n",
    "            outputs = self.rerank_model(**features)\n",
    "            scores = outputs.logits.squeeze(dim=1)\n",
    "            \n",
    "        # 更新文档分数\n",
    "        for i,doc in enumerate(documents):\n",
    "            doc['rerank_score'] = scores[i].item() if i < len(scores) else 0.0\n",
    "            \n",
    "        return sorted(documents,key=lambda x: x['rerank_score'],reverse=True)\n",
    "    \n",
    "    def generate(self,query,context_docs,max_length=150):\n",
    "        if not context_docs:\n",
    "            print(\"[WARNING] No context documents for generation\")\n",
    "            return 'No relevant context found'\n",
    "        \n",
    "        context_texts = [doc['text'] for doc in context_docs]\n",
    "        \n",
    "        inputs = self.generator_tokenizer.prepare_seq2seq_batch(query, # question\n",
    "                                           context_texts, # 上下文列表\n",
    "                                          return_tensors='pt',\n",
    "                                        )\n",
    "        # 生成结果\n",
    "        outputs = self.generator_model.generate(input_ids=inputs['input_ids'],\n",
    "                                               attention_mask=inputs['attention_mask'],\n",
    "                                               max_length=max_length,\n",
    "                                               num_beams=4,\n",
    "                                               early_stopping=True,\n",
    "                                               no_repeat_ngram_size=3,# 避免重复\n",
    "                                               length_penalty=0.8# 长度惩罚\n",
    "                                               )\n",
    "        \n",
    "        return self.generator_tokenizer.decode(outputs[0],\n",
    "                                              skip_special_tokens=True)\n",
    "    \n",
    "    def full_pipeline(self,query):\n",
    "        # 检索\n",
    "        retrieved = self.retriever(query,k=10)\n",
    "        \n",
    "        # 重排序-- 精排\n",
    "        reranked = self.rerank(query,retrieved)[:3] # 取top3\n",
    "        \n",
    "        # 生成答案\n",
    "        return self.generate(query,reranked)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self,rag_system):\n",
    "        self.rag = rag_system\n",
    "        # 自己构建测试集来判断\n",
    "        self.dataset = [\n",
    "            {\n",
    "                \"question\": \"What is the capital of France?\",\n",
    "                \"answer\": \"Paris\",\n",
    "                \"context\": [\"France is a country in Europe. Its capital is Paris.\"]\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"Who developed the theory of relativity?\",\n",
    "                \"answer\": \"Albert Einstein\",\n",
    "                \"context\": [\"Albert Einstein was a physicist who developed the theory of relativity.\"]\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is AI?\",\n",
    "                \"answer\": \"Artificial Intelligence\",\n",
    "                \"context\": [\"Artificial intelligence (AI) is the simulation of human intelligence processes by machines.\"]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    def exact_match(self,pred,true):\n",
    "        pred = pred.lower().strip()\n",
    "        true = true.lower().strip()\n",
    "        return int(pred==true)\n",
    "    \n",
    "    def f1_score(self,pred,true):\n",
    "        pred_tokens = set(pred.lower().split())\n",
    "        true_tokens = set(true.lower().split())\n",
    "        \n",
    "        if not pred_tokens or not true_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        # 计算交集\n",
    "        common = pred_tokens & true_tokens\n",
    "        \n",
    "        precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "        recall = len(common) / len(true_tokens) if true_tokens else 0\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    def context_relevance(self,retrieved,gold_context):\n",
    "        # 上下文评估\n",
    "        gold_text = ' '.join(gold_context).lower()\n",
    "        gold_token = set(gold_text.split())\n",
    "        max_overlap = 0\n",
    "        \n",
    "        for doc in retrieved:\n",
    "            doc_text = doc['text'].lower()\n",
    "            doc_token = set(doc_text.split())\n",
    "            # 计算雅可比相似度\n",
    "            overlap = len(gold_token & doc_token)\n",
    "            similarity = overlap / len(gold_token) if gold_token else 0\n",
    "            if similarity > max_overlap:\n",
    "                max_overlap = similarity\n",
    "                \n",
    "        return max_overlap\n",
    "    \n",
    "    def evaluate(self):\n",
    "        results = []\n",
    "        for item in tqdm(self.dataset,desc='Evaluating RAG'):\n",
    "            \n",
    "            answer = self.rag.full_pipeline(item['question'])\n",
    "        \n",
    "        # 单独检索\n",
    "            retrieved = self.rag.retriever(item['question'])\n",
    "        \n",
    "        # 计算指标\n",
    "            em = self.exact_match(answer,item['answer']) # 完全匹配率\n",
    "            f1 = self.f1_score(answer,item['answer']) # 词重叠的相似度\n",
    "            ctx_rel = self.context_relevance(retrieved,item['context']) # 检索相关性\n",
    "        \n",
    "            results.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"predicted\": answer,\n",
    "                    \"expected\": item[\"answer\"],\n",
    "                    \"em\": em,\n",
    "                    \"f1\": f1,\n",
    "                    \"context_relevance\": ctx_rel\n",
    "                })\n",
    "        \n",
    "        avg_em = sum(r[\"em\"] for r in results) / len(results)\n",
    "        avg_f1 = sum(r[\"f1\"] for r in results) / len(results)\n",
    "        avg_ctx = sum(r[\"context_relevance\"] for r in results) / len(results)\n",
    "        \n",
    "        return {\n",
    "            \"results\": results,  # 详细结果\n",
    "            \"metrics\": {  # 平均指标\n",
    "                \"exact_match\": avg_em,\n",
    "                \"f1_score\": avg_f1,\n",
    "                \"context_relevance\": avg_ctx\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout\n",
    "from IPython.display import display\n",
    "import os\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例文档数据\n",
    "\n",
    "    documents_1 = [\n",
    "        {\n",
    "            \"id\": \"1\",\n",
    "            \"title\": \"France\",\n",
    "            \"topic\": \"Geography\",\n",
    "            \"text\": \"France is a country in Europe. Its capital is Paris.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"2\",\n",
    "            \"title\": \"Einstein\",\n",
    "            \"topic\": \"Science\",\n",
    "            \"text\": \"Albert Einstein was a physicist who developed the theory of relativity.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"3\",\n",
    "            \"title\": \"Artificial Intelligence\",\n",
    "            \"topic\": \"Technology\",\n",
    "            \"text\": \"Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"4\",\n",
    "            \"title\": \"Machine Learning\",\n",
    "            \"topic\": \"Technology\",\n",
    "            \"text\": \"Machine learning (ML) is a type of artificial intelligence that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\"\n",
    "        }\n",
    "    ]\n",
    "    # 初始化向量数据库\n",
    "    vector_db = VectorDatabase()\n",
    "    vector_db.add_documents(documents_1)\n",
    "    \n",
    "    print(f\"Database contains {vector_db.get_document_count()} documents\")\n",
    "    \n",
    "    print(\"\\nTesting search function...\")\n",
    "    test_queries = [\"capital of france\", \"who created relativity\", \"define AI\"]\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        results = vector_db.search(query, k=2)\n",
    "        print(f\"Found {len(results)} results\")\n",
    "        for res in results:\n",
    "            print(f\"- {res['title']} (score: {res['score']:.3f})\")\n",
    "    \n",
    "    print(\"\\nInitializing RAG system...\")\n",
    "    rag_system = RAGSystem()\n",
    "    \n",
    "    print(\"\\nTesting full pipeline...\")\n",
    "    questions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who developed the theory of relativity?\",\n",
    "        \"What is AI?\"\n",
    "    ]\n",
    "    for q in questions:\n",
    "        print(f\"\\nQuestion: {q}\")\n",
    "        answer = rag_system.full_pipeline(q)\n",
    "        print(f\"Answer: {answer}\")\n",
    "    \n",
    "    print(\"\\nRunning evaluation...\")\n",
    "    evaluator = RAGEvaluator(rag_system)\n",
    "    eval_results = evaluator.evaluate()\n",
    "    \n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for metric, value in eval_results[\"metrics\"].items():\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
